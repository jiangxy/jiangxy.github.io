<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[foolbear's Pensieve]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://jxy.me/"/>
  <updated>2015-09-05T16:30:39.317Z</updated>
  <id>http://jxy.me/</id>
  
  <author>
    <name><![CDATA[foolbear]]></name>
    
  </author>
  
  <generator uri="http://zespia.tw/hexo/">Hexo</generator>
  
  <entry>
    <title><![CDATA[一些胡思乱想]]></title>
    <link href="http://jxy.me/2015/09/04/some-messy-thought/"/>
    <id>http://jxy.me/2015/09/04/some-messy-thought/</id>
    <published>2015-09-03T17:31:29.000Z</published>
    <updated>2015-09-05T16:30:26.000Z</updated>
    <content type="html"><![CDATA[<p>挺久没写了。一直忙着各种杂事。<br>最近稍微安定下来了，胡乱写些最近的想法吧。</p>
<a id="more"></a>
<h1 id="大公司与小公司">大公司与小公司</h1>
<p>就我个人的感受，小公司的人与人的联系要更紧密。因为人少，而且业务导向，人与人之间的直接交流会比较多，互相比较熟悉。<br>以前在网易的时候，大多数工作都在popo上，一天难得说几句话。很多人我只知道名字。。。</p>
<p>大公司流程比较多，小公司不太在意这些，只要能完成任务就行。这点我比较不习惯，一些项目没有文档／jira，只靠口口相传。。。没有需求／设计的过程，直接码代码。。。<br>我的感觉，流程不能过多，但也不能完全没有，这是个取舍的过程。<br>更多的流程会减少出错的概率，但也带来很大的overhead。没有完美的方案。</p>
<p>我讨厌开会，很多会议跟我关系不大，只是“为了以防万一”叫上尽可能多的人。。。就像转发邮件要cc尽可能多的人，事后如果出问题可以免责。。。这点小公司比较有优势。</p>
<p>大公司遇到牛人的概率会大点。大公司做事有些偏向于研究的性质，而小公司更多关注业务。如果能用很low的技术实现一个需求，小公司就会一直重复着这个技术，能用就行。久而久之，很多人技术上也不会有长进。这是要警惕的。<br>倒不是说小公司技术不行，而是缺乏动力去升级技术方案，除非被逼急了，随着业务的扩张，老的方案已经hold不住。<br>当然也要看场景，过度优化／设计也是不行的。</p>
<p>另外最近我看到了一些奇葩的代码，不得不吐槽，<code>extends Object</code>是要闹哪样。。。<br>感觉很多人脱离spring就不会写程序了啊，觉得java就是tomcat。。。<br>我在网易也见过一些奇葩代码，比如6个嵌套的for循环。。。</p>
<p>好的代码总是相似的，烂的代码各有不同。还是见识的太少啊。</p>
<h1 id="关于MAC">关于MAC</h1>
<p>特指macbook。<br>mac真的是生产力工具啊，创业公司配置mac不是没有理由的。<br>用熟了之后，效率比windows上升不知几个档次，非常适合程序猿。<br>键盘＋触控板，完全不需要鼠标了。</p>
<p>而且mac下有好多神器：</p>
<ul>
<li><a href="http://brew.sh" target="_blank" rel="external">brew</a>/<a href="https://github.com/caskroom/homebrew-cask" target="_blank" rel="external">brew cask</a>。包管理工具。我发现很多系统都有包管理啊apt-get/yum/pip/gem/npm，这是发展的必然趋势？</li>
<li><a href="https://www.alfredapp.com" target="_blank" rel="external">Alfred</a>。无法形容。。。什么都能干。。。关键是可以自己扩展。</li>
<li><a href="http://www.hammerspoon.org" target="_blank" rel="external">HammerSpoon</a>。有点类似alfred的workflow，但是更面向程序员，更加强大，可以自己写lua脚本控制系统的方方面面，相当于系统和用户的一个中间层。用的好的话，hammerspoon可以代替其他很多程序。</li>
<li><a href="http://vagrantup.com" target="_blank" rel="external">Vagrant</a>。本质上是一个虚拟机管理工具。我想搭一些测试环境，但又不想弄乱本身的系统。本来想用docker的，更轻量级，也更流行。但mac不支持lxc，不能直接用docker，很蛋疼，要通过一个虚拟机中转，不如直接用vagrant了。</li>
</ul>
<p>还有更多神奇的东西。不一一介绍了。</p>
<h1 id="hexo历史版本">hexo历史版本</h1>
<p>在mac上写blog，安装hexo的过程中碰到一些问题。由于我用的主题与hexo最新版不兼容，要安装2.8.3版本：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">npm <span class="operator"><span class="keyword">install</span> -g hexo@<span class="number">2.8</span><span class="number">.3</span> <span class="comment">--registry=http://r.cnpmjs.org</span></span></div></pre></td></tr></table></figure>

<p>加上registry参数是因为国外的镜像很慢。<br>记录下。</p>
<h1 id="python和ruby">python和ruby</h1>
<p>最近想挑一门动态语言深入学习，在python和ruby之间犹豫了下。<br>二者其实我之前都有接触过，但浅尝辄止。<br>python用来写简单的运维脚本，还折腾过一段时间Django。<br>ruby折腾Jekyll时了解过一点。</p>
<p>ruby很像perl，让我不爽。我对perl是敬而远之，能写perl的都是大神。<br>但又听说ruby有很多好玩的特性，学ruby能开阔眼界，尤其对我这样用惯了java的，有点心动。<br>ROR也很让人流口水。。。<br>以前一直以为ruby大部分是做web开发，但ruby也能实现brew这种神器，顿时对ruby改观了。就像我第一次知道nodejs，知道js也可以写后端一样。</p>
<p>但python似乎应用更广泛，类库更多。<br>而且spark官方提供python接口。<br>蛋疼的是python2和python3，不知道社区怎么搞成这样的。。。</p>
<p>至于网上其他人讨论的缩进／end／效率什么的，对我而言都不是事。<br>我觉得选择一门语言，主要看它的“哲学”。<br>语言能改造我思考问题的方式。</p>
<p>继续纠结。。。</p>
<h1 id="关于读书">关于读书</h1>
<p>以前我很喜欢买纸质书，虽然完整看完的不多。。。<br>后来就更喜欢电子书了，因为纸质书搬家太麻烦了。。。<br>最近在看《Hadoop Application Architectures》，非常赞，近期写个读书笔记吧。<br>希望以后还能有充足的时间看书吧。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>挺久没写了。一直忙着各种杂事。<br>最近稍微安定下来了，胡乱写些最近的想法吧。</p>
]]>
    
    </summary>
    
      <category term="杂谈" scheme="http://jxy.me/tags/%E6%9D%82%E8%B0%88/"/>
    
      <category term="hexo" scheme="http://jxy.me/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[谢谢，再见]]></title>
    <link href="http://jxy.me/2015/08/05/goodbye-netease/"/>
    <id>http://jxy.me/2015/08/05/goodbye-netease/</id>
    <published>2015-08-05T14:54:04.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>离开网易，感觉就像离开国企下海了。。。<br>吐槽是种病啊。。。</p>
<a id="more"></a>
<p>虽然是我自己的决定，虽然早有心理准备，还是有些伤感的。<br>毕竟呆了4年，学到了很多东西，认识了很多人。</p>
<p>我很幸运，以网易作为职业生涯的起点。<br>更幸运的是，抱着感激之心离开。</p>
<p>遗憾还是有的，有些事情本来可以做的更好。<br>希望接手我代码的同学不要打我吧。。。<br>希望他们看到我各种奇葩注释时，笑一笑。<br>希望后台能发展的更好。</p>
<p>我也想豪爽的说句“青山不改，绿水长流”。<br>但好像不太符合我的风格。</p>
<p>总之，谢谢网易，谢谢所有帮助过我的人，恕我不能一一道别了。<br>我们有缘再会。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>离开网易，感觉就像离开国企下海了。。。<br>吐槽是种病啊。。。</p>
]]>
    
    </summary>
    
      <category term="杂谈" scheme="http://jxy.me/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[面试总结]]></title>
    <link href="http://jxy.me/2015/07/19/interview-summary/"/>
    <id>http://jxy.me/2015/07/19/interview-summary/</id>
    <published>2015-07-19T08:48:04.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>去面试了一些公司，稍微总结下。</p>
<a id="more"></a>
<p>面试的过程不只是公司考察被试者，被试者也要考察公司，判断一家公司是否值得去。这是一个双向选择。无论大公司小公司、上市的还是创业的、国内的还是国外的，都需要这样一个判断过程。<br>其实这个判断是很感性的，有时候就是喜欢/不喜欢一家公司，没什么规则。而且每个人都不一样。比如我，如果能去google，可能不要钱也会去。。。毕竟那里可以说是大数据的发源地，很想去看看。有点像旅游观光。。。</p>
<p>但我还是尝试总结下，主要有以下几个因素（按重要性递减）：</p>
<ol>
<li>和你一起共事的人。这是最最重要的。包括主管/同组同事/其他组的人。如果能有技术大牛带你，最好不过。如果面试过程中觉得对方技术很渣，或者人品有问题，赶紧走。我碰到过一些面试官让我觉得很不舒服，要么不懂装懂，问一些假大空的问题（XXX的价值是什么？）；要么态度很差，居高临下。我觉得和这些人共事不会开心。</li>
<li>HR给人的感觉。重要性仅次于同事。因为HR往往代表着一家公司的风格/价值观，真的是一家公司的脸面。有些HR让你觉得很舒服，有些HR则会各种挑刺。我说我喜欢一个人码代码（没有码农喜欢写代码时被打断吧），有的HR觉得这是一种钻研精神，有的HR觉得这是你不懂合作。。。而且某些HR会诱导你说出一些对你不利的话，好压低对你的评价。说到价值观，我一直觉得应该是求同存异，但某些HR/公司一定要强行灌输价值观，如果不能认可，进去也不会开心。</li>
<li>工作的内容。上面两条是看人，这条是看事。进去之后做什么工作？会不会学到一些新的技术？会不会有成长？尽量是核心的业务、核心的部门。一些边缘的部门呆着可能会比较痛苦。</li>
<li>氛围。这是比较虚的一个东西，但各个公司的氛围真的是有差别的。面试的时候我一般会尽量多逛逛工作区，看看员工真实工作的样子。有些公司明显会比较压抑，大家交流很少，员工看着就像苦大仇深一样。。。有些公司等级森严，各种制度太多。我希望是一个比较自由，大家可以畅快交流的工作环境。如果有认识的人在里面，一定要多打听下这方面。</li>
<li>最后才是可以量化的一些指标：薪资、期权、福利之类的。说实话，如果一家公司真的让我觉得很舒服，即使工资低我也愿意去。</li>
</ol>
<p>以上算是一些简单的总结吧，只适用于我个人。当我面临多个机会时，我会按这些条件去筛选。</p>
<p>反过来想想，我也曾作为面试官去面一些人。如果按上面的判断标准，不知道我是否合格。。。我给人的感觉是什么样的呢。。。<br>不过我只会问一些技术问题，而且是比较细节的。我最常问的就是shuffle/二次排序。。。应该不会让对方不舒服吧。。。</p>
<p>胡言乱语，博君一笑。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>去面试了一些公司，稍微总结下。</p>
]]>
    
    </summary>
    
      <category term="杂谈" scheme="http://jxy.me/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[HBase豆知识]]></title>
    <link href="http://jxy.me/2015/07/06/hbase-tips/"/>
    <id>http://jxy.me/2015/07/06/hbase-tips/</id>
    <published>2015-07-06T06:26:34.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>近日重看了《HBase权威指南》，结合着0.98.8的代码，总结一些知识点。<br>由于hbase版本更新很快，而且每个版本变化都很大，本文不一定适用于其他版本。<br>我们是0.94和0.98混着用的，也可能有些0.94的知识点混在里面。。。</p>
<p>话说，<code>hadoop</code>还是<code>Hadoop</code>我都觉得挺正常，但<code>hbase</code>就不如<code>HBase</code>顺眼。。。</p>
<a id="more"></a>
<h1 id="B+树和LSM树">B+树和LSM树</h1>
<p>太理论的我也不懂。<br>B+树是传统RDBMS中实现索引的关键。特点是数据都在叶节点，而且查找操作非常高效。<br>但更新代价比较大，可能导致叶节点的分裂。更新索引的时间可能比真正写数据的时间还长，一些大表的索引比数据还大。<br>而且对磁盘的依赖较大，因为机械磁盘的随机读写性能都是比较差的。所以RDBMS都在往SSD发展。。。<br>较适合读多写少的情况。</p>
<p><a href="http://www.cnblogs.com/siegfang/archive/2013/01/12/lsm-tree.html" target="_blank" rel="external">LSM树</a>的核心思想在于延迟更新，会将数据/索引的更新暂时以日志的形式记录下来，等待后台线程去合并。<br>可以将随机写转化为顺序写（将update和delete都转化为insert），所以不像B+树那样受机械硬盘的限制。<br>特别适合大量写入的情况。读性能也不错，但storefile过多的话，估计读性能会下降比较快。<br>由于需要后台线程合并，有额外开销。当这种额外开销超过带来的性能收益时，就不值得了。也是一种trade-off。</p>
<h1 id="关于hlog">关于hlog</h1>
<p>注意hlog是regionserver级别的。所有region的日志都会写到一个文件中。当需要回复的时候，按不同的region拆分hlog。拆分好后，region才会开始回放日志。这个拆分的过程可能会非常慢，因为日志文件没有任何索引，只能从头遍历。<br>这种设计的前提是需要hlog拆分的情况比较少，可以将所有hlog写入转换为顺序写，提升性能。如果每个region维护一个hlog，可能造成大量随机写。</p>
<p>hlog其实就是hadoop的sequence file，其中key是HLogKey对象，包括region，tablename、sequence id、时间戳等信息；value是WALEdit对象。一个WALEdit对象中可以包括多KeyValue对象（这应该是为了保证行级别的原子性。如果更新一行中的多个列，会产生多个KeyValue对象，但在hlog中只保存一条记录）。</p>
<p>hlog对于写入影响很大，所以可以关闭wal或者延迟刷新wal以提升性能。</p>
<p>日志文件滚动有2种情况：</p>
<ol>
<li>regionserver中的LogRoller进程每一小时触发一次日志滚动。</li>
<li>当日志文件达到hbase.regionserver.hlog.blocksize大小时，触发日志滚动。</li>
</ol>
<p>每次日志滚动的时候，都会触发一次对oldlog的检查。这个检查有2种情况：</p>
<ol>
<li>遍历所有hlog file，如果某个hlog file中最大的sequence number小于所有store file中最大的sequence number，说明对应的hlog file中的记录已经全部持久化了，这个hlog file可以被删除了。所谓删除也不是马上删除，而是移动到一个临时目录（0.94是/hbase/.oldlogs），等待master中的一个LogCleaner线程来删除。相关逻辑见FSHLog.rollWriter方法。</li>
<li>如果hlog file文件数量大于hbase.regionserver.maxlogs，就遍历最老的一个hlog，找到哪条记录还没有被持久化，强制相应的region做一次flush，然后将最老的文件移动到临时目录等待删除。</li>
</ol>
<h1 id="hlog拆分">hlog拆分</h1>
<p>当集群启动或regionserver挂掉时，都需要拆分/回放日志。<br>以0.94 hbase为例。<br>所有hlog都存在/hbase/.logs/${regionserver.id}目录下。当回放hlog时，hmaster会按顺序遍历这个目录下所有文件，将对应region的日志放到一个临时目录/hbase/splitlog/${regionserver.id}/${region.name}。当一个hlog拆分完毕后，对应的文件会移动到/hbase/.oldlogs，等待master中的线程去定期删除。当所有hlog文件拆分完毕后，将拆分后的日志移动到/hbase/${table.name}/${region.name}/recovered.edits目录，然后打开region。region打开时如果发现recovered.edits目录，就会回放其中的日志，回放完毕才能对外服务。</p>
<p>hlog拆分机制经历过很多变化，从早期版本的单线程拆分，到多线程，到目前的分布式处理。</p>
<h1 id="关于HA">关于HA</h1>
<p>hbase没有真正的实现HA。根据上面的分析，当一个regionserver挂掉后，要经历很长时间的hlog拆分、回放过程。<br>拆分log时，会遍历/hbase/.logs下所有文件，日志越多恢复服务所需时间也越长。<br>所以及时删除log是很必要的，删除机制见我上面的分析。</p>
<h1 id="region_split">region split</h1>
<p>用户可以设置自己的split策略，我们一般用ConstantSizeRegionSplitPolicy，同时将hbase.hregion.max.filesize设为一个比较大的值，这样可以手动控制split的时机。注意这个属性限定的是一个store的大小，而不是整个region的大小。<br>split过程非常快，因为只是新建一些引用文件，当一个引用文件对应的数据被compact处理后，才会删除引用文件。<br>当一个region中存在引用文件时，不能再次split。</p>
<h1 id="触发compact检查的几种情况">触发compact检查的几种情况</h1>
<p>compact是hbase里特别折腾人的机制之一。<br>有几种操作会触发compact检查：</p>
<ol>
<li>memstore flush</li>
<li>在hbase shell里执行compact、major_compact命令</li>
<li>调用HAdmin类的相应方法</li>
<li>regionserver中有一个线程CompactionChecker，默认每10秒执行一次，检查所有online的region</li>
</ol>
<p>每次触发compact检查后，再判断是minor还是major（用户手动触发的major_compact除外）。<br>compact的临时数据会写到/hbase/${table.name}/${region.name}/.tmp目录中。</p>
<p>RegionServer中有一个对象CompactSplitThread负责compact/split/merge region。这货虽然叫XXThread但其实不是线程。。。<br>每个compact是否是major，由CompactionPolicy.isMajorCompaction方法决定。<br>默认的RatioBasedCompactionPolicy只会检查hbase.hregion.majorcompaction属性。<br>所以这个属性设为0之后就不会触发major compact。但由minor提升而来的major还是存在的（如果某次minor compact选择的storefile就是当前region的所有storefile，就会提升为major compcat，只会出现在写入较少的表上）。<br>每次compact后会再检查一次是否要split。</p>
<h1 id="关于flush与compcat">关于flush与compcat</h1>
<p>每次flush后，都会检查是否需要split、是否需要compact，见MemStoreFlusher类。<br>如果要split，就直接split；否则再检查是否要compact。是否compact由StoreEngine.needsCompcation方法决定。<br>默认是DefaultStoreEngine，其实是交给RatioBasedCompactionPolicy.needsCompaction方法决定。<br>我以前以为split后会立刻触发compact，看来不是。</p>
<h1 id="关于large和small线程池">关于large和small线程池</h1>
<p>CompactSplitThread中有两个线程池：large和small。<br>如果一次compact要处理的数据量大于hbase.regionserver.thread.compaction.throttle，就进入large线程池。否则进入small线程池。<br>这个large/small和是否major compact没有必然联系。不知为何要这样设计。</p>
<h1 id="关于hfile">关于hfile</h1>
<p>HFile格式：<br><img src="/2015/07/06/hbase-tips/1.png" alt=""></p>
<p>KeyValue格式：<br><img src="/2015/07/06/hbase-tips/2.png" alt=""></p>
<p>HFile一旦写入完成，就是不可变的。因为hdfs要修改已经存在的文件只能append，而HFile元数据在末尾，不能直接append。<br>DataIndex和MetaIndex是类似于B+树的索引结构。RegionServer启动的时候，会将所有索引加载到内存里，便于后续查找。<br>这个索引只能到块的级别（索引了每个块的rowid范围），同一个块内的KV是没索引的，只是按Key排序。所以想找特定的KV时，可能要遍历整个块。<br>注意只有rowid是有索引的，而且对于column family、column qualifier是没索引的。</p>
<p>hbase默认的块大小是64KB。写入的数据量大于64KB后，会生成一个新的块并写入。如果开启了压缩，写入的数据一般小于64KB。如果写入一个特别大的KV，也可能大于64KB。<br>hbase的块和hdfs的块没有任何关系。</p>
<p>KeyValue其实很多信息都是冗余的。比如column family，在同一个storefile中肯定全都是一样的。<br>为了减少查询开销，节省空间，务必选择名称较短的列族和列。</p>
<h1 id="hbase读路径">hbase读路径</h1>
<p>说说自己的理解，没有看代码求证过。</p>
<ol>
<li>如果读请求中包含时间戳条件，根据storefile的时间戳排除一些文件。</li>
<li>如果能应用bloom filter，又可以排除一些storefile。</li>
<li>在剩下的storefile和memstore里扫描所需数据。由于storefile和memstore是有rowid索引的，可以快速选出一批备选storefile。</li>
<li>扫描备选的storefile，定位到所需的block。遍历block，找到所需的KeyValue。在这个扫描过程中，还要加上其他一些过滤条件，比如特定的列名。</li>
<li>猜测扫描顺序是先memstore，再按时间降序扫描所有storefile。这样可以取得最新的数据，跟踪到同一份数据的update/delete。</li>
</ol>
<h1 id="关于二级索引">关于二级索引</h1>
<p>hbase中只有一种索引：rowkey（转换成byte[]按升序排列）。而如果需要多种索引就比较麻烦。</p>
<ol>
<li>存多份数据，并建不同的rowkey格式。缺点是数据量放大很多倍。</li>
<li>用户自己维护一个索引表。其实不一定是索引表，也可能是用于索引的列族。这种方式很难保证原子性，很可能数据表已经更新，但索引表还没更新（索引表可能是用MR定期更新）。而且用户写代码时也要自己处理索引表相关逻辑。</li>
<li>coprocessor。不是很了解，我的理解是将方法2中用户手动的操作转换为服务端自动的操作。</li>
</ol>
<h1 id="关于bloom_filter">关于bloom filter</h1>
<p>相关原理见<a href="http://blog.csdn.net/jiaomeng/article/details/1495500" target="_blank" rel="external">这篇文章</a>。<br>bloom filter对get操作很有效，因为可以快速判断一个rowkey是否在一个storefile里。<br>但scan操作不一定有效，因为scan是个范围，bloom filter无法判断这个范围的所有rowkey是否都在某个storefile里。</p>
<p>在0.98中建表是默认都开启了ROW级别的BloomFilter。<br>ROW级别的BloomFilter对Scan一定是无效的。<br>ROWCOL级别的BloomFilter对包含列条件的Scan可能有效。</p>
<p>所以bloom filter是否适用还是要看自己的使用场景。</p>
<h1 id="常见服务端优化">常见服务端优化</h1>
<ol>
<li>JVM调优。regionserver对内存非常敏感。目标：减少老年代的内存碎片，尽量让对象在新生代就死亡；减少FGC，否则可能触发zk超时。</li>
<li>开启MSLAB特性。默认是开启的，但可能浪费一些内存。</li>
<li>开启snappy压缩。snappy压缩优于lzo。</li>
<li>手动管理split和compcat、监控storefile数量。这也是无奈之举。尽量在流量低的时候做这些操作。</li>
<li>分散热点，将写入量较大的region分散到不同regionserver。</li>
</ol>
<h1 id="常见客户端优化">常见客户端优化</h1>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 客户端务必关闭autoFlush，这样数据会批量提交</span></div><div class="line">table.setAutoFlush(<span class="literal">false</span>,<span class="literal">false</span>);</div><div class="line"> </div><div class="line"><span class="comment">// 写数据时，对于不重要的数据可以异步写入WAL，可以提升性能</span></div><div class="line">put.setDurability(Durability.ASYNC_WAL);</div><div class="line"> </div><div class="line"><span class="comment">// 写数据时，对于不重要的数据可以关闭WAL</span></div><div class="line">put.setDurability(Durability.SKIP_WAL);</div><div class="line"> </div><div class="line"><span class="comment">// scan时，加上尽可能多的条件，不光starkey、endkey，如果有column family、column qualifier，也尽量加上</span></div><div class="line"><span class="comment">// 能加filter的话也尽量加上。这样可以让服务端过滤更多数据</span></div><div class="line"> </div><div class="line"><span class="comment">// scan时，设置cache和batch，减少RPC次数</span></div><div class="line"><span class="comment">// cache控制每次RPC请求返回多少行，batch控制每行最多返回多少列</span></div><div class="line"><span class="comment">// 在程序里设置cache和hbase.client.scanner.caching属性是一样的效果</span></div><div class="line"><span class="comment">// 参考http://m.blog.csdn.net/blog/jiaomicha/23871123</span></div><div class="line"> </div><div class="line"><span class="comment">// scan返回的ResultScanner，记得close</span></div><div class="line"> </div><div class="line"><span class="comment">// scan时，只返回需要的数据，减少网络流量</span></div><div class="line"><span class="comment">// 如果只需要某一列的，那就把列加为scan的条件</span></div><div class="line"><span class="comment">// 如果只需要rowkey，不需要任意列的数据，可以加filter只返回rowkey</span></div><div class="line"> </div><div class="line"><span class="comment">// 根据需要使用HTablePool</span></div><div class="line"><span class="comment">// 注意HTable对象不是线程安全的</span></div></pre></td></tr></table></figure>

<p>建表时的一些优化：</p>
<ol>
<li>尽量只有一个列族</li>
<li>预分区</li>
<li>列族和列的名字尽量短</li>
<li>尽量开启snappy/lzo压缩</li>
<li>开启bloom filter（0.98默认会开启ROW级别的，如果有特殊需求可以开启ROWCOL级别的）</li>
</ol>
<h1 id="服务端比较重要的一些配置项">服务端比较重要的一些配置项</h1>
<p>比较容易出问题的一些配置。</p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>zookeeper.session.timeout</td>
<td>zk客户端的超时设置，也受服务端限制</td>
</tr>
<tr>
<td>hbase.regionserver.handler.count</td>
<td>rpc handler用于处理客户端读写。当每次RPC操作数据量较小时，这个数字可以设的大一点，否则可能对regionserver产生较大内存压力，进而造成GC问题。要结合自己的内存大小来设置。</td>
</tr>
<tr>
<td>hbase.regionserver.maxlogs</td>
<td>每个regionserver保存的日志文件数量，不要太大，不然回放日志时间很长</td>
</tr>
<tr>
<td>hbase.regionserver.hlog.blocksize</td>
<td>每个hlog日志文件的大小，不要太大，不然回放日志时间很长</td>
</tr>
<tr>
<td>hbase.hstore.blockingStoreFiles</td>
<td>当某个store中storefile数量超过这个值时，整个region就会阻止继续写入，等待后台合并</td>
</tr>
<tr>
<td>hbase.hstore.blockingWaitTime</td>
<td>跟上一个配置相关。每次阻止写入多长时间。</td>
</tr>
<tr>
<td>hbase.hregion.memstore.block.multiplier</td>
<td>如果客户端写入过快来不及flush，memstore最多可以增加到几倍，之后会阻止写入</td>
</tr>
<tr>
<td>hbase.hregion.max.filesize</td>
<td>每个region的大小，超过这个大小会触发自动split，只有当hbase.regionserver.region.split.policy设置为ConstantSizeRegionSplitPolicy时才有效</td>
</tr>
<tr>
<td>hbase.regionserver.regionSplitLimit</td>
<td>当一个regionserver上region数量达到这个数字，就不会自动split了。但还是可以手动触发。</td>
</tr>
<tr>
<td>hbase.hregion.majorcompaction</td>
<td>major compact的周期，设为0可以关闭自动major compact。跟compaction policy有关，默认是RatioBasedCompactionPolicy</td>
</tr>
<tr>
<td>hbase.hstore.compaction.min</td>
<td>每次minor compact，最少选择多少个storefile？</td>
</tr>
<tr>
<td>hbase.hstore.compaction.max</td>
<td>每次minor compact，最多选择多少个storefile？</td>
</tr>
<tr>
<td>hbase.hstore.compaction.max.size</td>
<td>minor compact时，大于这大小的storefile会被排除</td>
</tr>
<tr>
<td>hbase.regionserver.thread.compaction.large</td>
<td>compact时large线程池的大小</td>
</tr>
<tr>
<td>hbase.regionserver.thread.compaction.small</td>
<td>compact时small线程池的大小</td>
</tr>
<tr>
<td>hbase.regionserver.thread.compaction.throttle</td>
<td>如果一次compact（不分minor/major）处理的数据量大于这个值，进入large线程池；否则进入small线程池。</td>
</tr>
</tbody>
</table>
]]></content>
    <summary type="html">
    <![CDATA[<p>近日重看了《HBase权威指南》，结合着0.98.8的代码，总结一些知识点。<br>由于hbase版本更新很快，而且每个版本变化都很大，本文不一定适用于其他版本。<br>我们是0.94和0.98混着用的，也可能有些0.94的知识点混在里面。。。</p>
<p>话说，<code>hadoop</code>还是<code>Hadoop</code>我都觉得挺正常，但<code>hbase</code>就不如<code>HBase</code>顺眼。。。</p>
]]>
    
    </summary>
    
      <category term="hbase" scheme="http://jxy.me/tags/hbase/"/>
    
      <category term="豆知识" scheme="http://jxy.me/tags/%E8%B1%86%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[2.5.2中queueMaxAppsDefault的一个bug]]></title>
    <link href="http://jxy.me/2015/07/06/scheduler-diff-2-5-2/"/>
    <id>http://jxy.me/2015/07/06/scheduler-diff-2-5-2/</id>
    <published>2015-07-06T02:51:09.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>升级hadoop 2.5.2过程中碰到的一个问题。<br>很难说是个bug，更像是功能上的一些变化，社区也没有相关jira。<br>看看代码研究下。<br>调度器的基本概念见<a href="/2015/04/30/yarn-resource-scheduler/">我以前的文章</a>。</p>
<a id="more"></a>
<h1 id="症状">症状</h1>
<p>在我们的fair-scheduler.xml中一直有这样一个属性：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">queueMaxAppsDefault</span>&gt;</span>2<span class="tag">&lt;/<span class="title">queueMaxAppsDefault</span>&gt;</span></div></pre></td></tr></table></figure>

<p>本意是将默认值设小一点，防止一些意外情况。在2.2.0中时，一直正常使用。<br>但升级到2.5.2后，发现这个设置变成了全局的，变成了整个集群只能并发执行2个任务。<br>删掉这个属性后就正常了。</p>
<p>我们的队列配置类似这样：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">allocations</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="title">queue</span> <span class="attribute">name</span>=<span class="value">"a"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">minResources</span>&gt;</span>10240 mb, 10 vcores<span class="tag">&lt;/<span class="title">minResources</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">maxResources</span>&gt;</span>409600 mb, 200 vcores<span class="tag">&lt;/<span class="title">maxResources</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">maxRunningApps</span>&gt;</span>20<span class="tag">&lt;/<span class="title">maxRunningApps</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">weight</span>&gt;</span>1.0<span class="tag">&lt;/<span class="title">weight</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">schedulingPolicy</span>&gt;</span>fair<span class="tag">&lt;/<span class="title">schedulingPolicy</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">aclSubmitApps</span>&gt;</span>a<span class="tag">&lt;/<span class="title">aclSubmitApps</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">aclAdministerApps</span>&gt;</span>hadoop,yarn,a<span class="tag">&lt;/<span class="title">aclAdministerApps</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="title">queue</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="title">queue</span> <span class="attribute">name</span>=<span class="value">"b"</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">minResources</span>&gt;</span>10240 mb, 10 vcores<span class="tag">&lt;/<span class="title">minResources</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">maxResources</span>&gt;</span>409600 mb, 200 vcores<span class="tag">&lt;/<span class="title">maxResources</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">maxRunningApps</span>&gt;</span>20<span class="tag">&lt;/<span class="title">maxRunningApps</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">weight</span>&gt;</span>1.5<span class="tag">&lt;/<span class="title">weight</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">schedulingPolicy</span>&gt;</span>fair<span class="tag">&lt;/<span class="title">schedulingPolicy</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">aclSubmitApps</span>&gt;</span>b<span class="tag">&lt;/<span class="title">aclSubmitApps</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">aclAdministerApps</span>&gt;</span>b,hadoop,yarn<span class="tag">&lt;/<span class="title">aclAdministerApps</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="title">queue</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">allocations</span>&gt;</span></div></pre></td></tr></table></figure>

<p>队列结构只有2层。一个root队列，其他都是LeafQueue。并且为每个LeafQueue单独配置了maxRunningApps属性。</p>
<h1 id="2-2-0的代码">2.2.0的代码</h1>
<p>2.2.0中，队列相关配置由QueueManager类管理。包括加载fair-scheduler.xml、更新配置等等。</p>
<p>FairScheduler中有一个线程UpdateThread，默认每0.5秒调用一次update方法：</p>
<figure class="highlight java"><figcaption><span>FairScheduler.java</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">   <span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">update</span>() {</div><div class="line">       <span class="comment">// 检查是否要重新加载fair-scheduler.xml</span></div><div class="line">	queueMgr.reloadAllocsIfNecessary(); <span class="comment">// Relaod alloc file</span></div><div class="line">	<span class="comment">// 关键的一个方法，计算每个队列是否超出了资源限制、app数量限制</span></div><div class="line">	<span class="comment">// 如果已经超出限制，所有的app的runnable属性会被设为false，不会再分配container</span></div><div class="line">	updateRunnability(); <span class="comment">// Set job runnability based on user/queue limits</span></div><div class="line"></div><div class="line">	<span class="comment">// 接下来的是抢占式调度的一些逻辑，不是本文重点</span></div><div class="line">	updatePreemptionVariables(); <span class="comment">// Determine if any queues merit preemption</span></div><div class="line">	FSQueue rootQueue = queueMgr.getRootQueue();</div><div class="line"></div><div class="line">	<span class="comment">// Recursively update demands for all queues</span></div><div class="line">	rootQueue.updateDemand();</div><div class="line"></div><div class="line">	rootQueue.setFairShare(clusterCapacity);</div><div class="line">	<span class="comment">// Recursively compute fair shares for all queues</span></div><div class="line">	<span class="comment">// and update metrics</span></div><div class="line">	rootQueue.recomputeShares();</div><div class="line">}</div></pre></td></tr></table></figure>

<p>继续看updateRunnability方法：</p>
<figure class="highlight java"><figcaption><span>FairScheduler.java</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">   <span class="keyword">private</span> <span class="keyword">void</span> <span class="title">updateRunnability</span>() {</div><div class="line">	List&lt;AppSchedulable&gt; apps = <span class="keyword">new</span> ArrayList&lt;AppSchedulable&gt;();</div><div class="line"></div><div class="line">	<span class="comment">// Start by marking everything as not runnable</span></div><div class="line">	<span class="comment">// 注意这里，这里只处理了所有leafQueue，对于parentQueue，没有考虑maxApp限制</span></div><div class="line">	<span class="keyword">for</span> (FSLeafQueue leafQueue : queueMgr.getLeafQueues()) {</div><div class="line">		<span class="keyword">for</span> (AppSchedulable a : leafQueue.getAppSchedulables()) {</div><div class="line">			a.setRunnable(<span class="keyword">false</span>);</div><div class="line">			apps.add(a);</div><div class="line">		}</div><div class="line">	}</div><div class="line">	<span class="comment">// Create a list of sorted jobs in order of start time and priority</span></div><div class="line">	Collections.sort(apps, <span class="keyword">new</span> FifoAppComparator());</div><div class="line">	<span class="comment">// Mark jobs as runnable in order of start time and priority, until</span></div><div class="line">	<span class="comment">// user or queue limits have been reached.</span></div><div class="line">	Map&lt;String, Integer&gt; userApps = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</div><div class="line">	Map&lt;String, Integer&gt; queueApps = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</div><div class="line"></div><div class="line">	<span class="keyword">for</span> (AppSchedulable app : apps) {</div><div class="line">		String user = app.getApp().getUser();</div><div class="line">		String queue = app.getApp().getQueueName();</div><div class="line">		<span class="keyword">int</span> userCount = userApps.containsKey(user) ? userApps.get(user) : <span class="number">0</span>;</div><div class="line">		<span class="keyword">int</span> queueCount = queueApps.containsKey(queue) ? queueApps</div><div class="line">				.get(queue) : <span class="number">0</span>;</div><div class="line">		<span class="comment">// 这里获得的queueMaxApps就是我们在xml文件里为每个队列配置的maxRunningApps属性</span></div><div class="line">		<span class="comment">// 如果某个队列没有配置，就返回queueMaxAppsDefault</span></div><div class="line">		<span class="keyword">if</span> (userCount &lt; queueMgr.getUserMaxApps(user)</div><div class="line">				&& queueCount &lt; queueMgr.getQueueMaxApps(queue)) {</div><div class="line">			userApps.put(user, userCount + <span class="number">1</span>);</div><div class="line">			queueApps.put(queue, queueCount + <span class="number">1</span>);</div><div class="line">			app.setRunnable(<span class="keyword">true</span>);</div><div class="line">		}</div><div class="line">	}</div><div class="line">}</div></pre></td></tr></table></figure>

<p>只有当NM发来心跳时才能分配container，分配过程是一次DFS。<br>当一个leafQueue下所有app的runnable都是false时，不会分配任何container。<br>具体代码不列了。见FairScheduler.nodeUpdate和FSParentQueue.assignContainer。</p>
<p>可见，对于root队列，FairScheduler没有检查任何限制条件。</p>
<h1 id="2-5-2的代码">2.5.2的代码</h1>
<p>2.5.2的FairScheduler代码变化非常大。我目前看到的几个：</p>
<ol>
<li>处理fair-scheduler.xml的逻辑从QueueMananger类剥离，独立作为一个service：AllocationFileLoaderService。</li>
<li>FairScheduler handle的事件增加了很多APP_ATTEMPT相关的，见handle方法。相应的app提交过程也变化了。</li>
<li>maxApp相关限制的判断独立为一个类：MaxRunningAppsEnforcer。</li>
</ol>
<p>还有很多变化是为了配合RM HA的。</p>
<p>提交app后会触发addApplicationAttempt方法（相关的事件链不列了。这个方法也是2.5.2新增的）</p>
<figure class="highlight java"><figcaption><span>FairScheduler.java</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">addApplicationAttempt</span>(</div><div class="line">      ApplicationAttemptId applicationAttemptId,</div><div class="line">      <span class="keyword">boolean</span> transferStateFromPreviousAttempt,</div><div class="line">      <span class="keyword">boolean</span> shouldNotifyAttemptAdded) {</div><div class="line">    <span class="comment">// 省略</span></div><div class="line">    <span class="comment">// 关键在这里，提交app时会判断这个app是否runnable</span></div><div class="line">    <span class="comment">// runnable=false的app会一直保持在ACCEPTED状态，不会分配任何container（包括AM Container）</span></div><div class="line">    <span class="keyword">boolean</span> runnable = maxRunningEnforcer.canAppBeRunnable(queue, user);</div><div class="line">    <span class="comment">// 将app加到队列中，会更改app的状态</span></div><div class="line">    queue.addApp(attempt, runnable);</div><div class="line">    <span class="comment">// 下面是一些统计信息</span></div><div class="line">    <span class="keyword">if</span> (runnable) {</div><div class="line">      maxRunningEnforcer.trackRunnableApp(attempt);</div><div class="line">    } <span class="keyword">else</span> {</div><div class="line">      maxRunningEnforcer.trackNonRunnableApp(attempt);</div><div class="line">    }</div><div class="line">    <span class="comment">// 省略</span></div><div class="line">}</div></pre></td></tr></table></figure>

<p>继续看canAppBeRunnable方法：</p>
<figure class="highlight java"><figcaption><span>MaxRunningAppsEnforcer.java</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">canAppBeRunnable</span>(FSQueue queue, String user) {</div><div class="line">  AllocationConfiguration allocConf = scheduler.getAllocationConfiguration();</div><div class="line">  <span class="comment">// 获得当前用户RUNNING的app数量</span></div><div class="line">  Integer userNumRunnable = usersNumRunnableApps.get(user);</div><div class="line">  <span class="keyword">if</span> (userNumRunnable == <span class="keyword">null</span>) {</div><div class="line">    userNumRunnable = <span class="number">0</span>;</div><div class="line">  }</div><div class="line">  <span class="keyword">if</span> (userNumRunnable &gt;= allocConf.getUserMaxApps(user)) {</div><div class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">  }</div><div class="line">  <span class="comment">// Check queue and all parent queues</span></div><div class="line">  <span class="comment">// 关键在这里，会一直回溯到root队列，判断appMax</span></div><div class="line">  <span class="comment">// 而我们对root队列没有单独配置maxRunningApps属性，就会返回queueMaxAppsDefault</span></div><div class="line">  <span class="keyword">while</span> (queue != <span class="keyword">null</span>) {</div><div class="line">    <span class="keyword">int</span> queueMaxApps = allocConf.getQueueMaxApps(queue.getName());</div><div class="line">    <span class="keyword">if</span> (queue.getNumRunnableApps() &gt;= queueMaxApps) {</div><div class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">    }</div><div class="line">    queue = queue.getParent();</div><div class="line">  }</div><div class="line"></div><div class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">}</div></pre></td></tr></table></figure>

<p>可见在2.5.2中，maxApp的限制对所有队列都生效，不区分ParentQueue和LeafQueue。<br>所以造成了整个集群只能运行2个任务的情况。</p>
<h1 id="总结">总结</h1>
<p>很难说那种方式更好。<br>2.5.2里的FairScheduler，有点像CapacityScheduler了，父队列会限制子队列。<br>而2.2.0里的FairScheduler，父队列和子队列基本没关系（除了ACL）。<br>之前测试的时候，没有测过并发执行多个任务，直到线上升级才发现这个问题，也是考虑不周。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>升级hadoop 2.5.2过程中碰到的一个问题。<br>很难说是个bug，更像是功能上的一些变化，社区也没有相关jira。<br>看看代码研究下。<br>调度器的基本概念见<a href="/2015/04/30/yarn-resource-scheduler/">我以前的文章</a>。</p>
]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://jxy.me/tags/hadoop/"/>
    
      <category term="fair scheduler" scheme="http://jxy.me/tags/fair-scheduler/"/>
    
      <category term="resourcemanager" scheme="http://jxy.me/tags/resourcemanager/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[闲言碎语]]></title>
    <link href="http://jxy.me/2015/06/27/some-gossip/"/>
    <id>http://jxy.me/2015/06/27/some-gossip/</id>
    <published>2015-06-27T06:34:13.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>一些想法，一些牢骚。<br>胡言乱语，博君一笑。</p>
<a id="more"></a>
<h1 id="关于技术">关于技术</h1>
<p>我以前是一个纯粹的唯技术论者，总是觉得技术是第一位的。可能这是学院派的通病吧。<br>但工作了一段时间，看到了一些事情，深刻觉得很多时候，<strong>技术真的没那么重要</strong>。<br>一个成功的产品，需要创意、运营、市场。。。各种各样的努力，其中技术的比重很小。<br>尤其是运营，我现在越发觉得好的运营可以弥补很多其他方面的不足。</p>
<p>有哪些公司、产品，是真正把技术创新转换为成功的？<br>也许google的PageRank算是吧。<br>facebook？twitter？我觉得这是靠创意成功的。他们的技术是在体量大了之后才慢慢发展的。<br>国内的公司更不用说了。</p>
<p>有些时候，我们设计一些很复杂的架构，用一些很新的技术。能单机解决的问题偏要分布式，能用mysql解决的偏要用mongodb/hbase。这是本末倒置。<br>我宁愿初期开发一个很土的实现，后期去重构。重构的代价可能还小于初期就搞一个复杂架构。<br>而且说实话，很多产品、项目，还熬不到需要重构的那个阶段就会死掉。</p>
<p>技术也很难有高下之分。以前道听途说，觉得php很low。但那又如何？很多产品要的就是猛糙快。<br>快速出原型、快速试错、快速死亡。就像细菌的分裂一样。。。虽然个体很脆弱，但总量很大，最后总能筛选出活下来的。活下来的就值得长期培养。<br>听说现在生物实验用的菌系都是培养了好几十年的。。。</p>
<p>也只有程序员才会关心O(logN)和O(N)的区别吧。<br>老板/用户/PM根本不会关心背后用了什么技术。只要能实现相关功能，并且没有用户抱怨，就是好的。能抓住老鼠就是好猫。</p>
<p>大多数用户都是很浅薄的。一个好看的页面，比一个优秀的算法重要的多。</p>
<p>当然我也没有说技术不重要，有人说程序员要有“匠人”精神，我也赞同。<br>但不要过度。</p>
<h1 id="关于运维">关于运维</h1>
<p>运维为什么很苦逼。因为做的事情都在暗处，大多数人都看不到，没有存在感。不出事呢是应该的，出事就要背锅。<br>考核的时候，如果一直没出事，能得个平均水准的分数；一旦出事，就只能呵呵了。<br>说的直白点，就是靠天吃饭。</p>
<p>那怎么办？搞天气预报啊。<br>各种监控系统、运维工具、报警工具，就是干这个的。<br>但开发维护这些系统需要一个庞大的团队，不是一两个人能搞的定的。<br>而且很多时候要定制开发，而不是只用一些通用的工具。</p>
<p>这跟公司文化也有关。有很多人觉得运维很简单，随便找人搞搞就行，或者找个开发兼职搞下。<br>很多小公司都没有专职的运维吧。<br>没有对运维投入人力物力，却提出很高的运维标准，比如N个9的可用率保证，我能呵呵么。</p>
<p>在RDBMS的年代，一个NB的DBA可以搞定一切。<br>但对hadoop而言，即使是单纯的运维也必须要看代码改代码。</p>
<p>改变这种情况需要自上而下的推动，可惜我无能为力。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>一些想法，一些牢骚。<br>胡言乱语，博君一笑。</p>
]]>
    
    </summary>
    
      <category term="杂谈" scheme="http://jxy.me/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[hadoop与windows]]></title>
    <link href="http://jxy.me/2015/06/27/hadoop-and-windows/"/>
    <id>http://jxy.me/2015/06/27/hadoop-and-windows/</id>
    <published>2015-06-27T05:40:21.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>以前写的一篇文章。<br>折腾了下windows下的编译、部署、使用。纯粹为了好玩，肯定不能用于生产环境。</p>
<a id="more"></a>
<h1 id="在windows中编译hadoop">在windows中编译hadoop</h1>
<p>以2.5.2的代码为基准。系统为32位WinXP SP3（虚拟机）。</p>
<h2 id="前置要求">前置要求</h2>
<p>看BUILDING.txt：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">Building <span class="function_start"><span class="keyword">on</span></span> Windows</div><div class="line"></div><div class="line"><span class="comment">----------------------------------------------------------------------------------</span></div><div class="line">Requirements:</div><div class="line"></div><div class="line">* Windows System</div><div class="line">* JDK <span class="number">1.6</span>+</div><div class="line">* Maven <span class="number">3.0</span> <span class="keyword">or</span> later</div><div class="line">* Findbugs <span class="number">1.3</span><span class="number">.9</span> (<span class="keyword">if</span> <span class="property">running</span> findbugs)</div><div class="line">* ProtocolBuffer <span class="number">2.5</span><span class="number">.0</span></div><div class="line">* Windows SDK <span class="keyword">or</span> Visual Studio <span class="number">2010</span> Professional</div><div class="line">* Unix command-line tools <span class="keyword">from</span> GnuWin32 <span class="keyword">or</span> Cygwin: sh, mkdir, rm, cp, tar, gzip</div><div class="line">* zlib headers (<span class="keyword">if</span> building native code bindings <span class="keyword">for</span> zlib)</div><div class="line">* Internet connection <span class="keyword">for</span> <span class="keyword">first</span> build (<span class="keyword">to</span> fetch all Maven <span class="keyword">and</span> Hadoop dependencies)</div></pre></td></tr></table></figure>

<p>大多数都好说，有几个要求比较麻烦：</p>
<h3 id="ProtocolBuffer">ProtocolBuffer</h3>
<p>protocbuffer项目迁到github了，目前好像最新是3.0版。<br>以前的版本还可以在google code上下载。直接下载编译好的：<a href="https://protobuf.googlecode.com/files/protoc-2.5.0-win32.zip" target="_blank" rel="external">https://protobuf.googlecode.com/files/protoc-2.5.0-win32.zip</a><br>解压后将protoc.exe加入PATH。我是直接扔到c:\windows里了，省事。</p>
<h3 id="Windows_SDK">Windows SDK</h3>
<p>可以去<a href="http://www.microsoft.com/en-us/download/details.aspx?id=8279" target="_blank" rel="external">http://www.microsoft.com/en-us/download/details.aspx?id=8279</a>下载。<br>但这里给出的是在线安装，非常非常慢。还是下载完整的程序自己安装吧。<br>完整的iso在这里下载：<a href="http://www.microsoft.com/en-us/download/details.aspx?id=8442" target="_blank" rel="external">http://www.microsoft.com/en-us/download/details.aspx?id=8442</a>。<br>注意根据需要选择32位或64位版本。<br>安装也是一路next。</p>
<h3 id="Unix_command-line_tools">Unix command-line tools</h3>
<p>很多文章推荐cygwin。但是cygwin太重量级了。<br>我推荐直接安装git bash，它自带一个MinGW32，包含常用的linux命令。<br>去<a href="http://git-scm.com/download/win" target="_blank" rel="external">http://git-scm.com/download/win</a>这里下载并安装。注意安装中有一步询问是否将相关命令加入PATH，选择加入。</p>
<p>其他依赖就很简单了。记得把相关命令都加入PATH。在cmd里输入mvn/protoc/ls等命令验证下。</p>
<h2 id="修改代码">修改代码</h2>
<p>原来的代码直接编译有些问题。<br>主要参考<a href="http://www.cnblogs.com/smartbooks/p/3694760.html" target="_blank" rel="external">这篇文章</a>。</p>
<p>原作者说要改3个地方：</p>
<ol>
<li>hadoop-common-project\hadoop-auth\pom.xml增加一个依赖org.mortbay.jetty:jetty-util。在2.2.0确实有这个问题。但2.5.2已经修复了。</li>
<li>修改hadoop-common-project\hadoop-common\src\main\native\native.sln。将{4C0C12D2-3CB0-47F8-BCD0-55BD5732DFA7}.Debug|Win32.XXX = Release|x64修改为{4C0C12D2-3CB0-47F8-BCD0-55BD5732DFA7}.Debug|Win32.XXX = Release|Win32。总共要修改4行。</li>
<li>修改hadoop-common-project\hadoop-common\src\main\native\native.vcxproj，将所有Release|x64替换为Release|Win32。</li>
</ol>
<p>我估计如果是在64位系统中编译，2、3两步是不用改的。</p>
<p>（我想在Win7 64位下测试下的，但Windows SDK死活安装不上，看日志里有报错vcredist_x64.exe installation failed with return code 5100，google的话能找到这个链接：<a href="http://support.microsoft.com/kb/2717426/de" target="_blank" rel="external">http://support.microsoft.com/kb/2717426/de</a>。但按这个链接卸载了vcredist也不行。折腾了半天。为这个重装系统也不值得。）</p>
<h2 id="编译">编译</h2>
<p>进入Windows SDK 7.1 Command Prompt，开始编译：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="preprocessor"># maven下载非常慢。设置代理也很麻烦。</span></div><div class="line"><span class="preprocessor"># 我以前在服务器（linux）上编译过2.5.2的hadoop。把~/.m2下所有文件拷到虚拟机里，替换原来的maven repo。</span></div><div class="line"><span class="keyword">set</span> ZLIB_HOME=E:\zlib128-dll\include</div><div class="line"><span class="preprocessor"># 注意大小写，必须是Platform</span></div><div class="line"><span class="keyword">set</span> Platform=Win32</div><div class="line"><span class="preprocessor"># 从本地下载tomcat。否则从外网下载非常慢。</span></div><div class="line">mvn package -Pdist,native-win -DskipTests -Dtar -Dtomcat.version=<span class="number">6.0</span><span class="number">.36</span> -Dtomcat.download.url=file:<span class="comment">//e:/apache-tomcat-6.0.36.tar.gz</span></div></pre></td></tr></table></figure>

<p>编译成功后在hadoop-dist生成tar包。和linux一样。</p>
<h2 id="一些注意事项">一些注意事项</h2>
<p>JDK的路径不要包含空格<br>hadoop的代码所在路径尽量简短。比如e:\h252<br>执行命令时总是出现一个warn：unable to load native lib。但编译时其实编译了native库的，开debug日志似乎是说hadoop.dll有问题，暂时不明白是怎么回事。<br>虽然我是32位winxp下编译的，但拿到64位win7中也能用，因为我只是当做客户端来用，访问hdfs，提交job之类的。如果在windows中部署集群估计就不能通用了。<br>其他一些事项见BUILDING.txt</p>
<h1 id="windows客户端使用">windows客户端使用</h1>
<p>这个客户端可以在windows下读写2.5.2、2.2.0的hdfs。可以向2.5.2的yarn提交任务，不能向2.2.0的yarn提交任务，见<a href="https://issues.apache.org/jira/browse/MAPREDUCE-4052" target="_blank" rel="external">MAPREDUCE-4052</a>。（有人给出过解决方法，但只是临时方案，比较麻烦）<br>在32位WinXP SP3和64位Win7中测试通过。</p>
<h2 id="设置JAVA_HOME">设置JAVA_HOME</h2>
<p>为避免一些不必要的问题，JAVA_HOME的路径不要有空格。<br>下载jce的jar替换JAVA_HOME/jre/lib/security中的同名文件。如果是OpenJDK不用替换。<br>修改HADOOP_HOME/etc/hadoop/hadoop-env.cmd：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">set JAVA_HOME=D:<span class="command">\test</span><span class="command">\jdk</span>1.6.0_30</div></pre></td></tr></table></figure>

<h2 id="kerberos认证">kerberos认证</h2>
<p>首先安装kerberos的win客户端：<a href="http://web.mit.edu/kerberos/dist/kfw/3.2/kfw-3.2.2/kfw-3-2-2.exe" target="_blank" rel="external">http://web.mit.edu/kerberos/dist/kfw/3.2/kfw-3.2.2/kfw-3-2-2.exe</a>。现在有4.x版本的了，但我还是觉得这个3.2的用着方便。<br>设置c:\windows\krb5.ini的过程不多说了。</p>
<p>认证首先要有keytab文件。<br>win版的kerberos似乎有些问题，必须手动设置一下票据缓存的位置，否则hadoop认证时会失败。而且在win7下和winxp下输出也不同。</p>
<p>Win7下的输出：<br><img src="/2015/06/27/hadoop-and-windows/1.png" alt=""></p>
<p>WinXP下的输出：<br><img src="/2015/06/27/hadoop-and-windows/2.png" alt=""></p>
<p>KRB5CCNAME环境变量可以设置为任意文件名，有权限就行。<br>无论如何，如果kinit命令没有出错，就可以认为kerberos认证成功了。</p>
<h2 id="访问hdfs和yarn">访问hdfs和yarn</h2>
<p>大多数命令都跟linux下是一样的。<br>执行ls命令验证下：<br><img src="/2015/06/27/hadoop-and-windows/3.png" alt=""></p>
<p>提交一个wordcount任务：<br><img src="/2015/06/27/hadoop-and-windows/4.png" alt=""></p>
<h2 id="一些注意事项-1">一些注意事项</h2>
<p>win客户端不能-text lzo文件，似乎没有windows可用的lzo库。google的话能找到一个链接：<a href="http://gnuwin32.sourceforge.net/packages/lzo.htm" target="_blank" rel="external">http://gnuwin32.sourceforge.net/packages/lzo.htm</a>，可以在win下安装，但试过不好使。<br>Unable to load native-hadoop这个warn，没找到办法消除。我已经编译了hadoop.dll，但还是有warn。可以set HADOOP_ROOT_LOGGER=DEBUG,console开下debug日志，看下具体的错误。暂时没法解决。<br>注意core-site.xml中的hadoop.tmp.dir设置，默认是/tmp/hadoop-${user.name}，会在硬盘上自动建个文件夹。</p>
<h1 id="在eclipse中读写HDFS">在eclipse中读写HDFS</h1>
<p>其实跟windows没啥关系。只是我一般用win下的eclipse，写到这正好想起来，顺便记下。仅供调试使用。<br>其实也可以用于Linux下，一样的道理。</p>
<p>关键问题就是如何获得必要的Configuration对象。正常情况下把core-site.xml和hdfs-site.xml放到classpath里，再conf.addResource(“hdfs-site.xml”)就可以了（core-site.xml会自动加载）。<br>但其实不是所有属性都是必要的。测试时完全可以在代码里手动设置，不用配置文件。<br>代码：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span>(String[] args) throws IOException {</div><div class="line">    <span class="comment">// kerberos的配置文件的位置，windows下叫krb5.ini，linux下叫krb5.conf</span></div><div class="line">    System.setProperty(<span class="string">"java.security.krb5.conf"</span>, <span class="string">"e:/Documents/TEMP/krb5.ini"</span>);</div><div class="line">    <span class="comment">// 在windows下会需要一个winutil.exe的文件，要放在hadoop.home.dir/bin目录下，否则会报个异常，但不影响程序运行</span></div><div class="line">    System.setProperty(<span class="string">"hadoop.home.dir"</span>, <span class="string">"D:\\Hadoop_src\\hadoop-2.2.0"</span>);</div><div class="line">    <span class="comment">// 默认的构造函数会在classpath中查找core-site.xml并加载</span></div><div class="line">    Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line"> </div><div class="line">    <span class="comment">// 第一种方法，直接加载配置文件。需要core-site.xml和hdfs-site.xml都在classpath中</span></div><div class="line">    <span class="comment">// conf.addResource("hdfs-site.xml");</span></div><div class="line"> </div><div class="line">    <span class="comment">// 第二种方法，自己手动set需要的属性，不需要任何xml配置文件</span></div><div class="line">    <span class="comment">// 这里又分两种情况，如果没开启HA或者知道哪台机器是ActiveNN，可以直接写死地址</span></div><div class="line">    <span class="comment">// 否则要加上HA相关配置</span></div><div class="line"> </div><div class="line">    <span class="comment">// non-HA case</span></div><div class="line">    <span class="comment">// conf.setBoolean("hadoop.security.authorization", true);</span></div><div class="line">    <span class="comment">// conf.set("hadoop.security.authentication", "kerberos");      </span></div><div class="line">    <span class="comment">// conf.set("fs.defaultFS", "hdfs://inspur129.photo.163.org:8020");</span></div><div class="line">    <span class="comment">// conf.set("dfs.namenode.kerberos.principal","hdfs/inspur129.photo.163.org@HADOOP.HZ.NETEASE.COM");</span></div><div class="line"> </div><div class="line">    <span class="comment">// HA case</span></div><div class="line">    <span class="comment">// 这里以测试集群为例，参考自己的hdfs-site.xml</span></div><div class="line">    conf.setBoolean(<span class="string">"hadoop.security.authorization"</span>, <span class="keyword">true</span>);</div><div class="line">    conf.<span class="keyword">set</span>(<span class="string">"hadoop.security.authentication"</span>, <span class="string">"kerberos"</span>);        </div><div class="line">    conf.<span class="keyword">set</span>(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://hp1"</span>);</div><div class="line">    conf.<span class="keyword">set</span>(<span class="string">"dfs.nameservices"</span>, <span class="string">"hp1"</span>);</div><div class="line">    conf.<span class="keyword">set</span>(<span class="string">"dfs.ha.namenodes.hp1"</span>, <span class="string">"nn1,nn2"</span>);</div><div class="line">    conf.<span class="keyword">set</span>(<span class="string">"dfs.namenode.rpc-address.hp1.nn1"</span>,<span class="string">"inspur120.photo.163.org:8020"</span>);</div><div class="line">    conf.<span class="keyword">set</span>(<span class="string">"dfs.namenode.rpc-address.hp1.nn2"</span>,<span class="string">"inspur129.photo.163.org:8020"</span>);</div><div class="line">    conf.<span class="keyword">set</span>(<span class="string">"dfs.client.failover.proxy.provider.hp1"</span>,<span class="string">"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"</span>);</div><div class="line">    conf.<span class="keyword">set</span>(<span class="string">"dfs.namenode.kerberos.principal"</span>,<span class="string">"hdfs/_HOST@HADOOP.HZ.NETEASE.COM"</span>);</div><div class="line"> </div><div class="line">    <span class="comment">// 至此已经获得了正确配置的Configuration对象，可以访问NN了</span></div><div class="line"> </div><div class="line">    <span class="comment">// 用keytab认证</span></div><div class="line">    UserGroupInformation.setConfiguration(conf);</div><div class="line">    UserGroupInformation.loginUserFromKeytab(<span class="string">"spark@HADOOP.HZ.NETEASE.COM"</span>,</div><div class="line">            <span class="string">"E:\\Documents\\TEMP\\spark.keytab"</span>);</div><div class="line"> </div><div class="line">    <span class="comment">// 获得FileSystem对象后，就可以读写hdfs了，客户端API隐藏了很多细节</span></div><div class="line">    FileSystem fs = FileSystem.<span class="keyword">get</span>(conf);</div><div class="line">    InputStream <span class="keyword">in</span> = fs.open(<span class="keyword">new</span> Path(<span class="string">"/tmp/hadoop-env.sh"</span>));</div><div class="line">    IOUtils.copyBytes(<span class="keyword">in</span>, System.<span class="keyword">out</span>, conf, <span class="keyword">false</span>);</div><div class="line">    fs.close();</div><div class="line">}</div></pre></td></tr></table></figure>

<p>推荐还是尽量使用HA的配置吧。</p>
<p>加上maven依赖即可运行：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">dependency</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="title">groupId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="title">artifactId</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">version</span>&gt;</span>2.2.0<span class="tag">&lt;/<span class="title">version</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">dependency</span>&gt;</span></div></pre></td></tr></table></figure>

<p>这种方法只能用于测试，把配置在程序中写死是很low的。<br>其实除了第一步设置winutil.exe的位置，其他代码在linux下是通用的。</p>
<p>题外话：<br>上述配置中的hp1/nn1/nn2等名称完全是客户端的，其实可以是任意字符串，只要能对应上就行，比如：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">conf</span>.<span class="keyword">set</span>(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://xyxyxy"</span>);</div><div class="line"><span class="keyword">conf</span>.<span class="keyword">set</span>(<span class="string">"dfs.nameservices"</span>, <span class="string">"xyxyxy"</span>);</div><div class="line"><span class="keyword">conf</span>.<span class="keyword">set</span>(<span class="string">"dfs.ha.namenodes.xyxyxy"</span>, <span class="string">"jiang,xi"</span>);</div><div class="line"><span class="keyword">conf</span>.<span class="keyword">set</span>(<span class="string">"dfs.namenode.rpc-address.xyxyxy.jiang"</span>,<span class="string">"inspur120.photo.163.org:8020"</span>);</div><div class="line"><span class="keyword">conf</span>.<span class="keyword">set</span>(<span class="string">"dfs.namenode.rpc-address.xyxyxy.xi"</span>,<span class="string">"inspur129.photo.163.org:8020"</span>);</div><div class="line"><span class="keyword">conf</span>.<span class="keyword">set</span>(<span class="string">"dfs.client.failover.proxy.provider.xyxyxy"</span>,<span class="string">"org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider"</span>);</div></pre></td></tr></table></figure>

<p>改成这样的配置也可以正常运行。因为hp1之类的名称只是逻辑上的概念，不会真的存在一个叫hp1的机器，客户端API会自动根据配置替换成实际的地址。<br>不过一般用户不用关心这个，直接抄我们配置好的hdfs-site.xml里的就好。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>以前写的一篇文章。<br>折腾了下windows下的编译、部署、使用。纯粹为了好玩，肯定不能用于生产环境。</p>
]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://jxy.me/tags/hadoop/"/>
    
      <category term="windows" scheme="http://jxy.me/tags/windows/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[socket连接过多导致dead node]]></title>
    <link href="http://jxy.me/2015/06/10/socket-exhausted-deadnode/"/>
    <id>http://jxy.me/2015/06/10/socket-exhausted-deadnode/</id>
    <published>2015-06-10T03:32:34.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>近日碰到的一个问题，某些用户的程序耗尽了一些节点的socket资源导致dead node。</p>
<a id="more"></a>
<h1 id="症状">症状</h1>
<p>我们有用zabbix监控NN metrics。<br>17点30左右收到报警，集群出现deadnode，但DN进程还在。<br>查看deadnode的日志：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="number">2015</span>-<span class="number">06</span>-<span class="number">05</span> <span class="number">16</span>:<span class="number">47</span>:<span class="number">00</span>,<span class="number">663</span> WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException <span class="operator">in</span> offerService</div><div class="line">java.net.BindException: Problem binding <span class="built_in">to</span> [hadoop72.photo<span class="number">.163</span>.org/<span class="number">10.160</span><span class="number">.173</span><span class="number">.13</span>:<span class="number">0</span>] java.net.BindException: Address already <span class="operator">in</span> use; For more details see: <span class="keyword">http</span>://wiki.apache.org/hadoop/BindException</div><div class="line"><span class="keyword">at</span> org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:<span class="number">719</span>)</div><div class="line"><span class="keyword">at</span> org.apache.hadoop.ipc.Client.call(Client.java:<span class="number">1351</span>)</div><div class="line"><span class="keyword">at</span> org.apache.hadoop.ipc.Client.call(Client.java:<span class="number">1300</span>)</div><div class="line"><span class="keyword">at</span> org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:<span class="number">206</span>)</div><div class="line"><span class="keyword">at</span> com.sun.proxy.$Proxy9.sendHeartbeat(Unknown Source)</div><div class="line"><span class="keyword">at</span> sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)</div><div class="line"><span class="keyword">at</span> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">25</span>)</div><div class="line"><span class="keyword">at</span> java.lang.reflect.Method.invoke(Method.java:<span class="number">597</span>)</div><div class="line"><span class="keyword">at</span> org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:<span class="number">186</span>)</div><div class="line"><span class="keyword">at</span> org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:<span class="number">102</span>)</div><div class="line"><span class="keyword">at</span> com.sun.proxy.$Proxy9.sendHeartbeat(Unknown Source)</div><div class="line"><span class="keyword">at</span> org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:<span class="number">167</span>)</div><div class="line"><span class="keyword">at</span> org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:<span class="number">445</span>)</div><div class="line"><span class="keyword">at</span> org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:<span class="number">525</span>)</div><div class="line"><span class="keyword">at</span> org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:<span class="number">676</span>)</div><div class="line"><span class="keyword">at</span> java.lang.Thread.run(Thread.java:<span class="number">662</span>)</div></pre></td></tr></table></figure>

<p>这个错误的意思是DN无法向NN发送心跳。<br>NN超过一定时间没有收到心跳，就会将DN标记为dead。这个超时机制见<a href="/2015/04/01/hadoop-tips/">这里第8条</a>。</p>
<p>但为什么无法发送心跳？检查网络、系统、交换机，都没有任何异常。<br>而且在排查过程中，dead node不断增多，最多时达到6个，已经开始出现missing block。部分用户的任务会失败。</p>
<p>仔细看上面的异常，发现DN进程试图绑定到0号端口。根据给出的wiki链接，0号端口是让系统自动分配一个空闲的端口。<br>而绑定失败，说明系统已经没有空闲的端口可以分配了。怀疑是网络连接过多。</p>
<p><code>netstat -anp | grep 1004</code>（1004是DN的RPC端口）看下，发现了8w+的TCP CLOSE_WAIT连接。这8w+里有进来的连接也有出去的连接，没细分。<br>本来-p参数应该能看到进程id的，就能统计到底哪个进程开了这么多socket。但netstat命令我没有sudo权限，只能看到自己的进程，真是蛋疼。</p>
<p>尝试重启DN进程，没有效果，还是deadnode。<br>尝试重启NM进程，连接数立刻就正常了。deadnode也消失了。<br>于是怀疑是某个用户的container进程的问题。</p>
<p>在RM的web界面上查看正在运行的job，根据启动时间排除下，只有一个job符合条件。<br>查看所有变为dead node的DN，确实都有这个job的reduce在上面跑。<br>尝试杀掉一个reduce attempt，连接数立刻下降。<br>可以确定就是这个job的问题。</p>
<p>杀掉这个job，所有DN恢复正常。</p>
<p>另外，在这个过程中，出现了一次NN切换，具体原因不明，不知是否和这次事件有关。好在没有造成什么影响。</p>
<h1 id="分析">分析</h1>
<p>事后跟用户沟通，他的代码大概是这么写的：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span>(Text key, Iterable&lt;LongWritable&gt; values,</div><div class="line">				Context context) {</div><div class="line">	FileSystem fs=FileSystem.<span class="keyword">get</span>(context.getConfiguration());</div><div class="line">	InputStream <span class="keyword">in</span>=fs.open(<span class="keyword">new</span> Path(<span class="string">"some_file"</span>));</div><div class="line">	<span class="comment">// 将数据读到一个map中</span></div><div class="line">}</div></pre></td></tr></table></figure>

<p>也就是说，每次reduce都会读取hdfs上一个文件。他的本意是做类似于join的事情，这个读取数据的逻辑应该放到setup()方法里的，但却放到了reduce()方法。<br>而且他最后没有调用in.close()方法，导致打开的socket链接不会释放。</p>
<p>这个问题很容易重现，我先写一段单机的代码试下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span>(String[] args) throws IOException,</div><div class="line">			InterruptedException {</div><div class="line">		Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line">		FileSystem fs = FileSystem.<span class="keyword">get</span>(conf);</div><div class="line">		<span class="comment">// 随便读取hdfs上一个文件</span></div><div class="line">		InputStream <span class="keyword">in</span> = fs.open(<span class="keyword">new</span> Path(<span class="string">"/user/mine/jxy/yarn.cmd"</span>));</div><div class="line">		<span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">1024</span>];</div><div class="line">		<span class="keyword">in</span>.read(buffer);</div><div class="line">		Thread.sleep(Integer.MAX_VALUE);</div><div class="line">	}</div></pre></td></tr></table></figure>

<p>运行后，用netstat查看：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">hadoop<span class="variable">@inspur116</span><span class="symbol">:~</span><span class="variable">$ </span>netstat -anp | grep <span class="number">1004</span></div><div class="line">(<span class="constant">Not</span> all processes could be identified, non-owned process info</div><div class="line"> will <span class="keyword">not</span> be shown, you would have to be root to see it all.)</div><div class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span><span class="symbol">:</span><span class="number">1004</span>            <span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span><span class="symbol">:*</span>               <span class="constant">LISTEN</span>      -               </div><div class="line">tcp        <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160</span>.<span class="number">128.191</span><span class="symbol">:</span><span class="number">21072</span>    <span class="number">10.160</span>.<span class="number">128.193</span><span class="symbol">:</span><span class="number">1004</span>     <span class="constant">CLOSE_WAIT</span>  <span class="number">27744</span>/java      </div><div class="line">tcp        <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160</span>.<span class="number">128.191</span><span class="symbol">:</span><span class="number">21075</span>    <span class="number">10.160</span>.<span class="number">128.193</span><span class="symbol">:</span><span class="number">1004</span>     <span class="constant">CLOSE_WAIT</span>  <span class="number">27841</span>/java      </div><div class="line">tcp        <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160</span>.<span class="number">128.191</span><span class="symbol">:</span><span class="number">21069</span>    <span class="number">10.160</span>.<span class="number">128.193</span><span class="symbol">:</span><span class="number">1004</span>     <span class="constant">CLOSE_WAIT</span>  <span class="number">25892</span>/java      </div><div class="line">unix  <span class="number">2</span>      [ <span class="constant">ACC</span> ]     <span class="constant">STREAM</span>     <span class="constant">LISTENING</span>     <span class="number">3620589941</span> -                   <span class="regexp">/var/run</span><span class="regexp">/hadoop-hdfs/dn</span>.<span class="number">1004</span></div></pre></td></tr></table></figure>

<p>因为我启动了3个进程，所以有3个CLOSE_WAIT的连接。<code>dn.1004</code>那个是用于short-circuit read的<code>unix domain socket</code>，而不是我们通常说的<code>network socket</code>，忽略。<br>可见如果不调用in.close()确实会造成很多CLOSE_WAIT的连接。</p>
<p>如果在一个进程中循环调用那段代码：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">hadoop<span class="variable">@inspur116</span><span class="symbol">:~</span><span class="variable">$ </span>netstat -anp | grep <span class="number">1004</span></div><div class="line">(<span class="constant">Not</span> all processes could be identified, non-owned process info</div><div class="line"> will <span class="keyword">not</span> be shown, you would have to be root to see it all.)</div><div class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span><span class="symbol">:</span><span class="number">1004</span>            <span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span><span class="symbol">:*</span>               <span class="constant">LISTEN</span>      -               </div><div class="line">tcp        <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160</span>.<span class="number">128.191</span><span class="symbol">:</span><span class="number">21137</span>    <span class="number">10.160</span>.<span class="number">128.193</span><span class="symbol">:</span><span class="number">1004</span>     <span class="constant">CLOSE_WAIT</span>  <span class="number">31153</span>/java      </div><div class="line">tcp        <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160</span>.<span class="number">128.191</span><span class="symbol">:</span><span class="number">21143</span>    <span class="number">10.160</span>.<span class="number">128.193</span><span class="symbol">:</span><span class="number">1004</span>     <span class="constant">CLOSE_WAIT</span>  <span class="number">31153</span>/java      </div><div class="line">tcp        <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160</span>.<span class="number">128.191</span><span class="symbol">:</span><span class="number">21182</span>    <span class="number">10.160</span>.<span class="number">128.193</span><span class="symbol">:</span><span class="number">1004</span>     <span class="constant">CLOSE_WAIT</span>  <span class="number">31153</span>/java      </div><div class="line">tcp        <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160</span>.<span class="number">128.191</span><span class="symbol">:</span><span class="number">21178</span>    <span class="number">10.160</span>.<span class="number">128.193</span><span class="symbol">:</span><span class="number">1004</span>     <span class="constant">CLOSE_WAIT</span>  <span class="number">31153</span>/java      </div><div class="line">tcp        <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160</span>.<span class="number">128.191</span><span class="symbol">:</span><span class="number">21139</span>    <span class="number">10.160</span>.<span class="number">128.193</span><span class="symbol">:</span><span class="number">1004</span>     <span class="constant">CLOSE_WAIT</span>  <span class="number">31153</span>/java      </div><div class="line">tcp        <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160</span>.<span class="number">128.191</span><span class="symbol">:</span><span class="number">21141</span>    <span class="number">10.160</span>.<span class="number">128.193</span><span class="symbol">:</span><span class="number">1004</span>     <span class="constant">CLOSE_WAIT</span>  <span class="number">31153</span>/java      </div><div class="line">tcp        <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160</span>.<span class="number">128.191</span><span class="symbol">:</span><span class="number">21180</span>    <span class="number">10.160</span>.<span class="number">128.193</span><span class="symbol">:</span><span class="number">1004</span>     <span class="constant">CLOSE_WAIT</span>  <span class="number">31153</span>/java      </div><div class="line">unix  <span class="number">2</span>      [ <span class="constant">ACC</span> ]     <span class="constant">STREAM</span>     <span class="constant">LISTENING</span>     <span class="number">3620589941</span> -                   <span class="regexp">/var/run</span><span class="regexp">/hadoop-hdfs/dn</span>.<span class="number">1004</span></div></pre></td></tr></table></figure>

<p>如果登陆到10.160.128.193这台DN上去看：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">hadoop<span class="variable">@inspur122</span><span class="symbol">:~</span><span class="variable">$ </span>netstat -anp | grep <span class="number">1004</span></div><div class="line">(<span class="constant">Not</span> all processes could be identified, non-owned process info</div><div class="line"> will <span class="keyword">not</span> be shown, you would have to be root to see it all.)</div><div class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span><span class="symbol">:</span><span class="number">1004</span>            <span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span>.<span class="number">0</span><span class="symbol">:*</span>               <span class="constant">LISTEN</span>      -               </div><div class="line">tcp        <span class="number">0</span>      <span class="number">0</span> <span class="number">10.160</span>.<span class="number">128.193</span><span class="symbol">:</span><span class="number">1004</span>     <span class="number">10.160</span>.<span class="number">128.191</span><span class="symbol">:</span><span class="number">21184</span>    <span class="constant">FIN_WAIT2</span>   -               </div><div class="line">unix  <span class="number">2</span>      [ <span class="constant">ACC</span> ]     <span class="constant">STREAM</span>     <span class="constant">LISTENING</span>     <span class="number">69412045</span> -                   <span class="regexp">/var/run</span><span class="regexp">/hadoop-hdfs/dn</span>.<span class="number">1004</span></div></pre></td></tr></table></figure>

<p>客户端出现CLOSE_WAIT我可以理解，因为没有主动关闭。就算接到了服务端的FIN包，虽然会ACK，但不会返还一个FIN包。<br>服务端出现FIN_WAIT2不是很明白，而且和客户端的CLOSE_WAIT不是一一对应的。大概是因为服务端主动关闭socket，等待客户端返还一个FIN包，一直没有回应，就超时了，直接关闭了吧。我计算机网络学的比较烂。。。<br>google找到了一个参数tcp_fin_timeout：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 这个目录下还有很多关于网络的参数</span></div><div class="line">hadoop@inspur122:~$ cat /<span class="keyword">proc</span>/sys/net/ipv4/tcp_fin_timeout</div><div class="line"><span class="number">60</span></div></pre></td></tr></table></figure>

<p>就是说一个FIN_WAIT2的连接超过60秒会直接关掉。大概吧。</p>
<p>接下来把这段代码写到wordcount里，每次map都读这个文件，而且不关闭。<br>提交job到测试集群，可以观察到连接数暴涨：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">hadoop<span class="variable">@inspur131</span><span class="symbol">:~</span><span class="variable">$ </span>netstat -anp | grep <span class="number">1004</span> | wc -l</div><div class="line"><span class="number">53539</span></div><div class="line">hadoop<span class="variable">@inspur131</span><span class="symbol">:~</span><span class="variable">$ </span>netstat -anp | grep <span class="number">1004</span> | wc -l</div><div class="line"><span class="number">89481</span></div><div class="line">hadoop<span class="variable">@inspur131</span><span class="symbol">:~</span><span class="variable">$ </span>netstat -anp | grep <span class="number">1004</span> | wc -l</div><div class="line"><span class="number">119334</span></div><div class="line">hadoop<span class="variable">@inspur131</span><span class="symbol">:~</span><span class="variable">$ </span>netstat -anp | grep <span class="number">1004</span> | wc -l</div><div class="line"><span class="number">173461</span></div></pre></td></tr></table></figure>

<p>过一段时间后出现dead node。跟线上集群之前的症状一样。<br>查看map的日志可以看到很多dead node异常。</p>
<p>但观察到一个有意思的事。/user/mine/jxy/yarn.cmd这个文件只有一个block，位于3台机器上。这3台机器会比其他节点先挂（挂指的是不能向NN发心跳）。<br>其他机器上挂掉时都是大量CLOSE_WAIT的连接。但这3台机器挂掉时还有大量的FIN_WAIT2连接。<br>可以认为，普通节点，socket资源都耗费在<strong>出去的连接</strong>上了。而这3台机器还有一些资源耗费在<strong>进来的连接</strong>上。</p>
<p>而且这3台机器也不是一起挂掉的，而是按一个固定的顺序。<br>因为client读取hdfs数据时，是从NN处获取一个DN列表（在我们的集群上就是3个），然后依次尝试。如果一个DN不通就尝试下一个。<br>猜测NN返回的列表顺序是固定的（不考虑本地性的情况下）。但没看代码验证过。</p>
<p>结论：InputStream必须close啊。。。</p>
<h2 id="关于socket">关于socket</h2>
<p><a href="http://en.wikipedia.org/wiki/Network_socket" target="_blank" rel="external">wiki</a>讲的很清楚：一个socket=IP+端口+协议。<br>所以TCP的<code>127.0.0.1:80</code>和UDP的<code>127.0.0.1:80</code>是两个不同的socket，不会互相影响。<br>但我们所说的socket一般默认都是指TCP。</p>
<h2 id="关于TCP连接状态">关于TCP连接状态</h2>
<p>关于TCP socket的状态，最权威的当然是<a href="https://www.rfc-editor.org/rfc/rfc793.txt" target="_blank" rel="external">RFC</a>。但一般人看不了这个。还是翻翻计算机网络的教材吧。。。好在我还留着。<br>关于CLOSE_WAIT，<a href="http://blog.sina.com.cn/s/blog_408c00570100eek0.html" target="_blank" rel="external">这篇文章</a>讲的还算清楚。</p>
<p>之前说过FIN_WAIT2会在一分钟内超时，其实CLOSE_WAIT放着不管也会超时并释放的，见<a href="http://ahuaxuan.iteye.com/blog/657511" target="_blank" rel="external">这个文章</a>。但默认超时是2个小时，足够把集群搞挂了。。。<br>另外这个超时机制不是针对CLOSE_WAIT的，对所有TCP连接都生效。</p>
<h1 id="hbase_regionserver">hbase regionserver</h1>
<p>其实处理<a href="/2015/04/07/hbase-switch-problem/">上次hbase的事故</a>时，我就观察到了一些regionserver有大量的CLOSE_WAIT连接。<br>当时虽然觉得有点奇怪，但没有深究。<br>借这次的机会看了下。本来以为是bug，相关JIRA：<a href="https://issues.apache.org/jira/browse/HDFS-5671" target="_blank" rel="external">HDFS-5671</a>、<a href="https://issues.apache.org/jira/browse/HBASE-9393" target="_blank" rel="external">HBASE-9393</a>、<a href="https://issues.apache.org/jira/browse/HBASE-11833" target="_blank" rel="external">HBASE-11833</a></p>
<p>但后来发现CLOSE_WAIT的连接都是ipv6的：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="title">tcp6</span>       <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160.128.139:21508</span>    <span class="number">10.160.128.139:1004</span>     CLOSE_WAIT  <span class="number">16987</span>/java      </div><div class="line">tcp6       <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160.128.139:64814</span>    <span class="number">10.160.128.139:1004</span>     CLOSE_WAIT  <span class="number">16987</span>/java      </div><div class="line">tcp6       <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160.128.139:16763</span>    <span class="number">10.160.128.139:1004</span>     CLOSE_WAIT  <span class="number">16987</span>/java      </div><div class="line">tcp6       <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160.128.139:34750</span>    <span class="number">10.160.128.139:1004</span>     CLOSE_WAIT  <span class="number">16987</span>/java      </div><div class="line">tcp6       <span class="number">1</span>      <span class="number">0</span> <span class="number">10.160.128.139:30541</span>    <span class="number">10.160.128.139:1004</span>     CLOSE_WAIT  <span class="number">16987</span>/java</div></pre></td></tr></table></figure>

<p>看来启动JVM时没加上<code>-Djava.net.preferIPv4Stack=true</code>参数，找时间重启下regionserver再观察下。</p>
<h1 id="遗留的问题">遗留的问题</h1>
<h2 id="NN为何切换？">NN为何切换？</h2>
<p>原因不明。。不知道当时NN的连接数状况。<br>如果确实是这个job导致的，为何切换后的NN一直很稳定。<br>先认为是个偶然事件吧。</p>
<p>调查NN切换的过程中倒是发现了checkpoint的另一个bug：<a href="https://issues.apache.org/jira/browse/HDFS-6184" target="_blank" rel="external">HDFS-6184</a>。相关分析见<a href="/2015/04/16/standby-namenode-slow-web/">这篇文章</a>最后。</p>
<h2 id="为何只有少量dead_node？">为何只有少量dead node？</h2>
<p>按理说，这个job能把整个集群搞挂的。为何最多时只挂了6个节点？<br>是因为发现比较及时么？<br>还是说用户数据倾斜比较严重，某些reduce数据量小，所以触发reduce()方法的次数也少？</p>
<h2 id="netstat_-anp为何有重复记录？"><code>netstat -anp</code>为何有重复记录？</h2>
<p>我之前说<code>netstat -anp | grep 1004 | wc -l</code>有8w+记录。但端口号最大就是65535啊，为啥会有这么多连接。<br>其实<code>netstat -anp</code>输出的记录里，有些是重复的，不知为何。如果去重后，就5w左右。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>近日碰到的一个问题，某些用户的程序耗尽了一些节点的socket资源导致dead node。</p>
]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://jxy.me/tags/hadoop/"/>
    
      <category term="hdfs" scheme="http://jxy.me/tags/hdfs/"/>
    
      <category term="socket" scheme="http://jxy.me/tags/socket/"/>
    
      <category term="运维" scheme="http://jxy.me/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Linux豆知识]]></title>
    <link href="http://jxy.me/2015/06/10/linux-tips/"/>
    <id>http://jxy.me/2015/06/10/linux-tips/</id>
    <published>2015-06-10T02:31:49.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>说是linux豆知识，其实主要是运维相关的。</p>
<a id="more"></a>
<h1 id="查看一个进程打开的文件">查看一个进程打开的文件</h1>
<p>进程的很多状态都可以在<code>/proc/$pid</code>目录中查看。<br>查看打开的文件可以看<code>/proc/$pid/fd</code>目录。<br>NM进程也是通过查询这个目录得知container内存占用的。</p>
<h1 id="awk中如何表示单引号">awk中如何表示单引号</h1>
<p>awk使用<code>&#39;{}&#39;</code>定义代码块，所以直接定义一个包含单引号的变量就会出错，比如<code>&#39;{a=&quot;&#39;&quot;;}&#39;</code>。<br>正确方法是用16进制的ASCII码（其他一些字符也是类似方法）：<br><code>awk -F &#39;#&#39; &#39;BEGIN{a=&quot;&quot;;}{a=a&quot;\x27,\x27&quot;$3;}END{print a;}&#39;</code></p>
<h1 id="查看CPU占用最高的java线程">查看CPU占用最高的java线程</h1>
<p>首先获得JVM的pid，然后查看线程<code>top -H -p $pid</code>（JVM的线程是映射到系统的线程上的）。<br>找到CPU最高的线程对应的id，转成16进制。<br><code>jstack -l $pid</code>打印出JVM的堆栈，查找对应的16进制数值即可。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>说是linux豆知识，其实主要是运维相关的。</p>
]]>
    
    </summary>
    
      <category term="linux" scheme="http://jxy.me/tags/linux/"/>
    
      <category term="豆知识" scheme="http://jxy.me/tags/%E8%B1%86%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[关于HDFS的数据可见性]]></title>
    <link href="http://jxy.me/2015/06/09/hdfs-data-visibility/"/>
    <id>http://jxy.me/2015/06/09/hdfs-data-visibility/</id>
    <published>2015-06-09T06:11:26.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>以前写的一篇文章。</p>
<a id="more"></a>
<p>以前一直知道，写入hdfs的数据不会马上可见。<br>稍微看了些代码，总结下。</p>
<h1 id="关于HDFS写">关于HDFS写</h1>
<h2 id="单一写，并发读">单一写，并发读</h2>
<p>传统的文件系统是允许对一个文件并发写入的，只是如果不同步的话，文件内容会乱掉。<a href="http://blog.chinaunix.net/uid-11452714-id-3771084.html" target="_blank" rel="external">http://blog.chinaunix.net/uid-11452714-id-3771084.html</a><br>HDFS不允许并发写，但可以并发读：<a href="http://www.cnblogs.com/ZisZ/p/3253570.html" target="_blank" rel="external">http://www.cnblogs.com/ZisZ/p/3253570.html</a><br>大多数分布式文件系统都不允许并发写，代价太大。</p>
<p>如果多线程试图同时写一个文件，只有一个线程可以正常写，其他线程会抛出AlreadyBeingCreatedException异常：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): failed to <span class="operator"><span class="keyword">create</span> file /tmp/appendTest <span class="keyword">for</span> DFSClient_NONMAPREDUCE_-<span class="number">427798443</span>_10 <span class="keyword">on</span> client <span class="number">172.31</span><span class="number">.132</span><span class="number">.146</span> because <span class="keyword">current</span> leaseholder <span class="keyword">is</span> trying <span class="keyword">to</span> recreate file.</span></div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.<span class="keyword">server</span>.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:<span class="number">2275</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.<span class="keyword">server</span>.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:<span class="number">2153</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.<span class="keyword">server</span>.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:<span class="number">2386</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.<span class="keyword">server</span>.namenode.FSNamesystem.appendFile(FSNamesystem.java:<span class="number">2347</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.<span class="keyword">server</span>.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:<span class="number">508</span>)</div></pre></td></tr></table></figure>

<p>如果一个客户端A获取了lease，但写数据时意外退出，文件没有close，lease不会自己释放（正常close的话lease是会释放的）。<br>只能等时间超过soft limit后，另一个客户端B尝试写同一个文件，NN回收lease；或者时间超过hard limit后lease被NN的一个后台线程回收。</p>
<p>所以如果客户端B尝试写同一个文件，如果还没超出hard limit，第一次尝试必定会失败的，因为同一个文件的lease还被占用着：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.RecoveryInProgressException): Failed <span class="built_in">to</span> <span class="built_in">close</span> <span class="built_in">file</span> /tmp/appendTest. Lease recovery is <span class="operator">in</span> progress. Try again later.</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:<span class="number">2310</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:<span class="number">2153</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:<span class="number">2386</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:<span class="number">2347</span>)</div></pre></td></tr></table></figure>

<p>再尝试时，如果时间已经超过soft limit，才能成功获得lease；否则必定继续失败，只有等超过soft limit后，NN才会把lease分给新的客户端。<br>所以写数据时一般要加上重试机制。<br>可以自己写个程序验证下，用以下命令可以看到正在写还没有close的文件：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="title">hadoop</span> fsck -openforwrite /tmp</div></pre></td></tr></table></figure>

<h2 id="写数据机制">写数据机制</h2>
<p>其实hadoop权威指南已经讲得比较清楚了，这里结合代码复述下。相关的类主要是DFSClient和DFSOutputStream。这块还有点复杂，我也没完全看懂。</p>
<p>我们一般通过FileSystem.create或FileSystem.append方法获得output stream（其实是DFSOutputStream），然后write(byte[])写数据。<br>注意客户端写数据是直接和datanode交互的，只有申请新block时才需要和namenode交互。<br>如果是create，客户端通过RPC协议（ClientProtocol.addBlock方法，这个类名字和mapreduce的RPC重复了。。。）向NN申请一个新block，开始写数据。<br>如果是append，客户端会先判断目标文件的最后一个block是否写满，如果已满就申请新的block，否则就在最后一个block上追加。<br>无论如何，客户端会得到一个目标block用于写入。</p>
<p>NN在分配一个block时还会返回对应的pipeline。如果副本数设置为3，那么pipeline就是3个节点。如果是create，NN挑3个节点组成pipeline（这里有规则的，但对我们这种单机房单机架的来说，就是随机）。如果客户端同时也是DN，那么必定有一个副本同时在当前节点上（这个没从代码上求证过）。如果是append，并且在原来的block上追加数据，那返回的pipeline就是原来的3个节点。</p>
<p>我们调用write(byte[])方法时，数据并没有马上写入pipeline。DFSOutputStream会暂时缓存数据。<br>数据的发送是以packet为单位的，一个packet大小默认64K（dfs.client-write-packet-size，默认65536）。DFSOutputStream内部有两个queue：dataQueue和ackQueue。待写入的数据达到64K时，DFSOutputStream将数据包装成一个packet并放入dataQueue，等待一个守护线程DataStreamer去消费。<br>DataStreamer从dataQueue中取出packet，发到pipeline，将packet加入ackQueue。pipiline中的所有节点都将数据写入后，DataStreamer会收到ack消息，并将packet从ackQueue中移除。这样才算是数据真正写入完毕。</p>
<p>其实一个packet不全是数据。DFSOutputStream会将数据组合成一个个chunk（dfs.bytes-per-checksum，默认512），每一个chunk加一个校验值。默认的校验（dfs.checksum.type，默认CRC32C）需要占用4个字节（见DataChecksum类），也就是说每个chunk实际占用516个字节。一个packet最多存储65536/516=127个chunk。所以，一个packet的实际大小只有127*516=65532字节，其中只有65024个字节是真正的数据。这个计算逻辑见DFSOutputStream：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">computePacketChunkSize</span>(<span class="keyword">int</span> psize, <span class="keyword">int</span> csize) {</div><div class="line">    <span class="keyword">int</span> chunkSize = csize + checksum.getChecksumSize();   <span class="comment">// 512+4</span></div><div class="line">    chunksPerPacket = Math.max(psize/chunkSize, <span class="number">1</span>);   <span class="comment">// 65536/516=127</span></div><div class="line">    packetSize = chunkSize*chunksPerPacket;      <span class="comment">// 127*516=65532</span></div><div class="line">    <span class="keyword">if</span> (DFSClient.LOG.isDebugEnabled()) {</div><div class="line">      DFSClient.LOG.debug(<span class="string">"computePacketChunkSize: src="</span> + src +</div><div class="line">                <span class="string">", chunkSize="</span> + chunkSize +</div><div class="line">                <span class="string">", chunksPerPacket="</span> + chunksPerPacket +</div><div class="line">                <span class="string">", packetSize="</span> + packetSize);</div><div class="line">    }</div><div class="line">  }</div></pre></td></tr></table></figure>

<p>DataStreamer每次写到pipeline的数据也不一定是64K。上面说过，一个满的packet只有65024个字节，而且write时还会生成一个header信息，先写出header，再写出packet本身。如果是最后一个packet，还可能凑不满127个chunk，就更小了。数据也不一定能被512字节整除。还要考虑到写入的数据不能超过block size（block size必须是chunk的整数倍，否则会报错），也会对packet大小做一些调整。</p>
<p>Packet的结构见DFSOutputStream.Packet类。</p>
<p>写数据时，其实是先写到一个512字节的buffer里，写满了就调用flushBuffer()方法计算checksum，将checksum和数据写入currentPacket。如果currentPacket已经写满了，就放入dataQueue，这里会阻塞，因为缓存的packet有个最大值，默认80个（这个80是写死在程序里的，不知为何）。如果已经写到block的最后，还会发送一个空的packet对象，要求DN将数据持久化（这个机制见下面的分析）。之后通过和NN的RPC协议申请一个新的block继续写。</p>
<h1 id="数据可见性">数据可见性</h1>
<p><a href="http://www.cnblogs.com/ZisZ/p/3253354.html" target="_blank" rel="external">http://www.cnblogs.com/ZisZ/p/3253354.html</a>（只能参考。原作者的hadoop版本比较老。）</p>
<p>客户端写入hdfs的数据不是立即可见的。</p>
<p>以前一直以为正在写的整个block都不可见，其实不是。<strong>只要写入pipeline并且ack的数据，都是可见的。</strong></p>
<p>只有缓存在写客户端的数据，对其他读客户端才是不可见的。根据上面的描述，客户端最多缓存80个packet（dataQueue和ackQueue的size之和，这个80是写死在程序里的），每个packet大概64K，所以总共有大概5M的数据不可见。之所以是“大概”，因为packet中有校验数据，而且有一个currentPacket不在queue里。准确的值是65024*81=5.023M，差不多。<br>如果客户端意外挂掉，缓存的数据会完全丢失，也就是说最多丢5M的数据。</p>
<p>但是，写到pipeline的数据虽然能看到，但不能保证不丢失。因为DN端也会将数据缓存（这个缓存机制还不太明白，没看过代码），而不是立即写到磁盘。极端情况下，pipeline里的3个节点都挂掉，写入pipeline的数据也会丢。<br>只有写满一个block时，客户端才会发送一个空的packet，这个packet的header有个特殊的标志位，要求DN将当前block的数据刷到磁盘。<br>所以极端情况下，可能会丢一个block的数据（这是某些资料的说法，我没看DN的代码求证过。感觉上有点问题，难道整个block都缓存在内存里？只有DN内存里的数据会丢吧，如果blocksize设的很大，岂不是很耗内存。所以感觉不太可能丢整个block，应该也是有一个buffer之类的）。不过3个节点一起挂掉的概率很小吧。</p>
<p>虽然写入pipeline的数据对客户端可见了（去读这个文件的话可以读到）。但如果看hadoop fs -ls看这个文件，会发现这个文件的大小没有变化，可能还是0字节。</p>
<p>因为客户端写入数据时只需要和DN交互，NN只知道这个文件有哪个block在写，但写入的数据量是不知道的。客户端读的时候也是直接读DN上的block，所以可以读到pipeline中的数据。<br>只有等一个block写完或者客户端主动close，NN那边才能看到大小的变化（只有这时才会与NN交互）。<br>如果客户端意外挂掉，等超过1个小时（hard limit）文件大小也会变化。<br>如果客户端意外挂掉，另一个客户端1分钟（soft limit）后重新获取lease并且append，上一个客户端写入pipeline的数据也还在的。</p>
<h1 id="hflush和hsync">hflush和hsync</h1>
<p>hflush要求客户端将所有buffer里的数据写入pipeline。之后数据对所有客户端可见。本质就是阻塞所有写入，将currentPacket加入dataQueue（即使currentPacket还没满），然后等待queue中的所有数据都ack。<br>hsync在hflush的基础上，会将currentPacket的isSync标识设为true，DN收到这样一个packet后，会将数据刷入磁盘。即使没有数据要flush，也会新建一个空的packet对象，设置isSync并发送。</p>
<p>这个两个方法都是为了防止数据丢失的。hflush防止客户端缓存的数据丢失，hsync防止客户端和pipeline缓存的数据丢失。<br>即使是hsync，也只是保证数据刷到磁盘，但可能在磁盘的缓存里。所以没有绝对的安全的。<br>而且这两个方法会影响写入的效率。</p>
<p>感觉上，如果对数据可见性有要求，可以定期hflush；客户端挂掉最多丢5M数据，不能接受这种情况，也要定期hflush；其他情况都没必要hflush。<br>hsync完全没必要，写到pipeline的数据已经很安全了。</p>
<p>上面说过写入pipeline的数据不会立即让NN端的文件大小改变。其实hsync时可以强制更新文件大小。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">FileSystem fs = FileSystem.<span class="keyword">get</span>(conf);</div><div class="line"><span class="comment">// 这里要强制转换下。HdfsDataOutputStream才有对应的hsync(EnumSet)方法，普通的DFSOutputStream没有</span></div><div class="line">HdfsDataOutputStream <span class="keyword">out</span> = (HdfsDataOutputStream) fs.create(<span class="keyword">new</span> Path(<span class="string">"/tmp/bigBlock2"</span>));</div><div class="line"><span class="keyword">out</span>.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));</div></pre></td></tr></table></figure>

<p>不用想就知道，肯定对性能影响非常大。</p>
<h1 id="关于HDFS读">关于HDFS读</h1>
<p>其实和本文关系不大，只是顺便整理下。</p>
<p>简单整理下HDFS读数据的机制。</p>
<ul>
<li>网络读。很好理解。客户端直接连DataNode，通过网络传输数据。最常见，适用于各种情况。</li>
<li>本地socket读。如果客户端同时是DataNode，并且要读的数据就在本地，可以省掉网络传输的过程。这也是MapReduce计算本地性的基本原理。“带宽是最宝贵的资源”</li>
<li>本地磁盘读。即Short-Circuit Local Reads。当要读的数据在本地时，可以不走socket，直接用系统调用读磁盘上的文件。效率更高，但需要编译对应系统的native lib。</li>
<li>内存读。即缓存机制。hadoop 2.3.0新增了DataNode端的缓存机制，可以将一些block缓存到内存中。是否适用看应用场景吧。好处是效率高，坏处是额外占用内存，而且这些内存是off-heap的，不受GC管理。</li>
</ul>
<h2 id="关于Short-Circuit_Local_Reads">关于Short-Circuit Local Reads</h2>
<p>关于这个配置还有些问题。</p>
<p>网上有很多文档给出的配置是要dfs.block.local-path-access.user属性的，只有特定的用户才能使用local read。实际上那是老的实现方法（legacy），基于<a href="https://issues.apache.org/jira/browse/HDFS-2246" target="_blank" rel="external">HDFS-2246</a>。<br>这种方法配置麻烦，配置项很多，并且有安全隐患。<br>在hadoop 2.1.0以后的版本已经有了新的实现，基于<a href="https://issues.apache.org/jira/browse/HDFS-347" target="_blank" rel="external">HDFS-347</a>。</p>
<p>新的实现需要libhadoop.so，要在不同系统上分别编译。只需要如下两个配置即可：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.client.read.shortcircuit<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.domain.socket.path<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="title">value</span>&gt;</span>/var/lib/hadoop-hdfs/dn_socket<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>

<p><a href="http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html" target="_blank" rel="external">hadoop 2.5.2的文档</a>中已经给出了2种实现的配置（2.2.0的文档中只有新的实现）。</p>
<p>org.apache.hadoop.hdfs.BlockReaderLocalLegacy类的注释：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">* This <span class="keyword">is</span> the legacy implementation based <span class="keyword">on</span> HDFS-<span class="number">2246</span>, which requires</div><div class="line">* permissions <span class="keyword">on</span> the datanode <span class="keyword">to</span> <span class="keyword">be</span> <span class="keyword">set</span> <span class="keyword">so</span> that clients can directly access the</div><div class="line">* blocks. The <span class="keyword">new</span> implementation based <span class="keyword">on</span> HDFS-<span class="number">347</span> should <span class="keyword">be</span> preferred <span class="keyword">on</span> UNIX</div><div class="line">* systems where the required native code <span class="built_in">has</span> been implemented.&lt;<span class="keyword">br</span>&gt;</div></pre></td></tr></table></figure>

<p>参考：<br><a href="http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/" target="_blank" rel="external">http://blog.cloudera.com/blog/2013/08/how-improved-short-circuit-local-reads-bring-better-performance-and-security-to-hadoop/</a><br><a href="http://www.importnew.com/6151.html" target="_blank" rel="external">http://www.importnew.com/6151.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>以前写的一篇文章。</p>
]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://jxy.me/tags/hadoop/"/>
    
      <category term="hdfs" scheme="http://jxy.me/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[一个有趣的federation部署]]></title>
    <link href="http://jxy.me/2015/06/02/funny-federation/"/>
    <id>http://jxy.me/2015/06/02/funny-federation/</id>
    <published>2015-06-02T10:07:21.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>最近碰到一个有意思的问题。QA需要3个集群用于测试，但只有5台机器，应该怎么搞？</p>
<a id="more"></a>
<p>常规的想法就是换端口、换dfs.[name|data].dir，在同一台机器上跑多个集群的daemon了。<br>这应该是可行的，虽然我没试过。<br>而且这批机器配置比较烂，跑多个daemon可能很慢。</p>
<p>突发奇想，是否可以用federation模拟多个hdfs。对federation而言，其实各个NN都是独立的。<br>只是一个DN会为多个NN服务，分多个BlockPool。<br>YARN先不考虑，用同一套就好了。</p>
<p>我们以前也测试过federation，那时用了4台机器，部署2个HA的NN。如果采用同样的方式，模拟3个hdfs要至少6台机器。<br>如果不开启HA，倒是3台机器就可以。但QA也需要测试一些HA相关的东西。</p>
<p>又一次突发奇想，在一个federation的集群中，是否可以同时存在HA和非HA的NN？这样我就可以部署一个HA的NN，2个非HA的NN，用4台机器模拟3个hdfs。</p>
<p>经测试，这是可行的。记录下部署过程和一些问题。<br>简单起见，以部署1个HA的NN+1个非HA的NN为例。</p>
<h1 id="修改配置文件">修改配置文件</h1>
<p>角色：inspur116是一个非HA的NN；inspur122和inspur129组成一对HA的NN，其中inspur122 Active，inspur129 StandBy。</p>
<p>只要修改core-site.xml和hdfs-site.xml即可，其他配置都和正常的一样。</p>
<p>core-site.xml中注释掉<code>fs.defaultFS</code>属性。因为在federation中有多个NN，使用哪个NN应该由客户端选择，而不要在服务端写死。即使是“普通”的federation，也务必要取消这个属性。</p>
<p>hdfs-site.xml中修改一些属性：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hp1,hp2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.namenodes.hp1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.rpc-address.hp1.nn1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>inspur122.photo.163.org:8020<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.rpc-address.hp1.nn2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>inspur129.photo.163.org:8020<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.rpc-address.hp2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>inspur116.photo.163.org:8020<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.client.failover.proxy.provider.hp1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>

<p>只列出了一些关键配置。注意HA和非HA配置的一些不同。对于HA NN而言，必须配置<code>dfs.client.failover.proxy.provider</code>属性。<br>其他配置，比如journal node、dfs.name.dir之类的，保持原样即可。</p>
<h1 id="分发配置">分发配置</h1>
<p>将core-site.xml和hdfs-site.xml分发到所有节点。<br>然后修改inspur116上的hdfs-site.xml，注释掉<code>dfs.namenode.shared.edits.dir</code>属性。因为对于一个非HA的NN而言，不能有这个属性，否则启动会报错。</p>
<h1 id="启动服务">启动服务</h1>
<p>和启动普通的federation类似。关键在于namenode format时用相同的clusterId。</p>
<p>启动ANN(inspur122)的过程不多说了，format、initShareEdits、formatZK、start zkfc、start namenode。<br>format后会得到一个clusterId:<code>CID-f19b006a-63a3-4f69-9069-1a2baf7599d3</code>，然后到inspur116上执行：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">sudo</span> /home/hadoop/hadoop-current/bin/hdfs namenode -format -clusterId CID<span class="operator">-f</span>19b006a-<span class="number">63</span>a3-<span class="number">4</span>f69-<span class="number">9069</span>-<span class="number">1</span>a2baf7599d3</div></pre></td></tr></table></figure>

<p>之后再启动inspur116的NN。对于非HA的NN而言就没那么多步骤了，直接用hadoop-daemon.sh启动即可。<br>inspur129稍后再启动。</p>
<p>观察web界面，inspur116和inspur122都正常。<br>启动DN，inspur116和inspur122同时收到心跳。<br>分别向inspur116和inspur122写入数据，可以证明这是两个独立的hdfs。</p>
<p>启动SNN(inspur129)。bootstrapStandby、start zkfc、start namenode。观察web界面，SNN也收到了所有DN的心跳。</p>
<p>强制关掉inspur122上的NN，正常failover到inspur129，元数据也正常。</p>
<p>至此hdfs启动完毕。yarn的启动很简单，不多说了。</p>
<h1 id="一些问题">一些问题</h1>
<ul>
<li>启动SNN时，可能要先saveNamespace才能bootstrapStandby。而dfsadmin相关命令是需要fs.defaultFS属性的。可以先取消core-site.xml中的fs.defaultFS的注释，再执行saveNamespace。</li>
<li>分发客户端时，加上fs.defalutFS属性。因为是模拟多个集群，用户不应该感知到背后federation的存在。我们是模拟3个集群，所以有3个客户端配置。用户可以直接用不同客户端读写对应的hdfs并提交YARN任务。</li>
<li>2个HA的NN+1个非HA的NN应该也是可以的。journal node应该是可以共用的。不想折腾了，就没测试。</li>
</ul>
<h1 id="log_aggression失效">log aggression失效</h1>
<p>所有federation集群都存在这个问题。<br>因为取消了fs.defaultFS属性，导致NM端做log aggression时，不会传到hdfs上，而是传到本地磁盘。<br>比如我们的设置中，log aggression的目录：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/var/log/hadoop-yarn/apps<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>

<p>NM在一个container执行结束后，应该把它的日志上传到hdfs的/var/log/hadoop-yarn目录下。但取消了fs.defaultFS后，FileSystem.get方法会认为默认的URI是<code>file:///</code>，而去操作本地磁盘。于是NM的日志也传到本地磁盘了：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 每个用户的日志是分目录存放的</span></div><div class="line">hadoop<span class="variable">@classb</span>-<span class="symbol">bigdata4:</span>~<span class="variable">$ </span>ls /var/log/hadoop-yarn/apps</div><div class="line">hadoop  hdfs  intern  qatest  test_hadoop_maomao</div></pre></td></tr></table></figure>

<p>如果在启动NM的时候，加上fs.defaultFS属性，比如设置成<code>hdfs://inspur116.photo.163.org:8020</code>，log aggression就正常了，所有日志都会传到这个hdfs上。但有些副作用。</p>
<p>当NM端没有fs.defaultFS、客户端有fs.defaultFS时，可以用类似下面的命令提交任务：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop jar wordcount.jar wordcount <span class="regexp">/user/</span>test<span class="regexp">/input /</span>user<span class="regexp">/test/</span>output</div></pre></td></tr></table></figure>

<p>这样我只要改动客户端的配置，就可以在YARN中操作不同hdfs的数据，用户不会感知到。</p>
<p>如果NM端有fs.defaultFS，无论客户端如何配置，都会以NM的为准。以上面的命令为例，如果/user/test/input不在NM端指向的那个hdfs上，就会出错。<br>用户提交任务时必须这么写，明确指定要访问哪个NN：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop jar wordcount.jar wordcount <span class="symbol">hdfs:</span>/<span class="regexp">/hp1/user</span><span class="regexp">/test/input</span> <span class="symbol">hdfs:</span>/<span class="regexp">/hp1/user</span><span class="regexp">/test/output</span></div></pre></td></tr></table></figure>

<p>这相当于<a href="/2015/06/01/multiple-namenode/">跨集群访问</a>了。</p>
<p><strong>问题的根源在于fs.defalutFS这个属性是final的，客户端不能覆盖服务端的配置。</strong><br>如果不是final，应该就可以了，但我没试过。</p>
<p>曾经尝试过直接修改log aggression的目标路径：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://inspur116.photo.163.org:8020/var/log/hadoop-yarn/apps<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>

<p>但不行。hadoop似乎没有把这个属性当一个URI来处理。提交任务时NM会有异常：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="number">2015</span>-<span class="number">06</span>-<span class="number">01</span> <span class="number">23</span>:<span class="number">08</span>:<span class="number">30</span>,<span class="number">970</span> INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Application application_1432887785054_0083 transitioned <span class="built_in">from</span> NEW <span class="built_in">to</span> INITING</div><div class="line"><span class="number">2015</span>-<span class="number">06</span>-<span class="number">01</span> <span class="number">23</span>:<span class="number">08</span>:<span class="number">30</span>,<span class="number">970</span> INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Adding container_1432887785054_0083_01_000001 <span class="built_in">to</span> application application_1432887785054_0083</div><div class="line"><span class="number">2015</span>-<span class="number">06</span>-<span class="number">01</span> <span class="number">23</span>:<span class="number">08</span>:<span class="number">31</span>,<span class="number">049</span> FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error <span class="operator">in</span> dispatcher thread</div><div class="line">java.lang.IllegalArgumentException: Wrong FS: hdfs://classb-bigdata5.server<span class="number">.163</span>.org:<span class="number">8020</span>/var/<span class="built_in">log</span>/hadoop-yarn/apps, expected: <span class="built_in">file</span>:<span class="comment">///</span></div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:<span class="number">642</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:<span class="number">69</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:<span class="number">516</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:<span class="number">398</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.verifyAndCreateRemoteLogDir(LogAggregationService.java:<span class="number">181</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initApp(LogAggregationService.java:<span class="number">301</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:<span class="number">413</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:<span class="number">64</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.yarn.event.AsyncDispatcher.<span class="built_in">dispatch</span>(AsyncDispatcher.java:<span class="number">134</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.yarn.event.AsyncDispatcher$<span class="number">1.</span>run(AsyncDispatcher.java:<span class="number">81</span>)</div><div class="line">        <span class="keyword">at</span> java.lang.Thread.run(Thread.java:<span class="number">662</span>)</div><div class="line"><span class="number">2015</span>-<span class="number">06</span>-<span class="number">01</span> <span class="number">23</span>:<span class="number">08</span>:<span class="number">31</span>,<span class="number">050</span> INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye..</div></pre></td></tr></table></figure>

<p>任务就卡住了。杀掉这个任务后NM也自杀了。。。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>最近碰到一个有意思的问题。QA需要3个集群用于测试，但只有5台机器，应该怎么搞？</p>
]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://jxy.me/tags/hadoop/"/>
    
      <category term="federation" scheme="http://jxy.me/tags/federation/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[checkpoint异常需要手动合并editlog]]></title>
    <link href="http://jxy.me/2015/06/01/checkpoint-error/"/>
    <id>http://jxy.me/2015/06/01/checkpoint-error/</id>
    <published>2015-06-01T09:29:08.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>最近我们集群的SNN发生了一些异常，记录下。</p>
<a id="more"></a>
<p>关于SNN的作用，见<a href="/2015/04/16/standby-namenode-slow-web/">我以前的一篇文章</a>。</p>
<h1 id="关于txid">关于txid</h1>
<p>txid用于标记一条editlog。<br>一般在ANN的dfs.name.dir有几种文件：</p>
<ul>
<li>fsimage_XXX，表示这个fsimage文件已经将txid=XXX的editlog合并进来了。这种文件必定有一个同名的md5文件对应。</li>
<li>fsimage.ckpt_XXX，表示正在做checkpoint的一个fsimage文件，其实就是正在从SNN copy的一个文件</li>
<li>edits_XXX-YYY，包含从XXX到YYY的所有editlog</li>
<li>edits_inprogress_XXX，当前正在写的editlog文件，初始的txid是XXX</li>
</ul>
<p>NN启动时会加载txid最大的一个fsimage_XXX文件。然后读取XXX之后的所有editlog并合并。如果editlog不连续，NN无法启动。<br>当NN启动完成后，会自动做一次checkpoint。之后开始写新的edits_inprogress文件。</p>
<h1 id="异常原因">异常原因</h1>
<p>4月30号删除了大量数据，fsimage文件大小从6G减小到3G。<br>由于找不回当时的日志了，不知道发生了什么。原来的fsimage_AAA有6G左右，经过一次checkpoint后生成了一个3G多的fsimage_BBB文件。<br>但这个BBB不是任何edits文件的边界，所以无法基于这个fsimage_BBB文件继续合并editlog。<br>journalnode的日志：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">2015-05-02 00:00:14,323 WARN org.apache.hadoop.ipc.Server: IPC Server <span class="operator"><span class="keyword">handler</span> <span class="number">4</span> <span class="keyword">on</span> <span class="number">8485</span>, <span class="keyword">call</span> org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol.getEd</span></div><div class="line">itLogManifest <span class="keyword">from</span> <span class="number">172.17</span><span class="number">.1</span><span class="number">.10</span>:<span class="number">48924</span> <span class="keyword">Call</span>#<span class="number">572756</span> Retry#<span class="number">0</span>: error: java.lang.IllegalStateException: Asked <span class="keyword">for</span> firstTxId <span class="number">1604743888</span> which <span class="keyword">is</span> <span class="keyword">in</span> the middle <span class="keyword">of</span> fi</div><div class="line">le /home/hadoop/jn/hz-cluster1/<span class="keyword">current</span>/edits_0000000001604742607-<span class="number">0000000001604746980</span></div><div class="line">java.lang.IllegalStateException: Asked <span class="keyword">for</span> firstTxId <span class="number">1604743888</span> which <span class="keyword">is</span> <span class="keyword">in</span> the middle <span class="keyword">of</span> file /home/hadoop/jn/hz-cluster1/<span class="keyword">current</span>/edits_0000000001604742607-</div><div class="line"><span class="number">0000000001604746980</span></div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.<span class="keyword">server</span>.namenode.FileJournalManager.getRemoteEditLogs(FileJournalManager.java:<span class="number">198</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.qjournal.<span class="keyword">server</span>.Journal.getEditLogManifest(Journal.java:<span class="number">640</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.qjournal.<span class="keyword">server</span>.JournalNodeRpcServer.getEditLogManifest(JournalNodeRpcServer.java:<span class="number">181</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.getEditLogManifest(QJournalProtocolServerSideTranslatorPB.java:<span class="number">203</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$<span class="number">2.</span>callBlockingMethod(QJournalProtocolProtos.java:<span class="number">17453</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ipc.ProtobufRpcEngine$<span class="keyword">Server</span>$ProtoBufRpcInvoker.<span class="keyword">call</span>(ProtobufRpcEngine.java:<span class="number">585</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ipc.RPC$<span class="keyword">Server</span>.<span class="keyword">call</span>(RPC.java:<span class="number">928</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ipc.<span class="keyword">Server</span>$<span class="keyword">Handler</span>$<span class="number">1.</span>run(<span class="keyword">Server</span>.java:<span class="number">2048</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ipc.<span class="keyword">Server</span>$<span class="keyword">Handler</span>$<span class="number">1.</span>run(<span class="keyword">Server</span>.java:<span class="number">2044</span>)</div><div class="line">    <span class="keyword">at</span> java.security.AccessController.doPrivileged(Native Method)</div><div class="line">    <span class="keyword">at</span> javax.security.auth.Subject.doAs(Subject.java:<span class="number">396</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<span class="number">1491</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ipc.<span class="keyword">Server</span>$<span class="keyword">Handler</span>.run(<span class="keyword">Server</span>.java:<span class="number">2042</span>)</div></pre></td></tr></table></figure>

<p>无论是NN直接读取editlog，还是通过JN读取editlog，都必须从一个edits文件的边界开始读，不能一个中间的txid开始读。<br>从生成这个文件开始，SNN无论尝试rollEditLog还是checkpoint都会直接失败。因为读取editlog会直接抛异常。<br>无法rollEditLog，导致ANN一直在写edits_in_process文件，这个文件非常大，将近10G。<br>无法checkpoint，导致ANN的fsimage停在BBB版本。如果failover肯定会失败。</p>
<h1 id="解决过程">解决过程</h1>
<p>4月30号之后，这个问题一直没被发现。<br>5月10号，萧山ANN出了点问题，碰到一个bug：<a href="https://issues.apache.org/jira/browse/HDFS-6289" target="_blank" rel="external">HDFS-6289</a>。<br>5月11号，为了防止同样的问题，打算重启下滨江的SNN（hadoop0.photo）。发现启动后总是显示有missing block。虽然能收到所有节点的心跳。<br>检查了hadoop0的日志，发现了之前说的读取editlog的异常。怀疑是元数据有问题。<br>看了下dfs.name.dir，发现了一个10G的edits文件，非常震惊。。。分析日志后发现了fsimage_BBB文件的异常。<br>万幸这个问题不影响ANN的正常功能。但风险很大，一旦ANN挂掉，就起不来了。</p>
<h2 id="让SNN合并editlog">让SNN合并editlog</h2>
<p>基于fsimage_BBB文件NN不可能正常启动。好在NN会保留fsimage的2个备份。原来6G多的fsimage_AAA还在，这个文件是正常的，AAA之后所有的editlog也在。只要基于这个文件，手动生成最新的fsimage即可。<br>首先尝试让hadoop0自己去合并，删掉dfs.name.dir中的fsimage_BBB即可。SNN启动过程中会加载txid最大的一个fsimage。<br>由于hadoop0是HA的配置，所以所有editlog都是从journal node中读取的。<br>总共有10G左右的editlog要合并（10多天的量），editlog要一条一条回放，要15小时左右。<br>于是放着NN进程自己合并，明天再来观察。</p>
<p>5月12号，检查hadoop0的日志，发现虽然合并editlog成功了，但之后报了一个kerberos的异常。NN还是无法启动。<br>猜测是kerberos ticket超过10小时过期了，之后NN读取journal node异常，直接自杀。<br>正常情况下NN中应该有线程负责定期更新ticket的，但合并editlog的过程却没有更新，因为NN相关的服务还没启动。<br>可能设计的时候没想到这种情况。</p>
<h2 id="单独启动一个NN">单独启动一个NN</h2>
<p>尝试启动一个单机的、关闭security的NN进程，用来合并editlog。这样NN会直接从dfs.name.dir读文件，而不是读journal node，也不存在kerberos的问题了。<br>合并后的文件应该可以通用的。<br>如果这个方法还不行，就只能分析fsimage的结构，自己写程序处理了。</p>
<p>在inspur131上直接copy一份tar包，修改配置，只保留几个配置文件：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">hadoop<span class="variable">@inspur131</span><span class="symbol">:~/jxy/edits/hadoop-</span><span class="number">2.2</span>.<span class="number">0</span><span class="variable">$ </span>ls -lh etc/hadoop</div><div class="line">总用量 <span class="number">24</span>K</div><div class="line">-rw-r--r-- <span class="number">1</span> hadoop netease  <span class="number">441</span>  <span class="number">5</span>月 <span class="number">12</span> <span class="number">12</span><span class="symbol">:</span><span class="number">20</span> core-site.xml</div><div class="line">-rw-r--r-- <span class="number">1</span> hadoop netease <span class="number">4.0</span>K  <span class="number">5</span>月 <span class="number">12</span> <span class="number">12</span><span class="symbol">:</span><span class="number">21</span> hadoop-env.sh</div><div class="line">-rw-r--r-- <span class="number">1</span> hadoop netease  <span class="number">575</span>  <span class="number">5</span>月 <span class="number">12</span> <span class="number">12</span><span class="symbol">:</span><span class="number">20</span> hdfs-site.xml</div><div class="line">-rw-r--r-- <span class="number">1</span> hadoop netease <span class="number">9.3</span>K  <span class="number">5</span>月 <span class="number">12</span> <span class="number">12</span><span class="symbol">:</span><span class="number">20</span> log4j.properties</div></pre></td></tr></table></figure>

<p>core-site.xml里只保留一个属性：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://inspur131.photo.163.org:8020<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>

<p>hdfs-site.xml也只保留一个属性：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/hadoop/jxy/edits/name<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>

<p>hadoop-env.sh里只设置JAVA_HOME属性。<br>以上就是一个单机的NN进程需要的最小配置了。</p>
<p>将fsimage_AAA文件和需要的所有edits文件，从NN拷到inspur131上，/home/hadoop/jxy/edits/name目录。然后启动NN：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 不开启security，也就不用sudo了</span></div><div class="line">hadoop<span class="variable">@inspur131</span><span class="symbol">:~/jxy/edits/hadoop-</span><span class="number">2.2</span>.<span class="number">0</span>/sbin<span class="variable">$ </span>./hadoop-daemon.sh start namenode</div></pre></td></tr></table></figure>

<p>由于没有设置HADOOP_LOG_DIR，日志会放在HADOOP_HOME/logs。<br>在NN的web界面可以看到合并的进度。</p>
<p>5月13号，inspur131上的NN进程合并完毕，总共用了15小时左右：<br><img src="/2015/06/01/checkpoint-error/nn.png" alt=""></p>
<p>由于没有启动DN，所以NN会一直处于safemode。<br>观察dfs.name.dir，已成功生成fsimage文件。<br><strong>editlog合并结束会自动saving checkpoint，见上图，不要自己-saveNamespace。</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">hadoop@inspur131:~/jxy/edits/name/current$</span> <span class="comment">ls</span> <span class="literal">-</span><span class="comment">lh</span> <span class="comment">|</span> <span class="comment">tail</span></div><div class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">1</span> <span class="comment">hadoop</span> <span class="comment">netease</span> <span class="comment">2</span><span class="string">.</span><span class="comment">7M</span>  <span class="comment">5月</span> <span class="comment">12</span> <span class="comment">11:51</span> <span class="comment">edits_0000000001604746981</span><span class="literal">-</span><span class="comment">0000000001604768189</span></div><div class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">1</span> <span class="comment">hadoop</span> <span class="comment">netease</span>  <span class="comment">10G</span>  <span class="comment">5月</span> <span class="comment">12</span> <span class="comment">11:55</span> <span class="comment">edits_0000000001604768190</span><span class="literal">-</span><span class="comment">0000000001681027556</span></div><div class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">1</span> <span class="comment">hadoop</span> <span class="comment">netease</span>  <span class="comment">16M</span>  <span class="comment">5月</span> <span class="comment">12</span> <span class="comment">11:55</span> <span class="comment">edits_0000000001681027557</span><span class="literal">-</span><span class="comment">0000000001681148275</span></div><div class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">1</span> <span class="comment">hadoop</span> <span class="comment">netease</span> <span class="comment">1</span><span class="string">.</span><span class="comment">0M</span>  <span class="comment">5月</span> <span class="comment">13</span> <span class="comment">05:50</span> <span class="comment">edits_inprogress_0000000001681148276</span></div><div class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">1</span> <span class="comment">hadoop</span> <span class="comment">netease</span> <span class="comment">6</span><span class="string">.</span><span class="comment">3G</span>  <span class="comment">5月</span> <span class="comment">12</span> <span class="comment">11:57</span> <span class="comment">fsimage_0000000001604721018</span></div><div class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">1</span> <span class="comment">hadoop</span> <span class="comment">netease</span>   <span class="comment">62</span>  <span class="comment">5月</span> <span class="comment">12</span> <span class="comment">11:57</span> <span class="comment">fsimage_0000000001604721018</span><span class="string">.</span><span class="comment">md5</span></div><div class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">1</span> <span class="comment">hadoop</span> <span class="comment">netease</span> <span class="comment">3</span><span class="string">.</span><span class="comment">2G</span>  <span class="comment">5月</span> <span class="comment">13</span> <span class="comment">05:50</span> <span class="comment">fsimage_0000000001681148275</span></div><div class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">1</span> <span class="comment">hadoop</span> <span class="comment">netease</span>   <span class="comment">62</span>  <span class="comment">5月</span> <span class="comment">13</span> <span class="comment">05:50</span> <span class="comment">fsimage_0000000001681148275</span><span class="string">.</span><span class="comment">md5</span></div><div class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">1</span> <span class="comment">hadoop</span> <span class="comment">netease</span>   <span class="comment">11</span>  <span class="comment">5月</span> <span class="comment">13</span> <span class="comment">05:50</span> <span class="comment">seen_txid</span></div><div class="line"><span class="literal">-</span><span class="comment">rw</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span><span class="comment">r</span><span class="literal">-</span><span class="literal">-</span> <span class="comment">1</span> <span class="comment">hadoop</span> <span class="comment">netease</span>  <span class="comment">207</span>  <span class="comment">5月</span> <span class="comment">13</span> <span class="comment">05:50</span> <span class="comment">VERSION</span></div></pre></td></tr></table></figure>

<h2 id="启动SNN">启动SNN</h2>
<p>将最新生成的fsimage文件拷到hadoop0，启动SNN。SNN会合并一些最新的editlog（11号、12号新生成的editlog，有2G多）。<br>接下来SNN进程却卡住了。接着有用户反应无法读写hdfs。观察RM界面，job能提交上来，但是一直在ACCPETED状态，无法运行。<br>大概持续了20分钟，杀掉SNN进程后，用户使用恢复正常。<br>由于HA集群中，客户端要先去查询hadoop0是否active。hadoop0卡住导致用户也无法使用。相关原理见<a href="/2015/04/09/hadoop-ha-active-nn/">之前的文章</a>。</p>
<p>猜测SNN卡住是在处理DN的心跳。所以打算利用NN的白名单机制，一次只加几个DN节点。<br>重新尝试启动SNN，白名单里只有一个DN。启动后SNN也是卡住了，但却没有影响用户的读写。过了10几分钟，NN回复正常。<br>尝试在白名单里加更多的DN节点，每次SNN都会卡一阵，但不阻塞hdfs读写。<br>最后所有DN都加进去，SNN成功离开safemode。<br>观察dfs.name.dir，checkpoint也正常了。</p>
<h1 id="后续">后续</h1>
<ol>
<li>SNN为什么会卡住？当时用jstack看是卡在一个读取editlog的线程上。但具体原因不明。</li>
<li>当时急于回复服务，很多现场没能保留下来。日志也找不回来了。以后应该注意下。</li>
<li>应该监控checkpoint过程。两种方式：监控SNN的日志，看是否正常触发；或者监控ANN的dfs.name.dir，看是否定期生成editlog和fsimage。</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>最近我们集群的SNN发生了一些异常，记录下。</p>
]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://jxy.me/tags/hadoop/"/>
    
      <category term="namenode" scheme="http://jxy.me/tags/namenode/"/>
    
      <category term="HA" scheme="http://jxy.me/tags/HA/"/>
    
      <category term="checkpoint" scheme="http://jxy.me/tags/checkpoint/"/>
    
      <category term="运维" scheme="http://jxy.me/tags/%E8%BF%90%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[跨集群访问]]></title>
    <link href="http://jxy.me/2015/06/01/multiple-namenode/"/>
    <id>http://jxy.me/2015/06/01/multiple-namenode/</id>
    <published>2015-06-01T09:07:48.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>简单总结下跨集群访问的多种方式。</p>
<a id="more"></a>
<h1 id="跨集群访问HDFS">跨集群访问HDFS</h1>
<h2 id="直接给出HDFS_URI">直接给出HDFS URI</h2>
<p>我们平常执行<code>hadoop fs -ls /</code>之类的操作时，其实是读取的core-site.xml中fs.defaultFS的配置，去连接这个配置指定的NameNode。<br>其实可以直接给出完整的URI，即可访问不同的hdfs：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">hadoop@inspur116:~<span class="regexp">/jxy/</span>test<span class="regexp">/multiple_nn/</span>hadoop-<span class="number">2.2</span>.<span class="number">0</span><span class="regexp">/bin$ ./</span>hadoop fs -ls hdfs:<span class="comment">//hadoop40.photo.163.org:8020/user</span></div><div class="line">Found <span class="number">17</span> items</div><div class="line">drwx------   - ad            hdfs              <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">09</span> <span class="number">14</span>:<span class="number">48</span> hdfs:<span class="comment">//hadoop40.photo.163.org:8020/user/ad</span></div><div class="line">drwxr-x---   - click_tracker hdfs              <span class="number">0</span> <span class="number">2015</span>-<span class="number">01</span>-<span class="number">22</span> <span class="number">16</span>:<span class="number">09</span> hdfs:<span class="comment">//hadoop40.photo.163.org:8020/user/click_tracker</span></div><div class="line">drwxr-xr-x   - comic         hdfs              <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">10</span> <span class="number">14</span>:<span class="number">48</span> hdfs:<span class="comment">//hadoop40.photo.163.org:8020/user/comic</span></div><div class="line">drwxr-x---   - easegame      hdfs              <span class="number">0</span> <span class="number">2015</span>-<span class="number">01</span>-<span class="number">13</span> <span class="number">14</span>:<span class="number">49</span> hdfs:<span class="comment">//hadoop40.photo.163.org:8020/user/easegame</span></div><div class="line"> </div><div class="line">hadoop@inspur116:~<span class="regexp">/jxy/</span>test<span class="regexp">/multiple_nn/</span>hadoop-<span class="number">2.2</span>.<span class="number">0</span><span class="regexp">/bin$ ./</span>hadoop fs -ls hdfs:<span class="comment">//backend3.photo.163.org:8020/user</span></div><div class="line">Found <span class="number">40</span> items</div><div class="line">drwx------   - ad            hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">01</span>-<span class="number">19</span> <span class="number">13</span>:<span class="number">35</span> hdfs:<span class="comment">//backend3.photo.163.org:8020/user/ad</span></div><div class="line">drwx------   - azkaban       hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">02</span> <span class="number">10</span>:<span class="number">38</span> hdfs:<span class="comment">//backend3.photo.163.org:8020/user/azkaban</span></div><div class="line">drwxr-x---   - blog          hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">02</span> <span class="number">10</span>:<span class="number">38</span> hdfs:<span class="comment">//backend3.photo.163.org:8020/user/blog</span></div><div class="line">drwx------   - censor        hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">10</span> <span class="number">03</span>:<span class="number">00</span> hdfs:<span class="comment">//backend3.photo.163.org:8020/user/censor</span></div></pre></td></tr></table></figure>

<p>这种方式缺点在于要知道NN的地址。如果不开启HA的话没什么问题，但开启HA后，NN的状态可能会在active和standby之间变化。如果去连接standby NN就会抛出异常。<br>而且这种写法也让HA失去意义了。我见过一些奇葩的用户，程序里直接写死了NN的IP地址。。。那还要HA干什么。</p>
<h2 id="在客户端配置多个HA_HDFS">在客户端配置多个HA HDFS</h2>
<p>其实就是HDFS Federation的配置。<br>关于HA的配置，其实完全是客户端的，跟服务端没关系，见<a href="http://jxy.me/2015/04/09/hadoop-ha-active-nn/" target="_blank" rel="external">HA集群中如何判断ActiveNN</a>。<br>所以可以修改客户端的hdfs-site.xml，修改以下配置：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- hz-cluster1是本来就有的，加一个hz-cluster2 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hz-cluster1,hz-cluster2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!-- 以下是新增的hz-cluster2的HA配置 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.namenodes.hz-cluster2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.rpc-address.hz-cluster2.nn1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop40.photo.163.org:8020<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.rpc-address.hz-cluster2.nn2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop41.photo.163.org:8020<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.http-address.hz-cluster2.nn1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop40.photo.163.org:50070<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.http-address.hz-cluster2.nn2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop41.photo.163.org:50070<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.client.failover.proxy.provider.hz-cluster2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>

<p>然后即可访问多个集群：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># fs.defaultFS是hz-cluster1，所以不指定完整URI时，访问hz-cluster1</span></div><div class="line">hadoop@inspur116:~/jxy/test/multiple_nn/hadoop-<span class="number">2.2</span>.<span class="number">0</span>/bin$ ./hadoop fs -ls /<span class="literal">user</span></div><div class="line">Found <span class="number">40</span> items</div><div class="line">drwx------   - ad            hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">01</span>-<span class="number">19</span> <span class="number">13</span>:<span class="number">35</span> /<span class="literal">user</span>/ad</div><div class="line">drwx------   - azkaban       hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">02</span> <span class="number">10</span>:<span class="number">38</span> /<span class="literal">user</span>/azkaban</div><div class="line">drwxr-x---   - blog          hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">02</span> <span class="number">10</span>:<span class="number">38</span> /<span class="literal">user</span>/blog</div><div class="line">drwx------   - censor        hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">10</span> <span class="number">03</span>:<span class="number">00</span> /<span class="literal">user</span>/censor</div><div class="line"> </div><div class="line">hadoop@inspur116:~/jxy/test/multiple_nn/hadoop-<span class="number">2.2</span>.<span class="number">0</span>/bin$ ./hadoop fs -ls hdfs://hz-cluster1/<span class="literal">user</span></div><div class="line">Found <span class="number">40</span> items</div><div class="line">drwx------   - ad            hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">01</span>-<span class="number">19</span> <span class="number">13</span>:<span class="number">35</span> hdfs://hz-cluster1/<span class="literal">user</span>/ad</div><div class="line">drwx------   - azkaban       hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">02</span> <span class="number">10</span>:<span class="number">38</span> hdfs://hz-cluster1/<span class="literal">user</span>/azkaban</div><div class="line">drwxr-x---   - blog          hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">02</span> <span class="number">10</span>:<span class="number">38</span> hdfs://hz-cluster1/<span class="literal">user</span>/blog</div><div class="line">drwx------   - censor        hdfs            <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">10</span> <span class="number">03</span>:<span class="number">00</span> hdfs://hz-cluster1/<span class="literal">user</span>/censor</div><div class="line"> </div><div class="line">hadoop@inspur116:~/jxy/test/multiple_nn/hadoop-<span class="number">2.2</span>.<span class="number">0</span>/bin$ ./hadoop fs -ls hdfs://hz-cluster2/<span class="literal">user</span></div><div class="line">Found <span class="number">17</span> items</div><div class="line">drwx------   - ad            hdfs              <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">09</span> <span class="number">14</span>:<span class="number">48</span> hdfs://hz-cluster2/<span class="literal">user</span>/ad</div><div class="line">drwxr-x---   - click_tracker hdfs              <span class="number">0</span> <span class="number">2015</span>-<span class="number">01</span>-<span class="number">22</span> <span class="number">16</span>:<span class="number">09</span> hdfs://hz-cluster2/<span class="literal">user</span>/click_tracker</div><div class="line">drwxr-xr-x   - comic         hdfs              <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">10</span> <span class="number">14</span>:<span class="number">48</span> hdfs://hz-cluster2/<span class="literal">user</span>/comic</div><div class="line">drwxr-x---   - easegame      hdfs              <span class="number">0</span> <span class="number">2015</span>-<span class="number">01</span>-<span class="number">13</span> <span class="number">14</span>:<span class="number">49</span> hdfs://hz-cluster2/<span class="literal">user</span>/easegame</div></pre></td></tr></table></figure>

<p>好处就是利用了HA，一台NN挂掉时不用改配置。</p>
<h2 id="利用ViewFs">利用ViewFs</h2>
<p><a href="http://hadoop.apache.org/docs/r2.5.2/hadoop-project-dist/hadoop-hdfs/ViewFs.html" target="_blank" rel="external">官方文档</a></p>
<p>这个功能本来是用于HDFS Federation的，但也可以用于多集群。<br>类似linux的mount命令，将多个hdfs挂载到不同的目录。在hdfs-site.xml中加上相关配置：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.viewfs.mounttable.dummyCluster.link./binjiang<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://hz-cluster1/<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line">    <span class="comment">&lt;!-- 这里用了HA的配置，也可以写死NN地址 --&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.viewfs.mounttable.dummyCluster.link./xiaoshan<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://hz-cluster2/<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>

<p>修改core-site.xml的配置：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>viewfs://dummyCluster<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>

<p>然后即可访问多个集群：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">hadoop@inspur116:~/jxy/test/multiple_nn/hadoop-<span class="number">2.2</span><span class="number">.0</span>/bin$ ./hadoop fs -ls /</div><div class="line">Found <span class="number">2</span> <span class="keyword">items</span></div><div class="line">-r<span class="comment">--r--r--   - da@HADOOP.HZ.NETEASE.COM lofter_apache          0 2015-02-10 18:57 /binjiang</span></div><div class="line">-r<span class="comment">--r--r--   - da@HADOOP.HZ.NETEASE.COM lofter_apache          0 2015-02-10 18:57 /xiaoshan</span></div><div class="line"> </div><div class="line">hadoop@inspur116:~/jxy/test/multiple_nn/hadoop-<span class="number">2.2</span><span class="number">.0</span>/bin$ ./hadoop fs -ls /binjiang/user</div><div class="line">Found <span class="number">40</span> <span class="keyword">items</span></div><div class="line">drwx<span class="comment">------   - ad            hdfs            0 2015-01-19 13:35 /binjiang/user/ad</span></div><div class="line">drwx<span class="comment">------   - azkaban       hdfs            0 2015-02-02 10:38 /binjiang/user/azkaban</span></div><div class="line">drwxr-x<span class="comment">---   - blog          hdfs            0 2015-02-02 10:38 /binjiang/user/blog</span></div><div class="line">drwx<span class="comment">------   - censor        hdfs            0 2015-02-10 03:00 /binjiang/user/censor</span></div><div class="line"> </div><div class="line"><span class="comment"># 也可以写完整的URI，否则就用fs.defaultFS的配置</span></div><div class="line">hadoop@inspur116:~/jxy/test/multiple_nn/hadoop-<span class="number">2.2</span><span class="number">.0</span>/bin$ ./hadoop fs -ls viewfs://dummyCluster/xiaoshan/user</div><div class="line">Found <span class="number">17</span> <span class="keyword">items</span></div><div class="line">drwx<span class="comment">------   - ad            hdfs              0 2015-02-09 14:48 viewfs://dummyCluster/xiaoshan/user/ad</span></div><div class="line">drwxr-x<span class="comment">---   - click_tracker hdfs              0 2015-01-22 16:09 viewfs://dummyCluster/xiaoshan/user/click_tracker</span></div><div class="line">drwxr-xr-x   - comic         hdfs              <span class="number">0</span> <span class="number">2015</span>-<span class="number">02</span>-<span class="number">10</span> <span class="number">14</span>:<span class="number">48</span> viewfs://dummyCluster/xiaoshan/user/comic</div><div class="line">drwxr-x<span class="comment">---   - easegame      hdfs              0 2015-01-13 14:49 viewfs://dummyCluster/xiaoshan/user/easegame</span></div><div class="line"> </div><div class="line"><span class="comment"># 这里其实是跨集群的copy，但背后细节被隐藏了</span></div><div class="line">hadoop@inspur116:~/jxy/test/multiple_nn/hadoop-<span class="number">2.2</span><span class="number">.0</span>/bin$ ./hadoop fs -cp /binjiang/tmp/hadoop /xiaoshan/tmp</div></pre></td></tr></table></figure>

<p>上面的例子中只有一个虚拟集群dummyCluster。其实也可以虚拟多个集群，每个虚拟集群可以挂载任意多hdfs，只要路径不冲突即可。还可以挂载子目录。具体不展开了，参考官方文档。</p>
<h2 id="一些问题">一些问题</h2>
<p>上面所有例子中的配置，都是客户端的。只在当前客户端生效。好处是不需要服务端改配置（一般要重启），用户可以自己选择。<br>如果两边集群的认证方式不同（比如一个kerberos一个simple），是否能访问没测试过。<br>在我们的配置中，两边的集群用的是同样的KDC，所以用户信息是相通的，hdfs权限也是相通的。如果用了两个KDC，是否能访问没测试过。<br>上述配置中的hz-cluster1/hz-cluster2等字符串，其实用户也可以换成任意值，都是客户端配置。</p>
<h1 id="跨集群YARN">跨集群YARN</h1>
<p>跨集群跑YARN任务是可行的，但有些限制。NM在跑任务时也是hdfs客户端，如果想让NM能访问不同hdfs的数据，就要加上面说过的那些配置（多个HA配置或ViewFs）。简言之，要改服务端配置。<br>如果不改服务端配置，就只能写死NN地址。<br>注意，任务提交到哪个YARN集群只跟当前客户端的yarn-site.xml有关。跟访问哪个hdfs无关。<br><strong>理论上YARN集群和hdfs集群可以是完全不同的物理机。</strong></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"># 以下都是以滨江集群的客户端为例</div><div class="line"> </div><div class="line"># 可以运行。输入数据在滨江集群，输出写到萧山集群</div><div class="line">hadoop@inspur116:~<span class="regexp">/jxy/</span>test<span class="regexp">/multiple_nn/</span>hadoop-<span class="number">2.2</span>.<span class="number">0</span><span class="regexp">/bin$ ./</span>hadoop jar ..<span class="regexp">/share/</span>hadoop<span class="regexp">/mapreduce/</span>hadoop-mapreduce-examples-<span class="number">2.2</span>.<span class="number">0</span>.jar wordcount <span class="regexp">/tmp/</span>hadoop hdfs:<span class="comment">//hadoop40.photo.163.org:8020/tmp/multiple_nn</span></div><div class="line"> </div><div class="line"># 可以运行。输入是萧山集群的，任务在滨江集群运行，输出写到滨江。</div><div class="line">hadoop@inspur116:~<span class="regexp">/jxy/</span>test<span class="regexp">/multiple_nn/</span>hadoop-<span class="number">2.2</span>.<span class="number">0</span><span class="regexp">/bin$ ./</span>hadoop jar ..<span class="regexp">/share/</span>hadoop<span class="regexp">/mapreduce/</span>hadoop-mapreduce-examples-<span class="number">2.2</span>.<span class="number">0</span>.jar wordcount hdfs:<span class="comment">//hadoop40.photo.163.org/user/comic/mobile/hadoop /tmp/multiple_nn</span></div><div class="line"> </div><div class="line"># 和上一条命令是一样效果。服务端有hz-cluster1的配置。</div><div class="line">hadoop@inspur116:~<span class="regexp">/jxy/</span>test<span class="regexp">/multiple_nn/</span>hadoop-<span class="number">2.2</span>.<span class="number">0</span><span class="regexp">/bin$ ./</span>hadoop jar ..<span class="regexp">/share/</span>hadoop<span class="regexp">/mapreduce/</span>hadoop-mapreduce-examples-<span class="number">2.2</span>.<span class="number">0</span>.jar wordcount hdfs:<span class="comment">//hadoop40.photo.163.org/user/comic/mobile/hadoop hdfs://hz-cluster1/tmp/multiple_nn</span></div><div class="line"> </div><div class="line"># 运行失败。只有当前客户端知道hz-cluster2指的是哪台机器，服务端没这个配置</div><div class="line">hadoop@inspur116:~<span class="regexp">/jxy/</span>test<span class="regexp">/multiple_nn/</span>hadoop-<span class="number">2.2</span>.<span class="number">0</span><span class="regexp">/bin$ ./</span>hadoop jar ..<span class="regexp">/share/</span>hadoop<span class="regexp">/mapreduce/</span>hadoop-mapreduce-examples-<span class="number">2.2</span>.<span class="number">0</span>.jar wordcount hdfs:<span class="comment">//hz-cluster2/user/comic/mobile/hadoop /tmp/multiple_nn</span></div><div class="line"> </div><div class="line"># 运行失败。理由同上。</div><div class="line">hadoop@inspur116:~<span class="regexp">/jxy/</span>test<span class="regexp">/multiple_nn/</span>hadoop-<span class="number">2.2</span>.<span class="number">0</span><span class="regexp">/bin$ ./</span>hadoop jar ..<span class="regexp">/share/</span>hadoop<span class="regexp">/mapreduce/</span>hadoop-mapreduce-examples-<span class="number">2.2</span>.<span class="number">0</span>.jar wordcount <span class="regexp">/tmp/</span>hadoop hdfs:<span class="comment">//hz-cluster2/tmp/multiple_nn</span></div></pre></td></tr></table></figure>

<p>distcp命令的本质也是提交一个MR，所以也受上述规则的限制。<br>如果修改服务端的配置，应该就可以用hz-cluster2了，没试过。要修改所有节点的配置文件，代价比较大。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>简单总结下跨集群访问的多种方式。</p>
]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://jxy.me/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[linode被qiang了]]></title>
    <link href="http://jxy.me/2015/05/19/linode-qianged/"/>
    <id>http://jxy.me/2015/05/19/linode-qianged/</id>
    <published>2015-05-19T07:53:47.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>终于轮到我了。。<br>已经有一段时间了，今天刚折腾好。</p>
<a id="more"></a>
<p>以前一直听说linode tokyo很容易被qiang，都是屏蔽整个IP段的。<br>我却一直没碰到过。但终究还是躲不掉。。。</p>
<p>先是ssh连不上，但能ping通。可能是端口号被qiang。<br>诡异的是<a href="https://www.linode.com/docs/networking/using-the-linode-shell-lish/" target="_blank" rel="external">lish</a>也死活连不上，总是卡住。所以没办法上去改端口号。<br>只能rebuild了，但我还有些<a href="/2015/03/07/hexo-and-git/">git仓库</a>在上面，所以要先备份下数据。</p>
<p>要用到linode的<a href="https://www.linode.com/docs/troubleshooting/rescue-and-rebuild/" target="_blank" rel="external">Rescue Mode</a>，参考：<a href="http://www.linode-vps.com/wp/linode-rescue-and-rebuild/" target="_blank" rel="external">http://www.linode-vps.com/wp/linode-rescue-and-rebuild/</a>。<br>大概的原理就是用cdrom启动你的linode，启动后是一个迷你的Finnix系统。</p>
<p>1.在linode manager界面上进入rescue mode，注意这里的磁盘挂载点，我原来的系统盘是/dev/xvda。进入rescue mode后，相当于原来的linode已经不存在了，是一个新的节点，只能通过lish登陆。<br>2.在lish中执行命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">mount -o <span class="keyword">exec</span>,barrier=<span class="number">0</span> /dev/xvda</div><div class="line"><span class="built_in">cd</span> /media/xvda</div><div class="line">mount -t proc proc proc/</div><div class="line">mount -t sysfs sys sys/</div><div class="line">mount -o <span class="built_in">bind</span> /dev dev/</div><div class="line">mount -t devpts pts dev/pts/</div><div class="line"><span class="comment"># 这是个很神奇的命令</span></div><div class="line">chroot /media/xvda /bin/bash</div></pre></td></tr></table></figure>

<p>之后会进入我本来的ubuntu系统。<br>3.在ubuntu中修改ssh端口号，然后<code>/etc/init.d/ssh start</code>。即可像平时一样ssh登陆。改过端口号就能ssh，很大可能是端口号被屏蔽。但还是不能解释为何lish连不上。<br>4.另开一个窗口ssh上去，将所有要备份的git仓库打包成tar.gz，下载到本地。</p>
<p>之后rebuild，但还是连不上。。。试了Debian/Ubuntu，试了各种内核，试了改/不改端口号，但只能偶尔连上，大部分时间都不行。</p>
<p>折腾了一段时间，发现连IP都ping不通了。。。这下彻底被qiang了。<br>好在域名还能正常解析。</p>
<p>只能申请换IP了。linode的客服非常赞的。很顺利的换了个IP。<br>上godaddy改下DNS，然后再rebuild，终于正常了。</p>
<p>接下来将备份的git仓库还原。其实很简单，解压后放到原来的路径就好了。<br>因为域名、端口号、路径都没变，所以git remote的url也不用改。尝试push，报错<code>bash: git-receive-pack: command not found</code>。这是因为服务端没有安装git。在linode上<code>sudo apt-get install git</code>。再次push，成功。</p>
<p>P.S.</p>
<p>用命令<code>ssh -p 8086 foolbear@foolbear.me</code>可以测试公钥是否正确添加。<br>在git bash中这个命令可以正确执行，这时读取的私钥是用户目录下的.ssh/id_rsa。<br>在windows cmd中这个命令会出错，读取的私钥是git安装目录下的.ssh/id_rsa（我用的git自带的ssh.exe）。如果用-i参数强制指定私钥为<code>C:\Users\foolbear\.ssh\id_rsa</code>则可以成功。<br>可以给ssh加上-v参数查看使用的私钥位置。<br>以前都没注意过这个问题。把私钥copy一份就可以了。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>终于轮到我了。。<br>已经有一段时间了，今天刚折腾好。</p>
]]>
    
    </summary>
    
      <category term="linode" scheme="http://jxy.me/tags/linode/"/>
    
      <category term="git" scheme="http://jxy.me/tags/git/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ContainerExecutor简介]]></title>
    <link href="http://jxy.me/2015/05/15/yarn-container-executor/"/>
    <id>http://jxy.me/2015/05/15/yarn-container-executor/</id>
    <published>2015-05-15T08:11:17.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>这篇文章拖了很长时间，本来是打算系统整理下YARN资源隔离机制的。<br>但太TM复杂了。。。很烦躁，看的头痛。。。<br>把目前写好的部分先发出来吧。</p>
<a id="more"></a>
<p>最近看到<a href="http://coolshell.cn/articles/17049.html" target="_blank" rel="external">cgroup的一篇文章</a>，突然想起YARN也是支持cgroup的。于是研究下。<br>Container相关逻辑都可以从ContainerManagerImpl类入手。</p>
<h1 id="ContainerExecutor">ContainerExecutor</h1>
<p>一些基础的概念不说了。<br>Container是资源调度的基础，也是资源隔离的基础。理论上来说不同Container之间不应该互相影响。<br>ContainerExecutor负责初始化、启动、杀掉Container。<br>YARN提供了两种ContainerExecutor，通过属性<code>yarn.nodemanager.container-executor.class</code>配置：</p>
<ul>
<li>DefaultContainerExecutor，简称DCE。每个Container运行在单独的进程里，但进程都是由NM的用户启动的。比如NM进程是用yarn用户启动的，那么所有Container的进程也由yarn用户启动。</li>
<li>LinuxContainerExecutor，简称LCE。每个Container由不同的用户启动。比如A用户提交的job的container，都由A用户启动。此外支持cgroup、支持单独的配置文件、支持简单的ACL。</li>
</ul>
<p>LCE明显隔离性更好，但有一些限制：</p>
<ol>
<li>需要linux native程序支持。准确的说是一个container-executor程序，用C写的，代码见hadoop-yarn-project\hadoop-yarn\hadoop-yarn-server\hadoop-yarn-server-nodemanager\src\main\native\container-executor。编译hadoop时务必同时编译container-executor。container-executor的路径由属性<code>yarn.nodemanager.linux-container-executor.path</code>指定。</li>
<li>container-executor还需要一个配置文件container-executor.cfg。而且这个配置文件和container-executor的二进制文件相对路径是固定的。默认情况下container-executor会去<code>../etc/hadoop</code>路径下寻找配置文件，找不到的话会报错。可以在编译hadoop时指定：<code>mvn package -Pdist,native -DskipTests -Dtar -Dcontainer-executor.conf.dir=../../conf</code>。不知道为何要这样设计。</li>
<li>由于用不同的用户启动Container，所以必须有对应的Linux用户存在。否则会抛异常。这带来一些管理上的麻烦，比如新增一个用户B时，必须在所有NM节点上执行<code>useradd B</code>。</li>
<li>container-executor和container-executor.cfg的所有者必须是root。而且他们所在的目录一直上溯到/，所有者也必须是root。所以我们一般把这两个文件放在/etc/yarn下。</li>
<li>container-executor文件的权限必须是<code>6050 or --Sr-s---</code>，因为它的原理就是setuid/setgid。group owner必须和启动NM的用户同组。比如NM由yarn用户启动，yarn用户属于hadoop组，那container-executor必须也是hadoop组。</li>
</ol>
<p>container-executor.cfg示例：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"># 下面这<span class="number">3</span>个属性，其实在yarn-site.xml里也有。两边必须一致。</div><div class="line">yarn.nodemanager.local-dirs=<span class="regexp">/mnt/</span>dfs<span class="regexp">/0/</span>yarn<span class="regexp">/local,/m</span>nt<span class="regexp">/dfs/</span><span class="number">1</span><span class="regexp">/yarn/</span>local,<span class="regexp">/mnt/</span>dfs<span class="regexp">/2/</span>yarn<span class="regexp">/local,/m</span>nt<span class="regexp">/dfs/</span><span class="number">3</span><span class="regexp">/yarn/</span>local,<span class="regexp">/mnt/</span>dfs<span class="regexp">/4/</span>yarn<span class="regexp">/local,/m</span>nt<span class="regexp">/dfs/</span><span class="number">5</span><span class="regexp">/yarn/</span>local,<span class="regexp">/mnt/</span>dfs<span class="regexp">/6/</span>yarn<span class="regexp">/local,/m</span>nt<span class="regexp">/dfs/</span><span class="number">7</span><span class="regexp">/yarn/</span>local</div><div class="line">yarn.nodemanager.linux-container-executor.<span class="keyword">group</span>=yarn</div><div class="line">yarn.nodemanager.log-dirs=<span class="regexp">/mnt/</span>dfs<span class="regexp">/0/</span>yarn<span class="regexp">/logs,/m</span>nt<span class="regexp">/dfs/</span><span class="number">1</span><span class="regexp">/yarn/</span>logs,<span class="regexp">/mnt/</span>dfs<span class="regexp">/2/</span>yarn<span class="regexp">/logs,/m</span>nt<span class="regexp">/dfs/</span><span class="number">3</span><span class="regexp">/yarn/</span>logs,<span class="regexp">/mnt/</span>dfs<span class="regexp">/4/</span>yarn<span class="regexp">/logs,/m</span>nt<span class="regexp">/dfs/</span><span class="number">5</span><span class="regexp">/yarn/</span>logs,<span class="regexp">/mnt/</span>dfs<span class="regexp">/6/</span>yarn<span class="regexp">/logs,/m</span>nt<span class="regexp">/dfs/</span><span class="number">7</span><span class="regexp">/yarn/</span>logs</div><div class="line"># 禁止这些用户提交yarn任务，主要是一些系统用户</div><div class="line">banned.users=hdfs,yarn,mapred,bin    </div><div class="line"># uid小于<span class="number">1000</span>的用户禁止提交yarn任务</div><div class="line">min.user.id=<span class="number">1000</span></div></pre></td></tr></table></figure>

<p>启动Container其实就是运行一个脚本。可以简单理解为：DCE是直接<code>/bin/bash $*</code>，LCE是<code>./container-executor $*</code>，额外包了一层。</p>
<p>如果直接执行container-executor：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">hadoop<span class="variable">@inspur116</span>:~/hadoop-current/bin$ ./<span class="keyword">container</span>-executor </div><div class="line">Usage: <span class="keyword">container</span>-executor --checksetup</div><div class="line">Usage: <span class="keyword">container</span>-executor --mount-cgroups hierarchy controller=path...</div><div class="line">Usage: <span class="keyword">container</span>-executor user yarn-user command command-args</div><div class="line">Commands:</div><div class="line">   initialize <span class="keyword">container</span>:  <span class="number">0</span> appid tokens nm-local-dirs nm-<span class="keyword">log</span>-dirs cmd app...</div><div class="line">   <span class="keyword">launch</span> <span class="keyword">container</span>:     <span class="number">1</span> appid containerid workdir <span class="keyword">container</span>-script tokens pidfile nm-local-dirs nm-<span class="keyword">log</span>-dirs resources</div><div class="line">   signal <span class="keyword">container</span>:     <span class="number">2</span> <span class="keyword">container</span>-pid signal</div><div class="line">   <span class="keyword">delete</span> as user:  <span class="number">3</span> relative-path</div></pre></td></tr></table></figure>

<h1 id="内存隔离">内存隔离</h1>
<p>YARN对内存其实没有真正隔离，而是监视Container进程的内存使用，超出限制后直接杀掉进程。相关逻辑见ContainersMonitorImpl类。<br>进程监控的逻辑见ProcfsBasedProcessTree类，原理就是读取/proc/$pid下面的文件，获得进程的内存占用。<br>具体的逻辑没详细看，还有点复杂的。</p>
<h1 id="CPU隔离">CPU隔离</h1>
<p>YARN在默认情况下，完全没有考虑CPU的隔离，即使用了LCE。<br>所以如果某个任务是CPU密集型的，可能消耗掉整个NM的CPU。<br>（跟具体的应用有关。对MR而言，最多用满一个核吧。）</p>
<h1 id="cgroup">cgroup</h1>
<p>YARN支持cgroup隔离CPU资源：<a href="https://issues.apache.org/jira/browse/YARN-3" target="_blank" rel="external">YARN-3</a>。<br>cgroup必须要LCE，但默认情况下没有开启。可以设置属性<code>yarn.nodemanager.linux-container-executor.resources-handler.class</code>为org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler以开启。<br>关于cgroup还有很多属性可以调整，见yarn-default.xml中的配置。</p>
<h1 id="localize过程">localize过程</h1>
<p>研究ContainerExecutor的过程中，发现了这个东西，研究的痛不欲生。。。<br>这其实就是类似于以前的distributed cache，但是YARN做的更通用了。</p>
<p>主要是分发container运行需要的所有文件，包括一些lib、token等等。<br>这个过程称为localize，由ResourceLocalizationService类负责。</p>
<p>分几步：</p>
<ol>
<li>建相关目录。$local.dir/usercache/$user/filecache，用于暂存用户可见的distributed cache；$local.dir/usercache/$user/appcache/$appid/filecache，用于暂存app可见的distributed cache；$log.dir/$appid/$containerid，用于暂存日志。我这里只列出了最深一级目录，父目录不存在也会新建。对DCE而言，直接用java代码建这些目录。对于LCE，调用container-executor建目录，见上文container-executor的Usage。注意这些目录会在所有磁盘上建（我们的节点一般是12块盘，就建12次），但只有一个会被真正使用。</li>
<li>将token文件写到$local.dir/usercache/$user/appcache/$appid目录。这里有bug，无论DCE还是LCE，都会将token文件写到第一个local-dir，所以可能会有竞争，导致后续container启动失败。见<a href="https://issues.apache.org/jira/browse/YARN-2566" target="_blank" rel="external">YARN-2566</a>、<a href="https://issues.apache.org/jira/browse/YARN-2623" target="_blank" rel="external">YARN-2623</a>。</li>
<li>对于DCE，直接new一个ContainerLocalizer对象，调用runLocalization方法。这个方法的作用是从ResourceLocalizationService处获取要分发的文件的URI，并下载到本地。对于LCE，会单独启动一个JVM进程，通过RPC协议LocalizationProtocol与ResourceLocalizationService通信。功能是一样的。</li>
</ol>
<p>接着就没继续研究了。。。主要是这个太复杂，而且没环境去调试。一些知识点：下载的lib会放在一个随机生成的数字目录下；下载的文件默认最大10GB，每10分钟清理一次；NM会将启动container的命令写到一个脚本里，但怎么将这些下载的lib加到classpath里的，还不太清楚。</p>
<p>关于localize过程的更多信息可以看<a href="http://zh.hortonworks.com/blog/resource-localization-in-yarn-deep-dive/" target="_blank" rel="external">HortonWorks Blog</a>。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>这篇文章拖了很长时间，本来是打算系统整理下YARN资源隔离机制的。<br>但太TM复杂了。。。很烦躁，看的头痛。。。<br>把目前写好的部分先发出来吧。</p>
]]>
    
    </summary>
    
      <category term="yarn" scheme="http://jxy.me/tags/yarn/"/>
    
      <category term="nodemanager" scheme="http://jxy.me/tags/nodemanager/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[hadoop 2.5.2 最小配置]]></title>
    <link href="http://jxy.me/2015/05/04/hadoop-2-5-2-min-conf/"/>
    <id>http://jxy.me/2015/05/04/hadoop-2-5-2-min-conf/</id>
    <published>2015-05-04T08:45:28.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>在<a href="/2015/05/04/network-error-fails-RM-HA/">测试YARN-2578</a>时，搭了一个简单的集群。记录下备用。</p>
<a id="more"></a>
<p>线上集群的配置太复杂，改起来也很麻烦。<br>既然是测试用，只要daemon能跑起来就可以了。</p>
<p>3个虚拟机：hadoop1/2/3。改hostname、改hosts的过程不说了。互相网络能通就行。</p>
<p>安装zookeeper的过程不说了。很简单。</p>
<p>直接去官网下载编译好的tar包：<a href="http://apache.fayea.com/hadoop/common/hadoop-2.5.2/hadoop-2.5.2.tar.gz" target="_blank" rel="external">hadoop-2.5.2.tar.gz</a>。</p>
<p>解压后改一些配置文件：</p>
<figure class="highlight xml"><figcaption><span>core-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hdfs://fk<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">final</span>&gt;</span>true<span class="tag">&lt;/<span class="title">final</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop1:2181,hadoop2:2181,hadoop3:2181<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>



<figure class="highlight xml"><figcaption><span>hdfs-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>fk<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.namenodes.fk<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.rpc-address.fk.nn1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop1:8020<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.rpc-address.fk.nn2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop2:8020<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.http-address.fk.nn1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop1:50070<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.http-address.fk.nn2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop2:50070<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.client.failover.proxy.provider.fk<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="comment">&lt;!-- JournalNode Configuration --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>qjournal://hadoop1:8485;hadoop2:8485;hadoop3:8485/fk<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/foolbear/hadoop_deploy/jn<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/foolbear/hadoop_deploy/name<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">final</span>&gt;</span>true<span class="tag">&lt;/<span class="title">final</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.data.dir<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/foolbear/hadoop_deploy/data<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">final</span>&gt;</span>true<span class="tag">&lt;/<span class="title">final</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>shell(/bin/true)<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>



<figure class="highlight xml"><figcaption><span>yarn-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop1:2181,hadoop2:2181,hadoop3:2181<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.zk-timeout-ms<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>90000<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>3<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.state-store.max-completed-applications<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>20<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="comment">&lt;!--ResourceManager HA--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>fkyarn<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop2<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>hadoop3<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="comment">&lt;!--aux services--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce_shuffle.class<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="comment">&lt;!--local and log dir--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.local-dirs<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/foolbear/hadoop_deploy/yarn_local<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.log-dirs<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>/home/foolbear/hadoop_deploy/yarn_logs<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"></div><div class="line"><span class="comment">&lt;!--机器烂，这个设小一点--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>



<figure class="highlight xml"><figcaption><span>mapred-site.xml</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="comment">&lt;!--这3个属性也是因为机器烂才设置的--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>yarn.app.mapreduce.am.resource.mb<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>800<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>800<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="title">name</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>800<span class="tag">&lt;/<span class="title">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></div></pre></td></tr></table></figure>

<p>理论上还要在hadoop-env.sh里设置下JAVA_HOME。但我有全局的JAVA_HOME了，就没单独设置。</p>
<p>启动过程也不说了，跟正常的一样，记着先formatZK、format namenode。</p>
<p>跑个wordcount试试：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./hadoop jar ../share/hadoop/mapreduce/hadoop-mapreduce-examples-<span class="number">2.5</span>.<span class="number">2</span>.jar wordcount /user/foolbear/hadoop/hdfs-site.xml /user/foolbear/output4</div></pre></td></tr></table></figure>

]]></content>
    <summary type="html">
    <![CDATA[<p>在<a href="/2015/05/04/network-error-fails-RM-HA/">测试YARN-2578</a>时，搭了一个简单的集群。记录下备用。</p>
]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://jxy.me/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[网络异常导致RM failover失败]]></title>
    <link href="http://jxy.me/2015/05/04/network-error-fails-RM-HA/"/>
    <id>http://jxy.me/2015/05/04/network-error-fails-RM-HA/</id>
    <published>2015-05-04T08:44:23.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>测试RM HA时发现一个JIRA：<a href="https://issues.apache.org/jira/browse/YARN-2578" target="_blank" rel="external">YARN-2578</a>。</p>
<a id="more"></a>
<p>以前我测试过重启RM所在机器/直接<code>kill -9</code>杀掉RM进程，都是可以正常failover的。</p>
<p>在google的过程中发现这个jira，如果网络失败会导致RM failover失败。</p>
<p>在虚拟机上重现这个bug：</p>
<p>部署了一个3节点的集群（ubuntu 10.04，JDK7u75）<br>hadoop1：ZK、JN、StandbyNN、DN、NM<br>hadoop2：ZK、JN、ActiveNN、ActiveRM、DN、NM<br>hadoop3：ZK、JN、StandbyRM、DN、NM</p>
<p>然后在hadoop2上执行<code>sudo ifconfig eth0 down</code>。<br>模拟网络挂掉的情况，相当于hadoop2跟其他节点网络都隔离开了。</p>
<p>hadoop2上的RM日志：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">04</span> <span class="number">14</span>:<span class="number">24</span>:<span class="number">24</span>,<span class="number">726</span> INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have <span class="operator">not</span> heard <span class="built_in">from</span> server <span class="operator">in</span> <span class="number">26668</span>ms <span class="keyword">for</span> sessionid <span class="number">0x14d1d76951c0001</span>, closing <span class="built_in">socket</span> connection <span class="operator">and</span> attempting reconnect</div><div class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">04</span> <span class="number">14</span>:<span class="number">24</span>:<span class="number">24</span>,<span class="number">830</span> INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: Exception <span class="keyword">while</span> executing <span class="operator">a</span> ZK operation.</div><div class="line">org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss</div><div class="line">	<span class="keyword">at</span> org.apache.zookeeper.KeeperException.<span class="built_in">create</span>(KeeperException.java:<span class="number">99</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:<span class="number">935</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:<span class="number">915</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$<span class="number">4.</span>run(ZKRMStateStore.java:<span class="number">888</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$<span class="number">4.</span>run(ZKRMStateStore.java:<span class="number">885</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:<span class="number">1004</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:<span class="number">1023</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.doMultiWithRetries(ZKRMStateStore.java:<span class="number">885</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.access$<span class="number">500</span>(ZKRMStateStore.java:<span class="number">82</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$VerifyActiveStatusThread.run(ZKRMStateStore.java:<span class="number">976</span>)</div><div class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">04</span> <span class="number">14</span>:<span class="number">24</span>:<span class="number">24</span>,<span class="number">831</span> INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore: Retrying operation <span class="command"><span class="keyword">on</span> <span class="title">ZK</span>. <span class="title">Retry</span> <span class="title">no</span>. <span class="title">1</span></span></div><div class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">04</span> <span class="number">14</span>:<span class="number">24</span>:<span class="number">25</span>,<span class="number">080</span> INFO org.apache.zookeeper.ClientCnxn: Opening <span class="built_in">socket</span> connection <span class="built_in">to</span> server hadoop3/<span class="number">192.168</span><span class="number">.177</span><span class="number">.140</span>:<span class="number">2181.</span> Will <span class="operator">not</span> attempt <span class="built_in">to</span> authenticate <span class="keyword">using</span> SASL (unknown error)</div><div class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">04</span> <span class="number">14</span>:<span class="number">24</span>:<span class="number">25</span>,<span class="number">080</span> ERROR org.apache.zookeeper.ClientCnxnSocketNIO: Unable <span class="built_in">to</span> <span class="built_in">open</span> <span class="built_in">socket</span> <span class="built_in">to</span> hadoop3/<span class="number">192.168</span><span class="number">.177</span><span class="number">.140</span>:<span class="number">2181</span></div><div class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">04</span> <span class="number">14</span>:<span class="number">24</span>:<span class="number">25</span>,<span class="number">081</span> WARN org.apache.zookeeper.ClientCnxn: Session <span class="number">0x14d1d76951c0000</span> <span class="keyword">for</span> server <span class="constant">null</span>, unexpected error, closing <span class="built_in">socket</span> connection <span class="operator">and</span> attempting reconnect</div><div class="line">java.net.SocketException: 网络不可达</div><div class="line">	<span class="keyword">at</span> sun.nio.ch.Net.connect0(Native Method)</div><div class="line">	<span class="keyword">at</span> sun.nio.ch.Net.connect(Net.java:<span class="number">465</span>)</div><div class="line">	<span class="keyword">at</span> sun.nio.ch.Net.connect(Net.java:<span class="number">457</span>)</div><div class="line">	<span class="keyword">at</span> sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:<span class="number">670</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.zookeeper.ClientCnxnSocketNIO.registerAndConnect(ClientCnxnSocketNIO.java:<span class="number">277</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.zookeeper.ClientCnxnSocketNIO.connect(ClientCnxnSocketNIO.java:<span class="number">287</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.zookeeper.ClientCnxn$SendThread.startConnect(ClientCnxn.java:<span class="number">967</span>)</div><div class="line">	<span class="keyword">at</span> org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:<span class="number">1003</span>)</div></pre></td></tr></table></figure>

<p>可以看到是zk客户端出错，不断尝试连接。</p>
<p>这时，NN的failover正常。hadoop3虽然变成ActiveRM了，但没有NM到hadoop3上注册，跟jira中描述的症状一样。<br>为何NN failover正常呢？因为NN HA和RM HA的机制不同。DN会同时向ANN和SNN发送心跳。而NM只会向ActiveRM发送心跳。见<a href="/2015/05/02/hadoop-resourcemanager-recovery/">另一篇文章</a>。</p>
<p>问题的根源在于NM发送心跳时没有超时机制，卡在了心跳那一步。</p>
<p>过了大概15分钟，NM终于超时，重新向hadoop3注册。日志：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">04</span> <span class="number">14</span>:<span class="number">39</span>:<span class="number">48</span>,<span class="number">161</span> INFO org.apache.hadoop.io.retry.RetryInvocationHandler: Exception <span class="keyword">while</span> invoking nodeHeartbeat <span class="keyword">of</span> <span class="type">class</span> ResourceTrackerPBClientImpl <span class="keyword">over</span> rm1. Trying <span class="keyword">to</span> fail <span class="keyword">over</span> immediately.</div><div class="line">java.io.IOException: Failed <span class="function_start"><span class="keyword">on</span></span> <span class="keyword">local</span> exception: java.io.IOException: 没有到主机的路由; Host Details : <span class="keyword">local</span> host <span class="keyword">is</span>: <span class="string">"hadoop1/192.168.177.139"</span>; destination host <span class="keyword">is</span>: <span class="string">"hadoop2"</span>:<span class="number">8031</span>;</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:<span class="number">764</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.ipc.Client.call(Client.java:<span class="number">1415</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.ipc.Client.call(Client.java:<span class="number">1364</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:<span class="number">206</span>)</div><div class="line">        <span class="keyword">at</span> com.sun.proxy.$Proxy27.nodeHeartbeat(Unknown Source)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.nodeHeartbeat(ResourceTrackerPBClientImpl.java:<span class="number">80</span>)</div><div class="line">        <span class="keyword">at</span> sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)</div><div class="line">        <span class="keyword">at</span> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</div><div class="line">        <span class="keyword">at</span> java.lang.reflect.Method.invoke(Method.java:<span class="number">606</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:<span class="number">187</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:<span class="number">102</span>)</div><div class="line">        <span class="keyword">at</span> com.sun.proxy.$Proxy28.nodeHeartbeat(Unknown Source)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$<span class="number">1.</span><span class="command">run</span>(NodeStatusUpdaterImpl.java:<span class="number">512</span>)</div><div class="line">        <span class="keyword">at</span> java.lang.Thread.<span class="command">run</span>(Thread.java:<span class="number">745</span>)</div><div class="line">Caused <span class="keyword">by</span>: java.io.IOException: 没有到主机的路由</div><div class="line">        <span class="keyword">at</span> sun.nio.ch.FileDispatcherImpl.read0(Native Method)</div><div class="line">        <span class="keyword">at</span> sun.nio.ch.SocketDispatcher.<span class="command">read</span>(SocketDispatcher.java:<span class="number">39</span>)</div><div class="line">        <span class="keyword">at</span> sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:<span class="number">223</span>)</div><div class="line">        <span class="keyword">at</span> sun.nio.ch.IOUtil.<span class="command">read</span>(IOUtil.java:<span class="number">197</span>)</div><div class="line">        <span class="keyword">at</span> sun.nio.ch.SocketChannelImpl.<span class="command">read</span>(SocketChannelImpl.java:<span class="number">379</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:<span class="number">57</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:<span class="number">142</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.net.SocketInputStream.<span class="command">read</span>(SocketInputStream.java:<span class="number">161</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.net.SocketInputStream.<span class="command">read</span>(SocketInputStream.java:<span class="number">131</span>)</div><div class="line">        <span class="keyword">at</span> java.io.FilterInputStream.<span class="command">read</span>(FilterInputStream.java:<span class="number">133</span>)</div><div class="line">        <span class="keyword">at</span> java.io.FilterInputStream.<span class="command">read</span>(FilterInputStream.java:<span class="number">133</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.ipc.Client$Connection$PingInputStream.<span class="command">read</span>(Client.java:<span class="number">512</span>)</div><div class="line">        <span class="keyword">at</span> java.io.BufferedInputStream.fill(BufferedInputStream.java:<span class="number">235</span>)</div><div class="line">        <span class="keyword">at</span> java.io.BufferedInputStream.<span class="command">read</span>(BufferedInputStream.java:<span class="number">254</span>)</div><div class="line">        <span class="keyword">at</span> java.io.DataInputStream.readInt(DataInputStream.java:<span class="number">387</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:<span class="number">1055</span>)</div><div class="line">        <span class="keyword">at</span> org.apache.hadoop.ipc.Client$Connection.<span class="command">run</span>(Client.java:<span class="number">950</span>)</div><div class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">04</span> <span class="number">14</span>:<span class="number">39</span>:<span class="number">48</span>,<span class="number">203</span> INFO org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider: Failing <span class="keyword">over</span> <span class="keyword">to</span> rm2</div><div class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">04</span> <span class="number">14</span>:<span class="number">39</span>:<span class="number">48</span>,<span class="number">211</span> WARN org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Node <span class="keyword">is</span> <span class="keyword">out of</span> sync <span class="keyword">with</span> ResourceManager, hence resyncing.</div><div class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">04</span> <span class="number">14</span>:<span class="number">39</span>:<span class="number">48</span>,<span class="number">211</span> WARN org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl: Message <span class="keyword">from</span> ResourceManager: Node <span class="keyword">not</span> found resyncing hadoop1:<span class="number">60618</span></div><div class="line"><span class="number">2015</span>-<span class="number">05</span>-<span class="number">04</span> <span class="number">14</span>:<span class="number">39</span>:<span class="number">48</span>,<span class="number">214</span> INFO org.apache.hadoop.yarn.server.nodemanager.NodeManager: Notifying ContainerManager <span class="keyword">to</span> block new container-requests</div></pre></td></tr></table></figure>

<p>在这15分钟内，可以提交任务到RM，但任务是无法执行的。</p>
<p>为何是15分钟？因为所有的RPC请求都有一个最基础的超时机制，见org.apache.hadoop.ipc.Client类。由两个参数控制：<br>ipc.client.connect.timeout，默认20秒<br>ipc.client.connect.max.retries.on.timeouts，默认重试45次<br>所以超时时间是(45-1)*20=880秒，约15分钟。</p>
<p>jira上的patch也非常简单，换一个有超时参数的getProxy方法即可。</p>
<p>hadoop的RPC机制还有点复杂的。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>测试RM HA时发现一个JIRA：<a href="https://issues.apache.org/jira/browse/YARN-2578" target="_blank" rel="external">YARN-2578</a>。</p>
]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://jxy.me/tags/hadoop/"/>
    
      <category term="resourcemanager" scheme="http://jxy.me/tags/resourcemanager/"/>
    
      <category term="nodemanager" scheme="http://jxy.me/tags/nodemanager/"/>
    
      <category term="HA" scheme="http://jxy.me/tags/HA/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[RM HA机制的一些研究]]></title>
    <link href="http://jxy.me/2015/05/02/hadoop-resourcemanager-recovery/"/>
    <id>http://jxy.me/2015/05/02/hadoop-resourcemanager-recovery/</id>
    <published>2015-05-02T01:59:22.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>之前一直在研究hadoop 2.5.2。稍微整理下RM HA机制。<br><a id="more"></a></p>
<h1 id="与NN_HA的区别">与NN HA的区别</h1>
<p>NN HA中，DN会同时向ActiveNN和StandbyNN发送心跳。<br>RM HA中，NM只会向ActiveRM发送心跳。StandyRM中的很多服务甚至不会启动，见代码（ResourceManager类）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">synchronized</span> <span class="keyword">void</span> transitionToActive() <span class="keyword">throws</span> Exception {</div><div class="line">    <span class="keyword">if</span> (rmContext.getHAServiceState() ==</div><div class="line">        HAServiceProtocol.HAServiceState.ACTIVE) {</div><div class="line">      LOG.info(<span class="string">"Already in active state"</span>);</div><div class="line">      <span class="keyword">return</span>;</div><div class="line">    }</div><div class="line"> </div><div class="line">    LOG.info(<span class="string">"Transitioning to active state"</span>);</div><div class="line"> </div><div class="line">    <span class="comment">// use rmLoginUGI to startActiveServices.</span></div><div class="line">    <span class="comment">// in non-secure model, rmLoginUGI will be current UGI</span></div><div class="line">    <span class="comment">// in secure model, rmLoginUGI will be LoginUser UGI</span></div><div class="line">    <span class="keyword">this</span>.rmLoginUGI.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;Void&gt;() {</div><div class="line">      <span class="annotation">@Override</span></div><div class="line">      <span class="keyword">public</span> Void <span class="title">run</span>() <span class="keyword">throws</span> Exception {</div><div class="line">        <span class="comment">// 这个方法会启动一些只在active状态下的服务，比如调度器、web界面、NM心跳监控等等</span></div><div class="line">        startActiveServices();</div><div class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</div><div class="line">      }</div><div class="line">    });</div><div class="line"> </div><div class="line">    rmContext.setHAServiceState(HAServiceProtocol.HAServiceState.ACTIVE);</div><div class="line">    LOG.info(<span class="string">"Transitioned to active state"</span>);</div><div class="line">  }</div><div class="line"> </div><div class="line">  <span class="keyword">synchronized</span> <span class="keyword">void</span> transitionToStandby(<span class="keyword">boolean</span> initialize)</div><div class="line">      <span class="keyword">throws</span> Exception {</div><div class="line">    <span class="keyword">if</span> (rmContext.getHAServiceState() ==</div><div class="line">        HAServiceProtocol.HAServiceState.STANDBY) {</div><div class="line">      LOG.info(<span class="string">"Already in standby state"</span>);</div><div class="line">      <span class="keyword">return</span>;</div><div class="line">    }</div><div class="line"> </div><div class="line">    LOG.info(<span class="string">"Transitioning to standby state"</span>);</div><div class="line">    <span class="keyword">if</span> (rmContext.getHAServiceState() ==</div><div class="line">        HAServiceProtocol.HAServiceState.ACTIVE) {</div><div class="line">      <span class="comment">// 进入StandBy状态后，会停止一些服务</span></div><div class="line">      stopActiveServices();</div><div class="line">      <span class="keyword">if</span> (initialize) {</div><div class="line">        resetDispatcher();</div><div class="line">        createAndInitActiveServices();</div><div class="line">      }</div><div class="line">    }</div><div class="line">    rmContext.setHAServiceState(HAServiceProtocol.HAServiceState.STANDBY);</div><div class="line">    LOG.info(<span class="string">"Transitioned to standby state"</span>);</div><div class="line">  }</div></pre></td></tr></table></figure>

<p>这个机制决定了RM的HA切换会比较慢，不像NN的切换那么迅速。</p>
<p>RM HA的另一个不同之处是选举机制内建在RM里（EmbeddedElectorService类），而NN HA是用单独的zkfc进程进行选举的。zkfc进程和NN进程通过RPC协议（ZKFCProtocol）进行通信。<br>zkfc的隔离性比较好。比如NN进程意外挂掉时，zkfc很快会监控到NN挂掉并重新发起选举。<br>如果是内建选举机制，可能会有bug，比如RM进程意外挂掉（比如直接kill -9），要等zk超时后才能再次选举。failover的时间会比较长。<br>这个超时机制见我<a href="/2015/04/01/ha-and-zookeeper/">另一篇文章</a>。</p>
<h1 id="客户端">客户端</h1>
<p>客户端（NM也可以认为是个客户端）判断Active的方法和NN HA是类似的（见我<a href="/2015/04/09/hadoop-ha-active-nn/">另一篇文章</a>），都是纯客户端操作，见RMProxy类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">protected</span> <span class="keyword">static</span> &lt;T&gt; T <span class="title">createRMProxy</span>(<span class="keyword">final</span> Configuration configuration,</div><div class="line">      <span class="keyword">final</span> Class&lt;T&gt; protocol, RMProxy instance) <span class="keyword">throws</span> IOException {</div><div class="line">    YarnConfiguration conf = (configuration <span class="keyword">instanceof</span> YarnConfiguration)</div><div class="line">        ? (YarnConfiguration) configuration</div><div class="line">        : <span class="keyword">new</span> YarnConfiguration(configuration);</div><div class="line">    RetryPolicy retryPolicy = createRetryPolicy(conf);</div><div class="line">    <span class="comment">// 在客户端判断哪个是ActiveRM</span></div><div class="line">    <span class="comment">// 判断yarn.resourcemanager.ha.enabled属性是否是true</span></div><div class="line">    <span class="keyword">if</span> (HAUtil.isHAEnabled(conf)) {</div><div class="line">      RMFailoverProxyProvider&lt;T&gt; provider =</div><div class="line">          instance.createRMFailoverProxyProvider(conf, protocol);</div><div class="line">      <span class="keyword">return</span> (T) RetryProxy.create(protocol, provider, retryPolicy);</div><div class="line">    } <span class="keyword">else</span> {</div><div class="line">      <span class="comment">// 取出yarn.resourcemanager.resource-tracker.address属性的值</span></div><div class="line">      <span class="comment">// 默认值是${yarn.resourcemanager.hostname}:8031</span></div><div class="line">      InetSocketAddress rmAddress = instance.getRMAddress(conf, protocol);</div><div class="line">      LOG.info(<span class="string">"Connecting to ResourceManager at "</span> + rmAddress);</div><div class="line">      T proxy = RMProxy.&lt;T&gt;getProxy(conf, protocol, rmAddress);</div><div class="line">      <span class="keyword">return</span> (T) RetryProxy.create(protocol, proxy, retryPolicy);</div><div class="line">    }</div><div class="line">  }</div></pre></td></tr></table></figure>

<h1 id="Recovery失败导致导致RM无法启动">Recovery失败导致导致RM无法启动</h1>
<p>非常严重的bug。单独拿出来说说。YARN HA的坑还是比较多的。<br>具体表现为启动RM无法启动，一直报异常。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">org.apache.hadoop.ha.ServiceFailedException: RM could <span class="keyword">not</span> transition <span class="keyword">to</span> Active</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:<span class="number">122</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:<span class="number">805</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:<span class="number">416</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:<span class="number">599</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.zookeeper.ClientCnxn$EventThread.<span class="command">run</span>(ClientCnxn.java:<span class="number">498</span>)</div><div class="line">Caused <span class="keyword">by</span>: org.apache.hadoop.ha.ServiceFailedException: Error when transitioning <span class="keyword">to</span> Active mode</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:<span class="number">284</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:<span class="number">120</span>)</div><div class="line">    ... <span class="number">4</span> more</div><div class="line">Caused <span class="keyword">by</span>: org.apache.hadoop.service.ServiceStateException: RMActiveServices cannot enter state STARTED <span class="keyword">from</span> state STOPPED</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.service.ServiceStateModel.checkStateTransition(ServiceStateModel.java:<span class="number">129</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.service.ServiceStateModel.enterState(ServiceStateModel.java:<span class="number">111</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.service.AbstractService.start(AbstractService.java:<span class="number">190</span>)</div></pre></td></tr></table></figure>

<p>相关JIRA：<br><a href="https://issues.apache.org/jira/browse/YARN-2010" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-2010</a><br><a href="https://issues.apache.org/jira/browse/YARN-2019" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-2019</a><br><a href="https://issues.apache.org/jira/browse/YARN-2588" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-2588</a></p>
<p>根源在于RM HA启动时会去zk读取之前的状态（recovery过程），如果zk中的数据有问题，recovery过程会抛出异常，这个异常没处理好，会直接抛到上层，导致RM进入STOPPED状态。</p>
<p>进入STOPPED状态后，就不能再变成其他状态了。（状态转换关系见ServiceStateModel类）。相关代码：</p>
<figure class="highlight java"><figcaption><span>RMActiveServices.java</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="annotation">@Override</span></div><div class="line"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceStart</span>() <span class="keyword">throws</span> Exception {</div><div class="line">  RMStateStore rmStore = rmContext.getStateStore();</div><div class="line">  <span class="comment">// The state store needs to start irrespective of recoveryEnabled as apps</span></div><div class="line">  <span class="comment">// need events to move to further states.</span></div><div class="line">  rmStore.start();</div><div class="line"> </div><div class="line">  <span class="keyword">if</span>(recoveryEnabled) {</div><div class="line">    <span class="keyword">try</span> {</div><div class="line">      rmStore.checkVersion();</div><div class="line">      <span class="keyword">if</span> (rmContext.isWorkPreservingRecoveryEnabled()) {</div><div class="line">        rmContext.setEpoch(rmStore.getAndIncrementEpoch());</div><div class="line">      }</div><div class="line">      RMState state = rmStore.loadState();</div><div class="line">      recover(state);</div><div class="line">    } <span class="keyword">catch</span> (Exception e) {</div><div class="line">      <span class="comment">// the Exception from loadState() needs to be handled for</span></div><div class="line">      <span class="comment">// HA and we need to give up master status if we got fenced</span></div><div class="line">      LOG.error(<span class="string">"Failed to load/recover state"</span>, e);</div><div class="line">      <span class="keyword">throw</span> e;</div><div class="line">    }</div><div class="line">  }</div><div class="line"> </div><div class="line">  <span class="keyword">super</span>.serviceStart();</div><div class="line">}</div></pre></td></tr></table></figure>

<p>异常链：RMActiveServices.serviceStart() -&gt; RMActiveServices.start() -&gt; ResourceManager.startActiveServices() -&gt; ResourceManager.transitionToActive() -&gt; ResourceManager.serviceStart() -&gt; AbstractService.start()</p>
<p>AbstractService相关代码：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="annotation">@Override</span></div><div class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span>() {</div><div class="line">  <span class="keyword">if</span> (isInState(STATE.STARTED)) {</div><div class="line">    <span class="keyword">return</span>;</div><div class="line">  }</div><div class="line">  <span class="comment">//enter the started state</span></div><div class="line">  <span class="keyword">synchronized</span> (stateChangeLock) {</div><div class="line">    <span class="keyword">if</span> (stateModel.enterState(STATE.STARTED) != STATE.STARTED) {</div><div class="line">      <span class="keyword">try</span> {</div><div class="line">        startTime = System.currentTimeMillis();</div><div class="line">        serviceStart();</div><div class="line">        <span class="keyword">if</span> (isInState(STATE.STARTED)) {</div><div class="line">          <span class="comment">//if the service started (and isn't now in a later state), notify</span></div><div class="line">          <span class="keyword">if</span> (LOG.isDebugEnabled()) {</div><div class="line">            LOG.debug(<span class="string">"Service "</span> + getName() + <span class="string">" is started"</span>);</div><div class="line">          }</div><div class="line">          notifyListeners();</div><div class="line">        }</div><div class="line">      } <span class="keyword">catch</span> (Exception e) {</div><div class="line">        noteFailure(e);</div><div class="line">        <span class="comment">// 这个方法会将service的状态改为STOPPED</span></div><div class="line">        ServiceOperations.stopQuietly(LOG, <span class="keyword">this</span>);</div><div class="line">        <span class="keyword">throw</span> ServiceStateException.convert(e);</div><div class="line">      }</div><div class="line">    }</div><div class="line">  }</div><div class="line">}</div></pre></td></tr></table></figure>

<p>JIRA上的补丁的原理就是加一层try cache，recovery过程的异常单独处理，不要抛到上层。<br>recovery的异常不应该影响RM的正常功能。</p>
<p>引起recovery异常的原因有很多。我碰到的情况是token过期。<br>RM启动时尝试读取zk中的APP信息：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="number">2015</span>-<span class="number">03</span>-<span class="number">27</span> <span class="number">15</span>:<span class="number">02</span>:<span class="number">44</span>,<span class="number">897</span> INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Recovering app: application_1425461709120_0005 <span class="keyword">with</span> <span class="number">1</span> attempts <span class="constant">and</span> final <span class="variable">state =</span> FINISHED</div><div class="line"><span class="number">2015</span>-<span class="number">03</span>-<span class="number">27</span> <span class="number">15</span>:<span class="number">02</span>:<span class="number">44</span>,<span class="number">897</span> INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Recovering attempt: appattempt_1425461709120_0005_000001 <span class="keyword">with</span> final state: FINISHED</div><div class="line"><span class="number">2015</span>-<span class="number">03</span>-<span class="number">27</span> <span class="number">15</span>:<span class="number">02</span>:<span class="number">44</span>,<span class="number">897</span> INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1425461709120_0005_000001 State change from NEW to FINISHED</div><div class="line"><span class="number">2015</span>-<span class="number">03</span>-<span class="number">27</span> <span class="number">15</span>:<span class="number">02</span>:<span class="number">44</span>,<span class="number">897</span> INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1425461709120_0005 State change from NEW to FINISHED</div><div class="line"><span class="number">2015</span>-<span class="number">03</span>-<span class="number">27</span> <span class="number">15</span>:<span class="number">02</span>:<span class="number">44</span>,<span class="number">897</span> INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: <span class="variable">USER=</span>censor    <span class="variable">OPERATION=</span>Application Finished - Succeeded    <span class="variable">TARGET=</span>RMAppManager    <span class="variable">RESULT=</span>SUCCESS    <span class="variable">APPID=</span>application_1425461709120_0005</div><div class="line"><span class="number">2015</span>-<span class="number">03</span>-<span class="number">27</span> <span class="number">15</span>:<span class="number">02</span>:<span class="number">44</span>,<span class="number">898</span> INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary: <span class="variable">appId=</span>application_1425461709120_0005,<span class="variable">name=</span>Iteration <span class="number">1</span> of <span class="number">50</span>\, input path: /user/censor/yixin/modeltmp/model-<span class="number">0</span>,<span class="variable">user=</span>censor,<span class="variable">queue=</span>censor\,default,<span class="variable">state=</span>FINISHED,<span class="variable">trackingUrl=</span>http://inspur116.photo.<span class="number">163</span>.org:<span class="number">8088</span>/proxy/application_1425461709120_0005/jobhistory/job/job_1425461709120_0005,<span class="variable">appMasterHost=</span>N/A,<span class="variable">startTime=</span><span class="number">1425463503225</span>,<span class="variable">finishTime=</span><span class="number">1425463538077</span>,<span class="variable">finalStatus=</span>SUCCEEDED</div></pre></td></tr></table></figure>

<p>出现异常（省略了部分内容）：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">org.apache.hadoop.security.<span class="keyword">token</span>.SecretManager$InvalidToken: yarn tried <span class="built_in">to</span> renew <span class="operator">an</span> expired <span class="keyword">token</span></div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.security.<span class="keyword">token</span>.delegation.AbstractDelegationTokenSecretManager.renewToken(AbstractDelegationTokenSecretManager.java:<span class="number">366</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewDelegationToken(FSNamesystem.java:<span class="number">6726</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewDelegationToken(NameNodeRpcServer.java:<span class="number">504</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:<span class="number">939</span>)</div><div class="line">Caused <span class="keyword">by</span>: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.<span class="keyword">token</span>.SecretManager$InvalidToken): yarn tried <span class="built_in">to</span> renew <span class="operator">an</span> expired <span class="keyword">token</span></div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.security.<span class="keyword">token</span>.delegation.AbstractDelegationTokenSecretManager.renewToken(AbstractDelegationTokenSecretManager.java:<span class="number">366</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewDelegationToken(FSNamesystem.java:<span class="number">6726</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewDelegationToken(NameNodeRpcServer.java:<span class="number">504</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:<span class="number">939</span>)</div></pre></td></tr></table></figure>

<p>如果之前app的状态是FINISHED（无论SUCCESS或FAILED），都不会有问题。但如果之前app的状态是UNDEFINED，RM会尝试重新运行这个程序。之前app用的token存在zk里了，RM会尝试更新这个token并重新运行程序。</p>
<p>但我是从2.2.0升到2.5.2的，以前的token早就无效了（7天），所以会有异常。<br>而且NN都重启过了，就算token没有过期，我觉得也会抛其他异常。相关代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">renewToken</span>(Token&lt;TokenIdent&gt; token,</div><div class="line">                       String renewer) <span class="keyword">throws</span> InvalidToken, IOException {</div><div class="line">  ByteArrayInputStream buf = <span class="keyword">new</span> ByteArrayInputStream(token.getIdentifier());</div><div class="line">  DataInputStream in = <span class="keyword">new</span> DataInputStream(buf);</div><div class="line">  TokenIdent id = createIdentifier();</div><div class="line">  id.readFields(in);</div><div class="line">  LOG.info(<span class="string">"Token renewal for identifier: "</span> + id + <span class="string">"; total currentTokens "</span></div><div class="line">      +  currentTokens.size());</div><div class="line">  <span class="comment">// token过期</span></div><div class="line">  <span class="keyword">long</span> now = Time.now();</div><div class="line">  <span class="keyword">if</span> (id.getMaxDate() &lt; now) {</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> InvalidToken(renewer + <span class="string">" tried to renew an expired token"</span>);</div><div class="line">  }</div><div class="line">  <span class="keyword">if</span> ((id.getRenewer() == <span class="keyword">null</span>) || (id.getRenewer().toString().isEmpty())) {</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AccessControlException(renewer +</div><div class="line">        <span class="string">" tried to renew a token without a renewer"</span>);</div><div class="line">  }</div><div class="line">  <span class="keyword">if</span> (!id.getRenewer().toString().equals(renewer)) {</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> AccessControlException(renewer +</div><div class="line">        <span class="string">" tries to renew a token with renewer "</span> + id.getRenewer());</div><div class="line">  }</div><div class="line">  <span class="comment">// 如果这个token不存在，估计也会抛异常</span></div><div class="line">  DelegationKey key = allKeys.get(id.getMasterKeyId());</div><div class="line">  <span class="keyword">if</span> (key == <span class="keyword">null</span>) {</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> InvalidToken(<span class="string">"Unable to find master key for keyId="</span></div><div class="line">        + id.getMasterKeyId()</div><div class="line">        + <span class="string">" from cache. Failed to renew an unexpired token"</span></div><div class="line">        + <span class="string">" with sequenceNumber="</span> + id.getSequenceNumber());</div><div class="line">  }</div></pre></td></tr></table></figure>

<p>解决方法：不改代码的话，只能手动清除zk中的数据。可以手动删除zk中的/rmstore路径：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">set</span>Acl /rmstore/ZKRMStateRoot world:anyone:cdrwa</div><div class="line">rmr /rmstore</div></pre></td></tr></table></figure>

<p>或者在yarn-site.xml中设置yarn.resourcemanager.zk-state-store.parent-path属性，比如/rmstore2，将数据存到另外一个路径。</p>
<h1 id="其他一些bug">其他一些bug</h1>
<h2 id="RM_failover可能导致job失败">RM failover可能导致job失败</h2>
<p>跟failover的时机有关：<a href="https://issues.apache.org/jira/browse/MAPREDUCE-5718" target="_blank" rel="external">MAPREDUCE-5718</a>。<br>如果map和reduce都结束后，AM将进入commit阶段（其实就是在staging dir写个文件），如果正好在这个时候failover，job虽然会重新提交，但必定失败。<br>触发几率应该不大，HA本身就是个备用机制，不能太依赖。</p>
<h2 id="不能手动failover">不能手动failover</h2>
<p>开启RM HA后不能手动failover，见<a href="http://search-hadoop.com/m/LgpTk2y6T7j1/+%2522doesnt+%2522user+hadoop%2522&amp;subj=Re+Can+not+execute+failover+for+RM+HA" target="_blank" rel="external">邮件列表</a>。会抛出异常：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">hadoop@inspur116:~/hadoop-current/bin$ ./yarn rmadmin -failover rm1 rm2</div><div class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.UnsupportedOperationException: RMHAServiceTarget doesn't have a corresponding ZKFC address</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.yarn.client.RMHAServiceTarget.getZKFCAddress(RMHAServiceTarget.java:<span class="number">51</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ha.HAServiceTarget.getZKFCProxy(HAServiceTarget.java:<span class="number">94</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ha.HAAdmin.gracefulFailoverThroughZKFCs(HAAdmin.java:<span class="number">315</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ha.HAAdmin.failover(HAAdmin.java:<span class="number">286</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ha.HAAdmin.runCmd(HAAdmin.java:<span class="number">453</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.ha.HAAdmin.<span class="command">run</span>(HAAdmin.java:<span class="number">382</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.yarn.client.cli.RMAdminCLI.<span class="command">run</span>(RMAdminCLI.java:<span class="number">318</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.util.ToolRunner.<span class="command">run</span>(ToolRunner.java:<span class="number">70</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.util.ToolRunner.<span class="command">run</span>(ToolRunner.java:<span class="number">84</span>)</div><div class="line">    <span class="keyword">at</span> org.apache.hadoop.yarn.client.cli.RMAdminCLI.main(RMAdminCLI.java:<span class="number">434</span>)</div></pre></td></tr></table></figure>

<p>相关JIRA：<a href="https://issues.apache.org/jira/browse/YARN-1177" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-1177</a></p>
<h2 id="网络问题导致RM_failover失败">网络问题导致RM failover失败</h2>
<p><a href="https://issues.apache.org/jira/browse/YARN-2578" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-2578</a><br>详细的分析见<a href="/2015/05/04/network-error-fails-RM-HA/">另一篇文章</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>之前一直在研究hadoop 2.5.2。稍微整理下RM HA机制。<br>]]>
    
    </summary>
    
      <category term="hadoop" scheme="http://jxy.me/tags/hadoop/"/>
    
      <category term="resourcemanager" scheme="http://jxy.me/tags/resourcemanager/"/>
    
      <category term="HA" scheme="http://jxy.me/tags/HA/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[YARN资源调度策略]]></title>
    <link href="http://jxy.me/2015/04/30/yarn-resource-scheduler/"/>
    <id>http://jxy.me/2015/04/30/yarn-resource-scheduler/</id>
    <published>2015-04-30T08:34:04.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>公司活动写的一篇文章。这里也发下吧。<br>介绍下YARN中资源调度相关概念和算法。以hadoop 2.2.0为准。<br><a id="more"></a></p>
<p>YARN虽然是从MapReduce发展而来，但其实更偏底层，它在硬件和计算框架之间提供了一个抽象层，用户可以方便的基于YARN编写自己的分布式计算框架，而不用关心硬件的细节。由此可以看出YARN的核心功能：资源抽象、资源管理（包括调度、使用、监控、隔离等等）。从某种程度上说YARN类似于IaaS。<br><img src="/2015/04/30/yarn-resource-scheduler/overview.png" alt=""></p>
<p>YARN的基本概念不再赘述。一些缩写：RM = ResourceManager、NM = NodeManager、AM = ApplicationMaster。</p>
<h1 id="什么是资源？">什么是资源？</h1>
<p>对一个资源管理系统而言，首先要定义出资源的种类，然后将每种资源量化，才能管理。这就是资源抽象的过程。</p>
<p>先抛开YARN不谈，在一个分布式、多用户的系统中，什么是资源？<br>我们通常所说的“资源”都是硬件资源，包括CPU使用/内存使用/磁盘用量/IO/网络流量等等。这是比较粗粒度的。也可以是抽象层次更高的TPS/请求数之类的。其实广义来看，时间、人力等“软件”也是资源，可惜一般很难量化。</p>
<p>为什么要有“资源”这个概念？首先可以用来衡量系统的瓶颈，系统能否充分利用资源？什么时候应该扩容？在多用户的系统中，“资源”还额外承担着限制用户的功能。比如当总体资源紧张时，重要的用户可以优先获得资源，有更高的资源上限，普通的用户更难获得资源。</p>
<p>YARN的资源抽象比较简单，只有两种资源：内存和CPU。而资源数量是管理员手动设置的，每个NM节点可以贡献一定数量的内存（MB）和CPU，由RM统一管理，不一定是真实的内存和CPU数。<br>其中内存资源是比较关键的，直接决定任务能否成功。如果某个任务需要的内存过多，可能无法执行，或者OOM。CPU资源的限制比较弱，只限定了一台NM上能并发执行多少任务。如果并发的过多，执行的可能比较慢。</p>
<h1 id="基本概念">基本概念</h1>
<h2 id="Container">Container</h2>
<p>Container是RM分配资源的基本单位。每个Container包含特定数量的CPU资源和内存资源，用户的程序运行在Container中，有点类似虚拟机。RM负责接收用户的资源请求并分配Container，NM负责启动Container并监控资源使用。如果使用的资源（目前只有内存）超出Container的限制，相应进程会被NM杀掉。<br>可见Container这个概念不只用于资源分配，也用于资源隔离。理论上来说不同Container之间不能互相影响。可惜现阶段YARN的隔离做的还不太好。</p>
<p>Container的另一个特性是客户端可以要求只在特定节点上分配，这样用户的程序可以只在特定的节点上执行。这跟计算本地性有关。后面会讲到。</p>
<p>Container是有最大资源限制的。在我们的设置中，每个Container最多只能有8G内存，8个CPU。这是由RM端的参数yarn.scheduler.maximum-allocation-mb和yarn.scheduler.maximum-allocation-vcores决定的。</p>
<h2 id="调度器与队列">调度器与队列</h2>
<p>在YARN中，调度器是一个可插拔的组件，常见的有FIFO，CapacityScheduler，FairScheduler。可以通过配置文件选择不同的调度器。<br>在RM端，根据不同的调度器，所有的资源被分成一个或多个队列（queue），每个队列包含一定量的资源。用户的每个application，会被唯一的分配到一个队列中去执行。队列决定了用户能使用的资源上限。<br>所谓资源调度，就是决定将资源分配给哪个队列、哪个application的过程。</p>
<p>可见调度器的两个主要功能：1.决定如何划分队列；2.决定如何分配资源。此外，还有些其他的特性：ACL、抢占、延迟调度等等。<br>后文会对这几种调度器分别介绍下。</p>
<h2 id="事件驱动">事件驱动</h2>
<p>YARN实现了一套基于状态机的事件驱动机制：很多对象内部有一个预先定义好的有限状态机，相应的事件会触发状态转换，状态转换的过程中触发预先定义的钩子，钩子执行的过程中又生成新的事件，继续状态转换。这种设计的好处是耦合小，但不太好理解。<br>几个角色：<br>Dispatcher —— 用于分发事件，一般是异步的。内部用一个BlockingQueue暂存所有事件。<br>Event —— 事件类型。<br>Handler —— 事件的消费者。每个消费者只handle特定的事件，所有Handler要在Dispatcher上注册。</p>
<p>这个机制在YARN的各个模块中用的非常广泛，不只用于调度器。</p>
<h2 id="pull-based">pull-based</h2>
<p>通俗的说，AM通过心跳向RM申请资源，但当次心跳申请的资源不能马上拿到，而是要再经过若干次心跳才能拿到。这是一种pull-based模型。</p>
<p>AM通过RPC协议ApplicationMasterProtocol与RM通信。这个协议在服务端的实现会调用YarnScheduler的allocate方法（所有调度器都必须实现YarnScheduler接口）。allocate方法有两个作用：1.申请、释放资源；2.表示AM是否存活。超过一段时间AM没有调用这个方法，RM会认为AM挂掉并尝试重新提交。</p>
<p>allocate方法有3个参数：<applicationid, 想要申请的资源,="" 要释放的container="">。调度器会暂存这个application的资源请求，同时取出上次心跳后新分配给这个application的container，包装为一个Allocation对象返回。如果上次要求的资源也还没分配，那返回的Allocation对象就不包含任何资源。</applicationid,></p>
<p>那真正分配container是什么时候？答案是NM的心跳时。当NM向RM发送心跳时，会触发一个NODE_UPDATE事件。schduler会handle这个事件尝试在这个node上分配container。里面有一系列判断，比如当前节点是否有足够资源、优先给哪个application分配资源。如果成功分配container，就加入一个List中，等待AM下次心跳来取。</p>
<p>这点跟以前的JobTracker比较像，也是TaskTracker各自去拉取任务。</p>
<h1 id="常见调度器">常见调度器</h1>
<p>前文说过，调度器的两个主要作用：1.决定如何划分队列；2.决定如何分配资源，这里又分两种情况：为队列分配资源和为单个application分配资源。从这两方面看下常见的调度器，重点分析下FairScheduler。</p>
<h2 id="FIFO">FIFO</h2>
<p>最简单、也是默认的调度器。只有一个队列，所有用户共享。<br>资源分配的过程也非常简单，先到先得，所以很容易出现一个用户占满集群所有资源的情况。<br>可以设置ACL，但不能设置各个用户的优先级。</p>
<p>优点是简单好理解，缺点是无法控制每个用户的资源使用。<br>一般不能用于生产环境中。</p>
<h2 id="CapacityScheduler">CapacityScheduler</h2>
<p>在FIFO的基础上，增加多用户支持，最大化集群吞吐量和利用率。它基于一个很朴素的思想：每个用户都可以使用特定量的资源，但集群空闲时，也可以使用整个集群的资源。也就是说，单用户的情况下，和FIFO差不多。<br>这种设计是为了提高整个集群的利用率，避免集群有资源但不能提交任务的情况。</p>
<p>特点：</p>
<ul>
<li>划分队列使用xml文件配置，每个队列可以使用特定百分比的资源</li>
<li>队列可以是树状结构，子队列资源之和不能超过父队列。所有叶子节点的资源之和必须是100%，只有叶子节点能提交任务</li>
<li>可以为每个队列设置ACL，哪些用户可以提交任务，哪些用户有admin权限。ACL可以继承</li>
<li>队列资源可以动态变化。最多可以占用100%的资源。管理员也可以手动设置上限。</li>
<li>配置可以动态加载，但只能添加队列，不能删除</li>
<li>可以限制整个集群或每个队列的并发任务数量</li>
<li>可以限定AM使用的资源比例，避免所有资源用来执行AM而只能无限期等待的情况</li>
</ul>
<p>当选择队列分配资源时，CapacityScheduler会优先选择资源用量在规定量以下的。<br>当选择一个队列中的application分配资源时，CapacityScheduler默认使用FIFO的规则，也可以设定每个app最多占用队列资源的百分比。</p>
<p>关于CapacityScheduler一个比较重要问题就是百分比是如何计算的。默认的算法是DefaultResourceCalculator类的ratio方法，只考虑了内存。也就是说CapacityScheduler调度时是只考虑内存的。管理员也可以手动设置选择其他算法。</p>
<p>优点：灵活、集群的利用率高<br>缺点：也是灵活。某个用户的程序最多可以占用100%的资源，如果他一直不释放，其他用户只能等待，因为CapacityScheduler不支持抢占式调度，必须等上一个任务主动释放资源。</p>
<h2 id="FairScheduler">FairScheduler</h2>
<p>我们一直在用的调度器。设计思路和CapacityScheduler不同，优先保证“公平”，每个用户只有特定数量的资源可以用，不可能超出这个限制，即使集群整体很空闲。</p>
<p>特点：</p>
<ul>
<li>使用xml文件配置，每个队列可以使用特定数量的内存和CPU</li>
<li>队列是树状结构。只有叶子节点能提交任务</li>
<li>可以为每个队列设置ACL</li>
<li>可以设置每个队列的权重</li>
<li>配置可以动态加载</li>
<li>可以限制集群、队列、用户的并发任务数量</li>
<li>支持抢占式调度</li>
</ul>
<p>优点：稳定、管理方便、运维成本低<br>缺点：相对CapacityScheduler，牺牲了灵活性。经常出现某个队列资源用满，但集群整体还有空闲的情况。整体的资源利用率不高。</p>
<p>下面重点看下资源分配的算法。</p>
<h3 id="Max-min_fairness算法">Max-min fairness算法</h3>
<p>FairScheduler主要关注“公平”，那么在一个共享的集群中，怎样分配资源才算公平？<br>常用的是max-min fairness算法：<a href="http://en.wikipedia.org/wiki/Max-min_fairness" target="_blank" rel="external">wiki</a>。这种策略会最大化系统中一个用户收到的最小分配。如果每一个用户都有足够地请求，会给予每个用户一份均等的资源。尽量不让任何用户被“饿死”。</p>
<p>一个例子：资源总量是10，有3个用户A/B/C，需要的资源分别是5/4/3，应该怎样分配资源？<br>第一轮：10个资源分成3份，每个用户得到3.33<br>第二轮：3.33超出了用户C的需求，超出了0.33，将这多余的0.33平均分给A和B，每个用户得0.16<br>所以最后的分配结果是A=3.49，B=3.49，C=3</p>
<p>上面的例子没有考虑权重，如果3个用户的权重分别是0.5/1/0.8，又应该如何分配资源？<br>第一轮：将权重标准化，3个用户的权重比是5:10:8。将所有资源分成5+10+8=23份，按比例分配给各个用户。A得到<code>10*5/23=2.17</code>，B得到<code>10*10/23=4.35</code>，C得到<code>10*8/23=3.48</code>。<br>第二轮：B和C的资源超出需求了，B超过0.25，C超过0.48。将多出资源分配给A。<br>最后的分配结果是A=2.9，B=4，C=3<br>由于进位的问题会有些误差。</p>
<p>更多用户的情况下同理。</p>
<h3 id="DRF">DRF</h3>
<p>Max-min fairness解决了单一资源下，多用户的公平分配。这个算法以前主要用来分配网络流量。但在现代的资源管理系统中，往往不只有一种资源。比如YARN，包含CPU和内存两种资源。多种资源的情况下，如何公平分配？Berkeley的大牛们提出了<a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Ghodsi.pdf" target="_blank" rel="external">DRF算法</a>。</p>
<p>DRF的很多细节不提了。核心概念在于让所有application的“主要资源占比”尽量均等。比如集群总共X内存，Y CPU。app1和app2是CPU密集型的，app1每次请求3个CPU，app2每次请求4个CPU；app3和app4是内存密集型的，app3每次请求10G内存，app4每次请求20G内存。设分给app1、app2、app3、app4的请求数分别是a、b、c、d。DRF算法就是希望找到一组abcd，使得3a/X=4b/X=10c/Y=20d/Y。<br>如何判断CPU/内存密集型？如果任务需要的CPU资源/集群总的CPU资源 &gt; 需要的内存资源/集群总的内存资源，就是CPU密集型，反之是内存密集型。<br>实际应用中一般没有最优解，而是一个不断动态调整的过程。和max-min fairness一样，也要经过多轮分配，才能达到一个公平的状态。</p>
<p>如果考虑权重的话，算法会更复杂一点。另外在单一资源的情况下，DRF会退化为max-min fairness。</p>
<h3 id="资源分配过程">资源分配过程</h3>
<p>了解了一些基本的算法，接下来看看FairScheduler的资源分配过程。<br>前文说过NM的心跳会触发一个NODE_UPDATE事件，scheduler同时也是这个事件的handler，会尝试在对应的节点上分配container。<br>在FairScheduler中，所有的queue、application都继承了Scheduable，构成一个树状结构。资源分配的过程就是从这颗树的根节点开始查找，直到找到一个合适的Scheduable对象的过程。<br><img src="/2015/04/30/yarn-resource-scheduler/tree.png" alt=""></p>
<p>以上图为例，共有3种对象：ParentQueue（root是一个特殊的ParentQueue）、LeafQueue、Application，只有LeafQueue才能提交app。<br>每个Queue可以定义自己的SchedulingPolicy，这个policy主要用于Scheduable对象的排序。目前共有3种SchedulingPolicy的实现：FifoPolicy、FairSharePolicy、DominantResourceFairnessPolicy，FIFO只能用于LeafQueue，其他两种Policy可以用于任意Queue。默认是FairSharePolicy。</p>
<p>分配Container是一次深度优先搜索：从root节点开始，首先检查当前节点资源是否用满，是则直接返回（这里会同时考虑CPU和内存）。如果当前节点是ParentQueue，就将所有子节点排序（SchedulingPolicy决定了这里的顺序），依次尝试在每个子节点上分配container；如果当前节点是LeafQueue，就将下面的app排序（也是SchedulingPolicy决定，但加入了一些app特有的判断条件），依次尝试为每个app分配资源；如果当前节点是app，会比较当前app的资源需求与节点的剩余资源，如果能满足，才真正分配container。至此整个资源分配过程才完成。如果找不到合适的app，最后会返回null。</p>
<p>从上面的过程可以看出，每次NM心跳时，都会触发一次资源分配，而且只能分配一个container。所以NM的心跳频率会影响到整个集群的吞吐量。另外可以配置参数yarn.scheduler.fair.assignmultiple让一次心跳分配多个container，默认是false。</p>
<p>下面看下默认的FairSharePolicy是如何排序的。这个policy只考虑内存资源，但跟max-min failness不太一样。max-min fairness关注整体资源的公平分配，而FairSharePolicy目的在于公平分配“被调度的机会”，所以最终的资源分配可能不是算法上的最优解。但目的是一样的，都是让所有app有机会运行，不会被饿死。</p>
<p>每个Schedulable对象都有minShare、maxShare、fairShare 3个属性，其中minShare、maxShare用于排序，fairShare用于抢占式调度，后文会讲到。此外还有权重属性（weight），也会用于排序。对于queue而言，minShare、maxShare就是fair-scheduler.xml里配置的minResource和maxResource，weight也是直接配置的。对于application而言minResource直接返回0，maxResource直接返回Integer.MAX_VALUE，weight如果没有配置yarn.scheduler.fair.sizebasedweight=true就直接返回1.0，意味着所有app的权重是相同的。</p>
<p>FairSharePolicy在比较两个Schedulable对象时，先看是否有已分配资源小于minShare的，如果是说明当前Scheduable处于饥饿状态，应该被优先满足。如果两个Schedulable都处于饥饿状态，就看谁占minShare的比例更小（谁更饿）。如果没有饥饿状态的，就比较两个Schedulable已用资源与权重的比例，这个比例越大，说明占用了越多的资源，为了公平，应该给另一个Schedulable分配资源。</p>
<p>DominantResourceFairnessPolicy是YARN中DRF算法的实现，会考虑内存和CPU两种资源，排序逻辑会更复杂些，这里略过。</p>
<h3 id="任务分配过程">任务分配过程</h3>
<p>任务分配过程决定一个app被分到哪个队列。相对于资源分配过程，这个过程简单的多。因为app在提交的时候一般会指定队列名。</p>
<p>第一步：检查提交的app是否指定了队列名。如果没有指定，检查是否存在和用户同名的队列。如果还不存在，就提交到default队列。default队列可以在配置文件中指定，也可以在调度器初始化时默认创建。<br>第二步：检查ACL，当前用户是否有向指定队列提交任务的权限。<br>第三步：如果通过ACL检查，发出一个APP_ACCEPTED事件。app加入LeafQueue的children，开始等待资源分配。</p>
<p>FairScheduler的一个特点是客户端可以动态创建队列，即指定一个不存在的队列。但生产环境中这一般是不允许的。</p>
<h3 id="抢占式调度">抢占式调度</h3>
<p>FairScheduler特有的功能。当某个队列资源不足时，调度器会杀死其他队列的container以释放资源，分给这个队列。这个特性默认是关闭的。<br>关键点有两个：1.启动抢占式调度的条件？2.选择哪些container去杀掉？</p>
<p>前文说过每个Schedulable对象都有minShare、fairShare属性。这两个属性是抢占式调度的阈值。当一个Schedulable使用的资源小于fairShare*0.5、或者小于minShare，并且持续超过一定时间（这两种情况的超时时间不同，可以设置），就会开始抢占式调度。</p>
<p>Schedulabe的fairShare是会不断变化的（minShare一般不会变化）。如果队列的minResource、maxResource、权重等属性变化，fairShare都要重新计算。application开始或结束，也都要重新计算fairShare。FairScheduler中有一个线程UpdateThread，默认每0.5秒调用一次update方法，就会重新计算fairShare。<br>计算fairShare的过程就是将“上层”Schedulable的fairShare，“公平”的分配给下层的Schedulable。计算过程从root queue开始。root queue的fairShare就是整个集群的可用资源。怎样才算公平？要综合考虑各个Schedulable的权重、minShare、maxShare，算法也是由SchedulingPolicy决定的。默认是FairSharePolicy。这个计算逻辑跟max-min fairness类似。</p>
<p>当FairScheduler决定开始抢占时，首先会计算要抢得的资源量。对于使用资源量小于minShare的，要恢复到minShare；对于使用量小于fairShare*0.5的，需要恢复到fairShare。将所有要恢复的资源量相加，得出要抢的的资源总量。然后遍历所有LeafQueue，找到所有资源用量大于fairShare的app，将他们在运行的container加入一个List，按优先级升序排列。然后遍历，优先杀死优先级低的container。当释放足够的资源后，抢占停止。</p>
<p>如何确定container的优先级？这是由AM在申请资源的时候决定的。用一个整数表示，数字越大优先级越低。以MapReduce为例，AM Container是0，Reduce Container是10，Map Contaienr是20。意味着一个map任务更容易被杀死。</p>
<p>抢占式调度可以一定程度上保证公平，但不可控因素比较多。如果用户的长时间任务因此失败，是不可接受的。所以生产环境一般关闭这个特性。</p>
<h3 id="计算本地性">计算本地性</h3>
<p>从MapReduce时代开始，“移动计算比移动数据更经济”的概念就深入人心。在YARN中，当然也继承了这一传统。这一特性主要是用来配合HDFS的，因为HDFS的多副本，任务应该尽量在选择block所在的机器上执行，可以减少网络传输的消耗。如果开启了Short-Circuit Read特性，还可以直接读本地文件，提高效率。</p>
<p>本地性有3个级别：NODE_LOCAL、RACK_LOCAL、OFF_SWITCH，分别代表同节点、同机架、跨机架。计算效率会依次递减。</p>
<p>根据前文所述，Container在申请时可以指定节点，但这不是强制的。只有NM心跳的时候才会分配资源，所以container无法一般确定自己在那个节点上执行，基本是随机的。scheduler能做的只是尽量满足NODE_LOCAL，尽量避免OFF_SWITCH。计算本地性更多的要AM端配合，当AM拿到资源后，优先分配给NODE_LOCAL的任务。</p>
<p>但FairScheduler中，允许一个app错过若干次调度机会，以便能分到一个NODE_LOCAL的节点。由yarn.scheduler.fair.locality.threshold.node控制。这个参数是一个百分比，表示相对整个集群的节点数目而言，一个app可以错过多少次机会。<br>比如yarn.scheduler.fair.locality.threshold.node为0.2，集群节点数为10。那么FairScheduler分配这个资源时，发现当前发来心跳的NM不能满足这个app的NODE_LOCAL要求，就会跳过，继续寻找下一个APP。相当于这个app错过一次调度机会，最多可以错过2次。<br>对RACK_LOCAL而言，有一个参数yarn.scheduler.fair.locality.threshold.rack，作用差不多。</p>
<h1 id="发展趋势">发展趋势</h1>
<p>YARN的发展一直比较快，调度/资源相关的一些值得关注的改进：</p>
<p>Label based scheduling<br><a href="https://issues.apache.org/jira/browse/YARN-796" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-796</a><br><a href="https://issues.apache.org/jira/browse/YARN-3214" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-3214</a><br>可以给不同的节点加上标签，比如某些节点CPU频率比较高、某些节点内存比较大，RM在调度的时候，可以更有针对性。甚至可以分成多个小集群供不同用户使用。</p>
<p>管理更多资源<br>磁盘：<a href="https://issues.apache.org/jira/browse/YARN-2139" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-2139</a><br>网络：<a href="https://issues.apache.org/jira/browse/YARN-2140" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-2140</a></p>
<p>Dynamic resource configuration<br><a href="https://issues.apache.org/jira/browse/YARN-291" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-291</a><br>动态加载资源配置。包括自动探测实际的机器配置，而不是管理员手动设置。</p>
<p>app级别的权重设置<br><a href="https://issues.apache.org/jira/browse/YARN-1963" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-1963</a></p>
<p>使用Docker做资源隔离<br><a href="https://issues.apache.org/jira/browse/YARN-1964" target="_blank" rel="external">https://issues.apache.org/jira/browse/YARN-1964</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>公司活动写的一篇文章。这里也发下吧。<br>介绍下YARN中资源调度相关概念和算法。以hadoop 2.2.0为准。<br>]]>
    
    </summary>
    
      <category term="yarn" scheme="http://jxy.me/tags/yarn/"/>
    
      <category term="resourcemanager" scheme="http://jxy.me/tags/resourcemanager/"/>
    
      <category term="scheduler" scheme="http://jxy.me/tags/scheduler/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Maven dependency exclude的bug]]></title>
    <link href="http://jxy.me/2015/04/30/maven-exlcude-bug/"/>
    <id>http://jxy.me/2015/04/30/maven-exlcude-bug/</id>
    <published>2015-04-30T08:33:15.000Z</published>
    <updated>2015-09-03T15:42:14.000Z</updated>
    <content type="html"><![CDATA[<p>最近碰到的一个bug。<br><a id="more"></a></p>
<p>maven的依赖传递，可以过滤掉特定的依赖。<br>但有时我希望能过滤掉所有的依赖，比如引用haodop-common的时候，只要hadoop-common的jar包，hadoop-common引入的其他依赖全部过滤。</p>
<p>maven2里不支持这种功能。maven 3.0.x/3.1.x里可以这么写，一个小trick：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="variable">&lt;dependency&gt;</span></div><div class="line">    <span class="variable">&lt;groupId&gt;</span>org.apache.hadoop<span class="variable">&lt;/groupId&gt;</span></div><div class="line">    <span class="variable">&lt;artifactId&gt;</span>hadoop-common<span class="variable">&lt;/artifactId&gt;</span></div><div class="line">    <span class="variable">&lt;version&gt;</span>${hadoop.version}<span class="variable">&lt;/version&gt;</span></div><div class="line">    <span class="variable">&lt;exclusions&gt;</span></div><div class="line">        <span class="variable">&lt;exclusion&gt;</span></div><div class="line">            <span class="variable">&lt;artifactId&gt;</span><span class="keyword">*</span><span class="variable">&lt;/artifactId&gt;</span></div><div class="line">            <span class="variable">&lt;groupId&gt;</span><span class="keyword">*</span><span class="variable">&lt;/groupId&gt;</span></div><div class="line">        <span class="variable">&lt;/exclusion&gt;</span></div><div class="line">    <span class="variable">&lt;/exclusions&gt;</span></div><div class="line"><span class="variable">&lt;/dependency&gt;</span></div></pre></td></tr></table></figure>

<p>但是构建时会有warning：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[WARNING] </div><div class="line">[WARNING] Some problems were encountered <span class="keyword">while</span> building the effective model <span class="keyword">for</span> com.netease.hadoop:meta-data:jar:<span class="number">0.1</span>.<span class="number">0</span>-SNAPSHOT</div><div class="line">[WARNING] <span class="string">'dependencies.dependency.exclusions.exclusion.groupId'</span> <span class="keyword">for</span> org.apache.hadoop:hadoop-common:jar <span class="keyword">with</span> value <span class="string">'*'</span> does <span class="keyword">not</span> match a valid id pattern. @ line <span class="number">62</span>, column <span class="number">30</span></div><div class="line">[WARNING] <span class="string">'dependencies.dependency.exclusions.exclusion.artifactId'</span> <span class="keyword">for</span> org.apache.hadoop:hadoop-common:jar <span class="keyword">with</span> value <span class="string">'*'</span> does <span class="keyword">not</span> match a valid id pattern. @ line <span class="number">61</span>, column <span class="number">33</span></div><div class="line">[WARNING] </div><div class="line">[WARNING] It <span class="keyword">is</span> highly recommended <span class="keyword">to</span> fix these problems because they threaten the stability <span class="keyword">of</span> your build.</div><div class="line">[WARNING] </div><div class="line">[WARNING] <span class="keyword">For</span> this reason, <span class="keyword">future</span> Maven versions might no longer support building such malformed projects.</div><div class="line">[WARNING]</div></pre></td></tr></table></figure>

<p>在maven 3.2.1里，这成为一个正式的功能了，见<a href="http://maven.apache.org/docs/3.2.1/release-notes.html" target="_blank" rel="external">Release Notes</a>。其实就是把warning去掉了。。。</p>
<p>但是用<code>mvn dependency:tree</code>时还是有warning，而且输出的树结构里，hadoop-common引入的依赖还在，exclude元素根本没生效，这是maven-dependency-plugin的bug。2.1版本有warning，2.8版本就正常了。其他版本没试过。</p>
<p>maven-assembly-plugin也有bug，本该被排除的依赖还是会被打包，试了最新的2.5.4版本也是有问题。<br>我的写法：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="title">dependencySets</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="title">dependencySet</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="title">outputDirectory</span>&gt;</span>lib<span class="tag">&lt;/<span class="title">outputDirectory</span>&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="title">dependencySet</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="title">dependencySets</span>&gt;</span></div></pre></td></tr></table></figure>

<p>还不知道要怎么处理，只能自己手动在dependencySet里过滤了。</p>
<p>我只想说，隐性的bug真tm多。。。而且debug非常烦。折腾了2天。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>最近碰到的一个bug。<br>]]>
    
    </summary>
    
      <category term="maven" scheme="http://jxy.me/tags/maven/"/>
    
  </entry>
  
</feed>
